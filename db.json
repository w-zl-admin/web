{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"themes/hexo-theme-next-5.0.1/source/404.html","path":"404.html","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/css/main.styl","path":"css/main.styl","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by.svg","path":"images/cc-by.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/loading.gif","path":"images/loading.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/placeholder.gif","path":"images/placeholder.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/quote-l.svg","path":"images/quote-l.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/quote-r.svg","path":"images/quote-r.svg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/images/searchicon.png","path":"images/searchicon.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/affix.js","path":"js/src/affix.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/bootstrap.js","path":"js/src/bootstrap.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/hook-duoshuo.js","path":"js/src/hook-duoshuo.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/motion.js","path":"js/src/motion.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/post-details.js","path":"js/src/post-details.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/scrollspy.js","path":"js/src/scrollspy.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/utils.js","path":"js/src/utils.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/2x2ContingencyTable.png","path":"upload/1/2x2ContingencyTable.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/EntropyFunction.png","path":"upload/1/EntropyFunction.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/class.png","path":"upload/1/class.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/chisq.png","path":"upload/1/chisq.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/decisionTreeSample.png","path":"upload/1/decisionTreeSample.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/mclass.png","path":"upload/1/mclass.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/nclass.png","path":"upload/1/nclass.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/overfit1.jpg","path":"upload/1/overfit1.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/pca_before.png","path":"upload/1/pca_before.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/pruned_tree1.png","path":"upload/1/pruned_tree1.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/pruned_tree2.png","path":"upload/1/pruned_tree2.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/raw_tree.png","path":"upload/1/raw_tree.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/summary.png","path":"upload/1/summary.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/summary1.png","path":"upload/1/summary1.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/towbit.png","path":"upload/1/towbit.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/site/me.jpg","path":"upload/site/me.jpg","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/LICENSE","path":"vendors/fastclick/LICENSE","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/README.md","path":"vendors/fastclick/README.md","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/bower.json","path":"vendors/fastclick/bower.json","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/bower.json","path":"vendors/font-awesome/bower.json","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/HELP-US-OUT.txt","path":"vendors/font-awesome/HELP-US-OUT.txt","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/CONTRIBUTING.md","path":"vendors/jquery_lazyload/CONTRIBUTING.md","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/README.md","path":"vendors/jquery_lazyload/README.md","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/bower.json","path":"vendors/jquery_lazyload/bower.json","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/jquery.lazyload.js","path":"vendors/jquery_lazyload/jquery.lazyload.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/jquery.scrollstop.js","path":"vendors/jquery_lazyload/jquery.scrollstop.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/bower.json","path":"vendors/velocity/bower.json","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.min.js","path":"vendors/velocity/velocity.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.ui.js","path":"vendors/velocity/velocity.ui.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.ui.min.js","path":"vendors/velocity/velocity.ui.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/2/NN.png","path":"upload/2/NN.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/2/NN_with_bias.png","path":"upload/2/NN_with_bias.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery/index.js","path":"vendors/jquery/index.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/schemes/pisces.js","path":"js/src/schemes/pisces.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_loading.gif","path":"vendors/fancybox/source/fancybox_loading.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/blank.gif","path":"vendors/fancybox/source/blank.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_loading@2x.gif","path":"vendors/fancybox/source/fancybox_loading@2x.gif","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_overlay.png","path":"vendors/fancybox/source/fancybox_overlay.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_sprite.png","path":"vendors/fancybox/source/fancybox_sprite.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_sprite@2x.png","path":"vendors/fancybox/source/fancybox_sprite@2x.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/jquery.fancybox.css","path":"vendors/fancybox/source/jquery.fancybox.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/jquery.fancybox.js","path":"vendors/fancybox/source/jquery.fancybox.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/jquery.fancybox.pack.js","path":"vendors/fancybox/source/jquery.fancybox.pack.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/lib/fastclick.js","path":"vendors/fastclick/lib/fastclick.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/lib/fastclick.min.js","path":"vendors/fastclick/lib/fastclick.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/css/font-awesome.css","path":"vendors/font-awesome/css/font-awesome.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/css/font-awesome.css.map","path":"vendors/font-awesome/css/font-awesome.css.map","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/css/font-awesome.min.css","path":"vendors/font-awesome/css/font-awesome.min.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff2","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/ua-parser-js/dist/ua-parser.min.js","path":"vendors/ua-parser-js/dist/ua-parser.min.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/ua-parser-js/dist/ua-parser.pack.js","path":"vendors/ua-parser-js/dist/ua-parser.pack.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/3/CNN.png","path":"upload/3/CNN.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/FontAwesome.otf","path":"vendors/font-awesome/fonts/FontAwesome.otf","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","path":"vendors/font-awesome/fonts/fontawesome-webfont.eot","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","path":"vendors/font-awesome/fonts/fontawesome-webfont.woff","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.js","path":"vendors/velocity/velocity.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/fancybox_buttons.png","path":"vendors/fancybox/source/helpers/fancybox_buttons.png","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-media.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","path":"vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","path":"vendors/font-awesome/fonts/fontawesome-webfont.ttf","modified":1,"renderable":1},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","path":"vendors/font-awesome/fonts/fontawesome-webfont.svg","modified":1,"renderable":1}],"Cache":[{"_id":"themes/hexo-theme-next-5.0.1/.bowerrc","hash":"80e096fdc1cf912ee85dd9f7e6e77fd40cf60f10","modified":1461824879541},{"_id":"themes/hexo-theme-next-5.0.1/.editorconfig","hash":"792fd2bd8174ece1a75d5fd24ab16594886f3a7f","modified":1461824879630},{"_id":"themes/hexo-theme-next-5.0.1/.gitignore","hash":"efec790f5b7a0256763e1cc08f12c4f0aff509f6","modified":1461824901163},{"_id":"themes/hexo-theme-next-5.0.1/.hound.yml","hash":"b76daa84c9ca3ad292c78412603370a367cc2bc3","modified":1461824879719},{"_id":"themes/hexo-theme-next-5.0.1/.javascript_ignore","hash":"d619ee13031908cd72666e4ff652d2ea3483b1c3","modified":1461824879803},{"_id":"themes/hexo-theme-next-5.0.1/.jshintrc","hash":"9928f81bd822f6a8d67fdbc909b517178533bca9","modified":1461824901201},{"_id":"themes/hexo-theme-next-5.0.1/README.en.md","hash":"565ba52b3825b85a9f05b41183caca7f18b741d4","modified":1461824879858},{"_id":"themes/hexo-theme-next-5.0.1/README.md","hash":"500b5606eb6a09c979d16128f8b00f4bf9bc95ac","modified":1461824901232},{"_id":"themes/hexo-theme-next-5.0.1/bower.json","hash":"f89c6700a11d81e067cc97273ca6bf96cb88c8f9","modified":1461824901275},{"_id":"themes/hexo-theme-next-5.0.1/gulpfile.coffee","hash":"26e5b1b945704c8bc78b928feede895c4c111c95","modified":1461824879971},{"_id":"themes/hexo-theme-next-5.0.1/package.json","hash":"63e9c0f1dd9e5d7f51b4ae383981ef939a2ed45d","modified":1461824880025},{"_id":"source/_posts/CNNs.md","hash":"c26ee199c54502d4be7b6796a2cd321bcb172b11","modified":1468309067406},{"_id":"source/_posts/人工神经网络、反向传播算法和python3的实现.md","hash":"eedec4d56c4c84c6065023e7aba61a2fc2c94fa7","modified":1467362176417},{"_id":"source/categories/index.md","hash":"ea42d31c71916ecfe556108c4a7b4d6c3170df9c","modified":1464142849176},{"_id":"source/tags/index.md","hash":"c3043576700e4a14a023526ec5c1cce3c783507b","modified":1464142765236},{"_id":"themes/hexo-theme-next-5.0.1/_config.yml","hash":"0c69020434b9baf28cdd5dfff57161e31404d1f4","modified":1467362238355},{"_id":"themes/hexo-theme-next-5.0.1/.github/CONTRIBUTING.md","hash":"5ab257af816986cd0e53f9527a92d5934ac70ae9","modified":1461824880091},{"_id":"themes/hexo-theme-next-5.0.1/.github/ISSUE_TEMPLATE.md","hash":"c2024ded82143807c28a299c5fe6b927ef3525ff","modified":1461824880144},{"_id":"themes/hexo-theme-next-5.0.1/languages/de.yml","hash":"786afba25cfc98845a20d9901823ebeebcd1cbbf","modified":1461824880281},{"_id":"themes/hexo-theme-next-5.0.1/languages/default.yml","hash":"9db835c0543ade5a89bc80ec5a898203227cf3d8","modified":1461824880340},{"_id":"themes/hexo-theme-next-5.0.1/languages/en.yml","hash":"f03799cbdb5a33064ead080bcac4baca1f6bc5f9","modified":1461824880440},{"_id":"themes/hexo-theme-next-5.0.1/languages/fr-FR.yml","hash":"1a084623c39de74301f3e92f9388a3a815a542ca","modified":1461824880513},{"_id":"themes/hexo-theme-next-5.0.1/languages/ja.yml","hash":"a2c7b6301b5474aab798946fb700289df237c3cf","modified":1461824880584},{"_id":"themes/hexo-theme-next-5.0.1/languages/pt.yml","hash":"ca239b39bf65c9462e59d51b12f0fe566d453197","modified":1461824880658},{"_id":"themes/hexo-theme-next-5.0.1/languages/ru.yml","hash":"cc7b964a46587aea0e57b0a5269d8fd25570858e","modified":1461824880701},{"_id":"themes/hexo-theme-next-5.0.1/languages/zh-Hans.yml","hash":"053ef66b1b9694138055e2ba5ee98dfa2f21d35a","modified":1464074399000},{"_id":"themes/hexo-theme-next-5.0.1/languages/zh-hk.yml","hash":"519ab3d817ec3bc5bfc91159c494b6b3c170bea7","modified":1461824880791},{"_id":"themes/hexo-theme-next-5.0.1/languages/zh-tw.yml","hash":"6b1f345aaefc13e6723dc8a6741b59ac05c20dfd","modified":1461824880837},{"_id":"themes/hexo-theme-next-5.0.1/layout/_layout.swig","hash":"993df74467835eeb223d92206f36ecd6cfe5119e","modified":1461824880888},{"_id":"themes/hexo-theme-next-5.0.1/layout/archive.swig","hash":"b5b59d70fc1563f482fa07afd435752774ad5981","modified":1461824880933},{"_id":"themes/hexo-theme-next-5.0.1/layout/category.swig","hash":"6422d196ceaff4220d54b8af770e7e957f3364ad","modified":1461824881009},{"_id":"themes/hexo-theme-next-5.0.1/layout/index.swig","hash":"427d0b95b854e311ae363088ab39a393bf8fdc8b","modified":1461824881133},{"_id":"themes/hexo-theme-next-5.0.1/layout/page.swig","hash":"8019d02232a6dd1a665b6a4d2daef8e5dd2f0049","modified":1461824881333},{"_id":"themes/hexo-theme-next-5.0.1/layout/post.swig","hash":"e2e512142961ddfe77eba29eaa88f4a2ee43ae18","modified":1461824901327},{"_id":"themes/hexo-theme-next-5.0.1/layout/tag.swig","hash":"07cf49c49c39a14dfbe9ce8e7d7eea3d4d0a4911","modified":1461824881393},{"_id":"themes/hexo-theme-next-5.0.1/scripts/merge-configs.js","hash":"0c56be2e85c694247cfa327ea6d627b99ca265e8","modified":1461824883884},{"_id":"themes/hexo-theme-next-5.0.1/source/404.html","hash":"ee2b6788ecf5d2b2eec3cb2a26fa41b9fa3cbaa8","modified":1465795768657},{"_id":"source/_posts/利用决策树来识别Cufflinks中的技术噪音.md","hash":"af338ecad44e0c6654526e38f1a8ea25187f6301","modified":1465784532397},{"_id":"themes/hexo-theme-next-5.0.1/test/.jshintrc","hash":"096ed6df627373edd820f24d46b8baf528dee61d","modified":1461824893355},{"_id":"themes/hexo-theme-next-5.0.1/test/helpers.js","hash":"a1f5de25154c3724ffc24a91ddc576cdbd60864f","modified":1461824893421},{"_id":"themes/hexo-theme-next-5.0.1/test/intern.js","hash":"11fa8a4f5c3b4119a179ae0a2584c8187f907a73","modified":1461824893442},{"_id":"themes/hexo-theme-next-5.0.1/source/fonts/.gitkeep","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824890101},{"_id":"themes/hexo-theme-next-5.0.1/layout/_macro/post.swig","hash":"cee85781fe87d98433c994a0571ad7c39a48155b","modified":1461824881517},{"_id":"themes/hexo-theme-next-5.0.1/layout/_macro/post-collapse.swig","hash":"43c3433155ccd9abcbe7dce2e6bfa1f3a66af18b","modified":1461824881452},{"_id":"themes/hexo-theme-next-5.0.1/layout/_macro/reward.swig","hash":"70bd1f11a558a7fc4b8fd7860377f31e0357ceb6","modified":1461824881572},{"_id":"themes/hexo-theme-next-5.0.1/layout/_macro/sidebar.swig","hash":"6609240fe73a53e436d0c8302279e28596dc9c70","modified":1461824881616},{"_id":"themes/hexo-theme-next-5.0.1/layout/_macro/wechat-subscriber.swig","hash":"85327c2174d09c6d69c9033592e6c8f7eb7ac3ba","modified":1461824881660},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/comments.swig","hash":"82a9bc2ba60ce68419128ff60624bd74b15dfb78","modified":1461824881707},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/duoshuo-hot-articles.swig","hash":"5d4638c46aef65bf32a01681495b62416ccc98db","modified":1461824881761},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/footer.swig","hash":"3003fda5a8af553451f7945f4e1ce006ca72e4c4","modified":1461824881852},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/head.swig","hash":"53bf077755e217a2b5ae107da62b6a824a0baa52","modified":1461824881898},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/header.swig","hash":"7739068b69f87ae641dea19c3445169ae80d5df4","modified":1461824881960},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/pagination.swig","hash":"9e8e21d194ef44d271b1cca0bc1448c14d7edf4f","modified":1461824882003},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/search.swig","hash":"011b9d6c9f0a2f4654908ea20b9391f9b7981271","modified":1461824882050},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/boostrap.swig","hash":"03aaebe9d50f6acb007ec38cc04acd1cfceb404d","modified":1461824882665},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/baidu-push.swig","hash":"82d060fe055d6e423bbc9199f82dfe5c68e74779","modified":1461824882619},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/commons.swig","hash":"766b2bdda29523ed6cd8d7aa197f996022f8fd94","modified":1461824882707},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/vendors.swig","hash":"0b91cadecead8e0b5211cc42b085998d94af503a","modified":1461824882775},{"_id":"themes/hexo-theme-next-5.0.1/scripts/tags/center-quote.js","hash":"535fc542781021c4326dec24d8495cbb1387634a","modified":1461824883964},{"_id":"themes/hexo-theme-next-5.0.1/scripts/tags/full-image.js","hash":"3acce36db0feb11a982c6c799aa6b6b47df2827c","modified":1461824884024},{"_id":"themes/hexo-theme-next-5.0.1/scripts/tags/group-pictures.js","hash":"49252824cd53184dc9b97b2f2d87ff28e1b3ef27","modified":1461824884078},{"_id":"themes/hexo-theme-next-5.0.1/source/css/main.styl","hash":"20702c48d6053c92c5bcdbc68e8d0ef1369848a0","modified":1461824884154},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nc-nd.svg","hash":"c6524ece3f8039a5f612feaf865d21ec8a794564","modified":1461824890155},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nc-sa.svg","hash":"3031be41e8753c70508aa88e84ed8f4f653f157e","modified":1461824890197},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nc.svg","hash":"8d39b39d88f8501c0d27f8df9aae47136ebc59b7","modified":1461824890241},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-nd.svg","hash":"c563508ce9ced1e66948024ba1153400ac0e0621","modified":1461824890295},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by-sa.svg","hash":"aa4742d733c8af8d38d4c183b8adbdcab045872e","modified":1461824890350},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-by.svg","hash":"28a0a4fe355a974a5e42f68031652b76798d4f7e","modified":1461824890391},{"_id":"themes/hexo-theme-next-5.0.1/source/images/cc-zero.svg","hash":"87669bf8ac268a91d027a0a4802c92a1473e9030","modified":1461824890472},{"_id":"themes/hexo-theme-next-5.0.1/source/images/loading.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1461824890511},{"_id":"themes/hexo-theme-next-5.0.1/source/images/placeholder.gif","hash":"5fbd472222feb8a22cf5b8aa5dc5b8e13af88e2b","modified":1461824890555},{"_id":"themes/hexo-theme-next-5.0.1/source/images/quote-l.svg","hash":"94e870b4c8c48da61d09522196d4dd40e277a98f","modified":1461824890596},{"_id":"themes/hexo-theme-next-5.0.1/source/images/quote-r.svg","hash":"e60ae504f9d99b712c793c3740c6b100d057d4ec","modified":1461824890632},{"_id":"themes/hexo-theme-next-5.0.1/source/images/searchicon.png","hash":"67727a6a969be0b2659b908518fa6706eed307b8","modified":1461824890672},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/schemes/mist.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824882890},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/schemes/muse.swig","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824882933},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_mixins/Mist.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824888224},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_mixins/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824888268},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_mixins/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824888493},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_variables/Muse.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824889872},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_variables/custom.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1461824890005},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/head/external-fonts.swig","hash":"9c853c10ab79a3940aa3c0affe0badb881d5e890","modified":1461824882134},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/search/localsearch.swig","hash":"ff5523d5dacaa77a55a24e50e6e6530c3b98bfad","modified":1461824882217},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/search/swiftype.swig","hash":"959b7e04a96a5596056e4009b73b6489c117597e","modified":1461824882300},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/search/tinysou.swig","hash":"eefe2388ff3d424694045eda21346989b123977c","modified":1461824882370},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/share/add-this.swig","hash":"c07f7b2f264e5215b8ed42d67e8cef2477558364","modified":1461824882430},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/share/baidushare.swig","hash":"7ca5cb4daa58b3504e17f3e02975e794bc634658","modified":1461824882473},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/share/duoshuo_share.swig","hash":"89c5a5240ecb223acfe1d12377df5562a943fd5d","modified":1461824882515},{"_id":"themes/hexo-theme-next-5.0.1/layout/_partials/share/jiathis.swig","hash":"63315fcf210799f894208c9f512737096df84962","modified":1461824882558},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/pages/post-details.swig","hash":"069d1357c717572256e5cdee09574ebce529cbae","modified":1461824882835},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/schemes/pisces.swig","hash":"a44acf9b0d0f44ef3dfc767376a95c984cc127de","modified":1461824883043},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/analytics.swig","hash":"0a89c04055bade7baa5962f1d5aefe438d83a244","modified":1461824883100},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/comments.swig","hash":"907b931d775d32405d02a25b3b0a3ac03bf804d0","modified":1461824883158},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/lean-analytics.swig","hash":"2fc4a0d2c825a512f39c0eadd78452e90615465a","modified":1461824883251},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/localsearch.swig","hash":"1561bd0c107d725252c6d746e9ac177fc18f93bf","modified":1461824883315},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/mathjax.swig","hash":"5bafc33f57508d1d04a9930165240f6e9efa8d6d","modified":1461824883396},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/tinysou.swig","hash":"cb3a5d36dbe1630bab84e03a52733a46df7c219b","modified":1461824883441},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_custom/custom.styl","hash":"328d9a9696cc2ccf59c67d3c26000d569f46344c","modified":1461824888162},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_mixins/Pisces.styl","hash":"adc83b19e793491b1c6ea0fd8b46cd9f32e592fc","modified":1461824888312},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_mixins/base.styl","hash":"4e49707c99c8bbcfa0a607dfdaff0fbb7dffd2a3","modified":1461824888430},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_variables/Mist.styl","hash":"c8d35a6b9e3bff6d8fdb66de853065af9d37562d","modified":1461824889822},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_variables/Pisces.styl","hash":"11d443fc97648d2965d7f8bad9f4a493996fe62f","modified":1461824889918},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_variables/base.styl","hash":"2676996e819af2079c552f755ab7933939e151cd","modified":1461824889961},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_variables/default.styl","hash":"8ec3307fe42d738b1bbda4b6419d0995f5560222","modified":1461824890054},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/affix.js","hash":"978e0422b5bf1b560236d8d10ebc1adcf66392e3","modified":1461824890720},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/bootstrap.js","hash":"39bf93769d9080fa01a9a875183b43198f79bc19","modified":1461824890760},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/hook-duoshuo.js","hash":"a6119070c0119f33e08b29da7d2cce2635eb40a0","modified":1461824890801},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/motion.js","hash":"269414e84df544a4ccb88519f6abae4943db3c67","modified":1461824890839},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/post-details.js","hash":"10247c78fe933a0cfcaad22a2a03e7a026864461","modified":1461824890867},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/scrollspy.js","hash":"fe4da1b9fe73518226446f5f27d2831e4426fc35","modified":1461824890899},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/utils.js","hash":"a3a3375de818964f4cbed4d0e2c2f97ccee7199e","modified":1461824890938},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/2x2ContingencyTable.png","hash":"3647d3787e752fd7ddde8eee88e845946835f561","modified":1465722031868},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/EntropyFunction.png","hash":"94f8f10b6ccdea2ac373dc900f14c513a7e74082","modified":1465722031900},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/class.png","hash":"c7b1485cb9eebead805a5c33f73da87acace6806","modified":1465722031883},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/chisq.png","hash":"edaa7adec72ee75e69a8c457e4abe7bd42c57915","modified":1465722031875},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/decisionTreeSample.png","hash":"8922a5ef12d19b92fbd0faf3f144ee3161bb4790","modified":1465722031892},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/mclass.png","hash":"fdd25c17474817b0610fbbe55df8d7abd8ded476","modified":1465722031906},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/nclass.png","hash":"48484399a11c8b69bcaccf751f894f1c04c6527f","modified":1465722031912},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/overfit1.jpg","hash":"4d013f89cb3208515ff3da06c1e1fc7578c5934f","modified":1465722032064},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/pca_before.png","hash":"6f74357187607e81614ad9797e4f5e5b0b18ff2d","modified":1465722032071},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/pruned_tree1.png","hash":"1616dd9cf7dbc829389d69d34492d7f037f0e304","modified":1465722032078},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/pruned_tree2.png","hash":"f8fab7c48872fcaaa10757db64cb3cd0e33fffe7","modified":1465722032087},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/raw_tree.png","hash":"1616dd9cf7dbc829389d69d34492d7f037f0e304","modified":1465722032095},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/summary.png","hash":"e52017823f4f8ef08ab918001b755fd485a54cc9","modified":1465722032104},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/summary1.png","hash":"2214664f8465d0ef7ecd09f1fe1d414f08805769","modified":1465782960813},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/1/towbit.png","hash":"f1aa3599dd40f454409831447c512530d2711cde","modified":1465722032110},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/site/me.jpg","hash":"653a4ee12332e14820ee5cb5e58702db52a6e50d","modified":1461826361000},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/.bower.json","hash":"cc40a9b11e52348e554c84e4a5c058056f6b7aeb","modified":1461824891025},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/.gitattributes","hash":"2db21acfbd457452462f71cc4048a943ee61b8e0","modified":1461824891068},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/.bower.json","hash":"93ebd5b35e632f714dcf1753e1f6db77ec74449b","modified":1461824891738},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/LICENSE","hash":"dcd5b6b43095d9e90353a28b09cb269de8d4838e","modified":1461824891783},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/README.md","hash":"1decd8e1adad2cd6db0ab50cf56de6035156f4ea","modified":1461824891827},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/bower.json","hash":"13379463c7463b4b96d13556b46faa4cc38d81e6","modified":1461824891875},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/.bower.json","hash":"7da985a99674e54f514d4fd9fcd3bcea6e7e41d5","modified":1461824892009},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/.gitignore","hash":"69d152fa46b517141ec3b1114dd6134724494d83","modified":1461824892150},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/.npmignore","hash":"dcf470ab3a358103bb896a539cc03caeda10fa8b","modified":1461824892174},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/bower.json","hash":"279a8a718ab6c930a67c41237f0aac166c1b9440","modified":1461824892238},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery/.bower.json","hash":"91745c2cc6c946c7275f952b2b0760b880cea69e","modified":1461824892751},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/HELP-US-OUT.txt","hash":"69a4c537d167b68a0ccf1c6febd138aeffca60d6","modified":1461824892198},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/.bower.json","hash":"b7638afc93e9cd350d0783565ee9a7da6805ad8e","modified":1461824892801},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/CONTRIBUTING.md","hash":"4891864c24c28efecd81a6a8d3f261145190f901","modified":1461824892831},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/README.md","hash":"895d50fa29759af7835256522e9dd7dac597765c","modified":1461824892866},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/bower.json","hash":"65bc85d12197e71c40a55c0cd7f6823995a05222","modified":1461824892892},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/jquery.lazyload.js","hash":"481fd478650e12b67c201a0ea41e92743f8b45a3","modified":1461824892922},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery_lazyload/jquery.scrollstop.js","hash":"0e9a81785a011c98be5ea821a8ed7d411818cfd1","modified":1461824892948},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/.bower.json","hash":"05f960846f1c7a93dab1d3f9a1121e86812e8c88","modified":1461824893052},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/bower.json","hash":"2ec99573e84c7117368beccb9e94b6bf35d2db03","modified":1461824893082},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1461824893137},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.ui.js","hash":"6a1d101eab3de87527bb54fcc8c7b36b79d8f0df","modified":1461824893166},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1461824893199},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/2/NN.png","hash":"674b92579e5b5f822943885e856f979060d56641","modified":1466739756405},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/2/NN_with_bias.png","hash":"433cee23248a48cd3784dad5070b92d40b27108b","modified":1467361909489},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/jquery/index.js","hash":"41b4bfbaa96be6d1440db6e78004ade1c134e276","modified":1461824892773},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/analytics/baidu-analytics.swig","hash":"7c43d66da93cde65b473a7d6db2a86f9a42647d6","modified":1461824883492},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/analytics/cnzz-analytics.swig","hash":"44e761721e8ad787ef571a3cc57bbc12d318a2a3","modified":1461824883533},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/analytics/facebook-sdk.swig","hash":"334176d838ee528e58468d8bc74ff3a6d3f25b2b","modified":1461824883575},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/analytics/google-analytics.swig","hash":"30a23fa7e816496fdec0e932aa42e2d13098a9c2","modified":1461824883616},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/analytics/tencent-analytics.swig","hash":"3658414379e0e8a34c45c40feadc3edc8dc55f88","modified":1461824883672},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/comments/disqus.swig","hash":"3491d3cebabc8a28857200db28a1be65aad6adc2","modified":1461824883755},{"_id":"themes/hexo-theme-next-5.0.1/layout/_scripts/third-party/comments/duoshuo.swig","hash":"8c7af79407d223486fba72b8150fe045a553bf70","modified":1461824883802},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/back-to-top.styl","hash":"b49efc66bd055a2d0be7deabfcb02ee72a9a28c8","modified":1461824884232},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/buttons.styl","hash":"0dfb4b3ba3180d7285e66f270e1d3fa0f132c3d2","modified":1461824884300},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/comments.styl","hash":"471f1627891aca5c0e1973e09fbcb01e1510d193","modified":1461824884362},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/components.styl","hash":"10994990d6e0b4d965a728a22cf7f6ee29cae9f6","modified":1461824884422},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/pagination.styl","hash":"711c8830886619d4f4a0598b0cde5499dce50c62","modified":1461824884471},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/tag-cloud.styl","hash":"dd8a3b22fc2f222ac6e6c05bd8a773fb039169c0","modified":1461824884520},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/outline/outline.styl","hash":"2186be20e317505cd31886f1291429cc21f76703","modified":1461824887698},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/scaffolding/base.styl","hash":"5304f99581da3a31de3ecec959b7adf9002fde83","modified":1461824887746},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/scaffolding/helpers.styl","hash":"4c84903d6a15a903235eec04a560fdeda80c12f3","modified":1461824887797},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/scaffolding/normalize.styl","hash":"ece571f38180febaf02ace8187ead8318a300ea7","modified":1461824887850},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/scaffolding/scaffolding.styl","hash":"013619c472c7e4b08311c464fcbe9fcf5edde603","modified":1461824887945},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/scaffolding/tables.styl","hash":"04e6c5257814c65e638ab70c53030e8dfdb3f37d","modified":1461824888008},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/_base.styl","hash":"c2d079788d6fc2e9a191ccdae94e50d55bf849dc","modified":1461824888577},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/_header.styl","hash":"5ae7906dc7c1d9468c7f4b4a6feddddc555797a1","modified":1461824888632},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/_logo.styl","hash":"38e5df90c8689a71c978fd83ba74af3d4e4e5386","modified":1461824888679},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/_menu.styl","hash":"b0dcca862cd0cc6e732e33d975b476d744911742","modified":1461824888742},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/_posts-expanded.styl","hash":"4303776991ef28f5742ca51c7dffe6f12f0acf34","modified":1461824888804},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1461824888847},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/index.styl","hash":"6cc10d943d92eac953e6978e46d27ae55584482c","modified":1461824888903},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Muse/_layout.styl","hash":"6ed60cc621bac096c0ed7534fa25b1a52dc571d4","modified":1461824889034},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Muse/_logo.styl","hash":"8829bc556ca38bfec4add4f15a2f028092ac6d46","modified":1461824889094},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Muse/_menu.styl","hash":"c2c6c4f6434b4f94aac2af5861cd769427f0ee10","modified":1461824889152},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Muse/_search.styl","hash":"1452cbe674cc1d008e1e9640eb4283841058fc64","modified":1461824889208},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Muse/index.styl","hash":"8aca5d9f1df157ab27e699c1b3ba9438b9e039ad","modified":1461824889254},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Pisces/_brand.styl","hash":"be22ad34f546a07f6d56b424338cdd898683eea4","modified":1461824889398},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Pisces/_full-image.styl","hash":"938d39eedc6e3d33918c1145a5bf1e79991d3fcf","modified":1461824889437},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Pisces/_layout.styl","hash":"8d7cecde4933900c7df2db9d0a98f5f82f88dc93","modified":1461824889489},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Pisces/_menu.styl","hash":"d09280e5b79f3b573edb30f30c7a5f03ac640986","modified":1461824889564},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Pisces/_posts.styl","hash":"2f878213cb24c5ddc18877f6d15ec5c5f57745ac","modified":1461824889620},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Pisces/_sidebar.styl","hash":"cd2acd8c415c552e397af2da61d7659688e7aab4","modified":1461824889686},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Pisces/index.styl","hash":"1b10ba2d3ad0c063c418dc94a0b7e0db4b342c53","modified":1461824889756},{"_id":"themes/hexo-theme-next-5.0.1/source/js/src/schemes/pisces.js","hash":"7506e7490c69a200831393c38d25e91c156bd471","modified":1461824890980},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1461824891155},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1461824891116},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1461824891206},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1461824891264},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1461824891306},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1461824891345},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1461824891392},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/jquery.fancybox.js","hash":"1cf3d47b5ccb7cb6e9019c64f2a88d03a64853e4","modified":1461824891425},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1461824891458},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/lib/fastclick.js","hash":"06cef196733a710e77ad7e386ced6963f092dc55","modified":1461824891923},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fastclick/lib/fastclick.min.js","hash":"2cae0f5a6c5d6f3cb993015e6863f9483fc4de18","modified":1461824891959},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/css/font-awesome.css","hash":"3b87c2560832748cd06f9bfd2fd6ea8edbdae8c7","modified":1461824892276},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/css/font-awesome.css.map","hash":"0189d278706509412bac4745f96c83984e1d59f4","modified":1461824892318},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/css/font-awesome.min.css","hash":"05ea25bc9b3ac48993e1fee322d3bc94b49a6e22","modified":1461824892361},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.woff2","hash":"574ea2698c03ae9477db2ea3baf460ee32f1a7ea","modified":1461824892723},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/ua-parser-js/dist/ua-parser.min.js","hash":"38628e75e4412cc6f11074e03e1c6d257aae495b","modified":1461824892987},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/ua-parser-js/dist/ua-parser.pack.js","hash":"214dad442a92d36af77ed0ca1d9092b16687f02f","modified":1461824893015},{"_id":"themes/hexo-theme-next-5.0.1/source/upload/3/CNN.png","hash":"144304e313162d8c886be9dc0355b7a54b5f3488","modified":1468308891578},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/FontAwesome.otf","hash":"0112e96f327d413938d37c1693806f468ffdbace","modified":1461824892437},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.eot","hash":"b3c2f08e73320135b69c23a3908b87a12053a2f6","modified":1461824892486},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.woff","hash":"507970402e328b2baeb05bde73bf9ded4e2c3a2d","modified":1461824892594},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/velocity/velocity.js","hash":"e63dc7cea055ca60a95d286f32349d88b10c5a4d","modified":1461824893112},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/footer/footer.styl","hash":"8994ffcce84deac0471532f270f97c44fea54dc0","modified":1461824884588},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/header/header.styl","hash":"ae1ca14e51de67b07dba8f61ec79ee0e2e344574","modified":1461824884687},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/header/menu.styl","hash":"c890ce7fe933abad7baf39764a01894924854e92","modified":1461824884809},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/header/headerband.styl","hash":"d27448f199fc2f9980b601bc22b87f08b5d64dd1","modified":1461824884741},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/header/site-meta.styl","hash":"6c00f6e0978f4d8f9a846a15579963728aaa6a17","modified":1461824884911},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/header/site-nav.styl","hash":"49c2b2c14a1e7fcc810c6be4b632975d0204c281","modified":1461824884973},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/highlight/highlight.styl","hash":"e340071ee392f55b32c540d690198f157e588433","modified":1461824885047},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/highlight/theme.styl","hash":"90f8f9706cd7fe829cf06e9959a65fd3f8b994fa","modified":1461824885100},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/pages/archive.styl","hash":"23dd966324937deeccc8f5fa16a6d32e4e46243b","modified":1461824885160},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/pages/categories.styl","hash":"4eff5b252d7b614e500fc7d52c97ce325e57d3ab","modified":1461824885487},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/pages/pages.styl","hash":"3c46efd6601e268093ce6d7b1471d18501878f0d","modified":1461824885554},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/pages/post-detail.styl","hash":"9bf4362a4d0ae151ada84b219d39fbe5bb8c790e","modified":1461824885630},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-collapse.styl","hash":"8c0661a7ca4be8768324f62396a21e75560b4ce1","modified":1461824885697},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-eof.styl","hash":"2cdc094ecf907a02fce25ad4a607cd5c40da0f2b","modified":1461824885754},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-expand.styl","hash":"b25132fe6a7ad67059a2c3afc60feabb479bdd75","modified":1461824885807},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-gallery.styl","hash":"387ce23bba52b22a586b2dfb4ec618fe1ffd3926","modified":1461824885861},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-meta.styl","hash":"d543d1377c1f61b70e3adb6da0eb12797552e5f2","modified":1461824885974},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-more-link.styl","hash":"15063d79b5befc21820baf05d6f20cc1c1787477","modified":1461824886036},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-nav.styl","hash":"cbca4842a54950e2934b3b8f3cd940f122111aef","modified":1461824886111},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-tags.styl","hash":"a352ae5b1f8857393bf770d2e638bf15f0c9585d","modified":1461824886163},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-title.styl","hash":"99d994bd19895a4aaa2fa8d12801ec72dd3cf118","modified":1461824886217},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post-type.styl","hash":"10251257aceecb117233c9554dcf8ecfef8e2104","modified":1461824886270},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/post/post.styl","hash":"d4303e6439e70dc23f9151c69bc31a7fb0ece820","modified":1461824886321},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar-author-links.styl","hash":"2e7ec9aaa3293941106b1bdd09055246aa3c3dc6","modified":1461824886395},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar-author.styl","hash":"89fef1caf94caf76ca09c643b83b0b4d4e417e08","modified":1461824886447},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar-blogroll.styl","hash":"c44f6a553ec7ea5508f2054a13be33a62a15d3a9","modified":1461824886497},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar-feed-link.styl","hash":"9486ddd2cb255227db102d09a7df4cae0fabad72","modified":1461824886557},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar-toggle.styl","hash":"2d3abbc85b979a648e0e579e45f16a6eba49d1e7","modified":1461824886809},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar-nav.styl","hash":"45fa7193435a8eae9960267438750b4c9fa9587f","modified":1461824886639},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar-toc.styl","hash":"7690b9596ec3a49befbe529a5a2649abec0faf76","modified":1461824886727},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/sidebar.styl","hash":"234facd038f144bd0fe09a31ed1357c5d74c517f","modified":1461824886875},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/sidebar/site-state.styl","hash":"58fb7604b44e3f56d880bbbd95d0baface38c4ee","modified":1461824886928},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/tags/blockquote-center.styl","hash":"c2abe4d87148e23e15d49ee225bc650de60baf46","modified":1461824887018},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/tags/full-image.styl","hash":"618f73450cf541f88a4fddc3d22898aee49d105d","modified":1461824887072},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/tags/group-pictures.styl","hash":"4851b981020c5cbc354a1af9b831a2dcb3cf9d39","modified":1461824887130},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/tags/tags.styl","hash":"8e66c2635d48e11de616bb29c4b1323698eebc0a","modified":1461824887194},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/third-party/baidushare.styl","hash":"93b08815c4d17e2b96fef8530ec1f1064dede6ef","modified":1461824887297},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/third-party/duoshuo.styl","hash":"2340dd9b3202c61d73cc708b790fac5adddbfc7f","modified":1461824887384},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/third-party/jiathis.styl","hash":"327b5f63d55ec26f7663185c1a778440588d9803","modified":1461824887452},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/third-party/localsearch.styl","hash":"795d94561888d31cb7a6ff4a125596809ea69b7d","modified":1461824887521},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_common/components/third-party/third-party.styl","hash":"f7ebd428f3058ec3ecc7648788712617bea520ba","modified":1461824887644},{"_id":"themes/hexo-theme-next-5.0.1/source/css/_schemes/Mist/outline/outline.styl","hash":"5dc4859c66305f871e56cba78f64bfe3bf1b5f01","modified":1461824888966},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/fancybox_buttons.png","hash":"e385b139516c6813dcd64b8fc431c364ceafe5f3","modified":1461824891526},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.css","hash":"1a9d8e5c22b371fcc69d4dbbb823d9c39f04c0c8","modified":1461824891555},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-buttons.js","hash":"91e41741c2e93f732c82aaacec4cfc6e3f3ec876","modified":1461824891580},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-media.js","hash":"3bdf69ed2469e4fb57f5a95f17300eef891ff90d","modified":1461824891612},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.css","hash":"4ac329c16a5277592fc12a37cca3d72ca4ec292f","modified":1461824891648},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/fancybox/source/helpers/jquery.fancybox-thumbs.js","hash":"53e194f4a72e649c04fb586dd57762b8c022800b","modified":1461824891688},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.ttf","hash":"27cf1f2ec59aece6938c7bb2feb0e287ea778ff9","modified":1461824892567},{"_id":"themes/hexo-theme-next-5.0.1/source/vendors/font-awesome/fonts/fontawesome-webfont.svg","hash":"2b3c8ba7008cc014d8fb37abc6f9f49aeda83824","modified":1461824892528}],"Category":[{"name":"机器学习","_id":"ciqj5btoy0005c49nmjl508xs"}],"Data":[],"Page":[{"title":"categories","date":"2016-05-25T02:20:13.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2016-05-25 10:20:13\ntype: categories\ncomments: false\n---\n","updated":"2016-05-25T02:20:49.176Z","path":"categories/index.html","layout":"page","_id":"ciqj5btog0001c49nl5itesir","content":"","excerpt":"","more":""},{"title":"tags","date":"2016-05-25T02:18:40.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2016-05-25 10:18:40\ntype: \"tags\"\ncomments: false\n---\n","updated":"2016-05-25T02:19:25.236Z","path":"tags/index.html","layout":"page","_id":"ciqj5btoq0003c49nt7ueeiah","content":"","excerpt":"","more":""}],"Post":[{"title":"CNNs","date":"2015-11-12T06:40:39.000Z","_content":"# 导读\n本文主要介绍了卷积神经网络中的数学模型。\n# 简介\n卷积神经网络（Convolutional Neural Networks）来源于生物的视觉研究，最后应用于分类与模式识别当中。\n# 数学模型\n![卷积神经网络](/upload/3/CNN.png '卷积神经网络')\n# 正向传播\n# 反向传播\n","source":"_posts/CNNs.md","raw":"---\ntitle: CNNs\ndate: 2015-11-12 14:40:39\ncategories:\n- 机器学习\ntags:\n- 卷积神经网络\n- 正向传播\n- 反向传播\n- 深度学习\n- 数学推导\n- DL\n- CNNs\n- Backpropagation\n---\n# 导读\n本文主要介绍了卷积神经网络中的数学模型。\n# 简介\n卷积神经网络（Convolutional Neural Networks）来源于生物的视觉研究，最后应用于分类与模式识别当中。\n# 数学模型\n![卷积神经网络](/upload/3/CNN.png '卷积神经网络')\n# 正向传播\n# 反向传播\n","slug":"CNNs","published":1,"updated":"2016-07-12T07:37:47.406Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciqj5bto00000c49n4xlpw3b7","content":"<h1 id=\"导读\"><a href=\"#导读\" class=\"headerlink\" title=\"导读\"></a>导读</h1><p>本文主要介绍了卷积神经网络中的数学模型。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>卷积神经网络（Convolutional Neural Networks）来源于生物的视觉研究，最后应用于分类与模式识别当中。</p>\n<h1 id=\"数学模型\"><a href=\"#数学模型\" class=\"headerlink\" title=\"数学模型\"></a>数学模型</h1><p><img src=\"/upload/3/CNN.png\" alt=\"卷积神经网络\" title=\"卷积神经网络\"></p>\n<h1 id=\"正向传播\"><a href=\"#正向传播\" class=\"headerlink\" title=\"正向传播\"></a>正向传播</h1><h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1>","excerpt":"","more":"<h1 id=\"导读\"><a href=\"#导读\" class=\"headerlink\" title=\"导读\"></a>导读</h1><p>本文主要介绍了卷积神经网络中的数学模型。</p>\n<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>卷积神经网络（Convolutional Neural Networks）来源于生物的视觉研究，最后应用于分类与模式识别当中。</p>\n<h1 id=\"数学模型\"><a href=\"#数学模型\" class=\"headerlink\" title=\"数学模型\"></a>数学模型</h1><p><img src=\"/upload/3/CNN.png\" alt=\"卷积神经网络\" title=\"卷积神经网络\"></p>\n<h1 id=\"正向传播\"><a href=\"#正向传播\" class=\"headerlink\" title=\"正向传播\"></a>正向传播</h1><h1 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h1>"},{"title":"人工神经网络、反向传播算法和python3的简单实现","date":"2015-09-05T03:32:21.000Z","_content":"# 人工神经网络\n人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数据的模式。\n## 数学模型\n![人工神经网络](/upload/2/NN.png '4层人工神经网络')\n### 逻辑单元\n人工神经网络中，每个神经元上的逻辑单元是Sigmoid激励函数（Sigmoid activation function）或称逻辑激励函数（Logistic activation function）：\n$$g(x)=\\frac{1}{1+e^{-x}}$$\n其导数：\n$${g}'(x)=\\frac{e^{-x}}{(1+e^{-x})^2}=\\frac{1}{1+e^{-x}}-\\frac{1}{(1+e^{-x})^2}=y(1-y)$$\n### 变量定义\n$a\\_{i}^{(l)}$表示第$l$层第$i$个神经元（或称做激励单元（Activation Unit））的输出值。\n$\\mathbf{a}^{(l)}$表示$l$层输出值组成的向量。\n$w^{(l)}\\_{ij}$表示$l$层中的$i$神经元与$l+1$层中的$j$神经元的连接权值。\n$\\mathbf{w}^{(l)}\\_{j}$表示$l$层中各个神经元与$l+1$层中的$j$神经元的连接权值构成的向量。\n$W^{(l)}$表示第$l$层与$l+1$每条边构成的权值矩阵，$W^{(l)} \\in \\mathbb{R}^{\\dim(\\mathbf{a}^{(l)}) \\times \\dim(\\mathbf{a}^{(l+1)})}$。\n$z^{(l)}\\_{i}$表示$l$层中的$i$神经元的逻辑单元输入值。$z\\_{i}^{l}=\\sum\\_{j=1}^{\\dim(\\mathbf{a}^{(l-1)})}a\\_{j}w\\_{ji}$\n$\\mathbf{z}^{(l)}$表示$l$层中各个神经元的逻辑单元输入值。\n## 向前传播\n向前传播是指通过随机$W$来依次计算各层$\\mathbf{a}$值。对于任意$l$层的$\\mathbf{a}^{(l)}$可以通过$\\mathbf{a}^{(l-1)}$来进行计算。\n$$\\because a\\_{i}^{(l)} = g(z\\_{i}^{(l)})$$\n$$z\\_{i}^{(l)}={\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}\\_{i}^{(l-1)}$$\n$$\\therefore a\\_{i}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}\\_{i}^{(l-1)})$$\n### 向量化\n$$\\mathbf{a}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}W^{(l-1)}) $$\n## 反向传播\n### 变量定义\n$L$为最后一层。\n$\\eta$学习率。\n### 误差函数\n整体误差为：\n$$J(W)=\\frac{1}{2}\\sum\\_{i=1}^{\\dim(\\mathbf{a}^{(L)})}(a\\_{i} - y\\_{i})^2=\\frac{1}{2} \\|\\| \\mathbf{y} - \\mathbf{a}^{(L)}  \\|\\|^2$$\n### 梯度下降\n反向传播的学习方法是基于梯度下降方法。因为权值首先被初始化为随机值，然后向误差减小的方向调整。数学表达式：\n$$\\Delta W= - \\eta\\frac{\\partial J}{\\partial W}$$\n分量表示为：\n$$\\Delta w\\_{ij}^{(l)} = - \\eta\\frac{\\partial J}{\\partial w\\_{ij}^{(l)}}$$\n权值更新为：\n$$w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} + \\Delta w\\_{ij}^{(l)}$$\n### 变量定义\n$\\delta\\_{i}^{(l)}=-\\frac{\\partial J}{\\partial z\\_{i}^{(l)}}$表示第$l$层第$i$个神经元敏感度（sensitive）。\n$\\mathbf{\\delta}^{(l)}$表示第$l$层各个神经元错误构成的向量。\n### 权值更新\n$$w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} + \\Delta w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial w\\_{ij}^{(l)}}:=w\\_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial z\\_{j}^{(l+1)}}\\frac{\\partial z\\_{j}^{(l+1)}}{\\partial w\\_{ij}^{(l)}}$$\n$$\\because z\\_{j}^{(l+1)} = \\sum\\_{i=1}^{\\dim(\\mathbf{a}^{(l)})}a\\_{j}^{(l)}w\\_{ij}^{(l)}$$\n$$\\frac{\\partial z\\_{j}^{(l+1)}}{\\partial w\\_{ij}^{(l)}}=a\\_{i}^{(l)}$$\n$$\\because -\\frac{\\partial J}{\\partial z\\_{j}^{(l+1)}}=\\delta\\_{j}^{(l+1)}$$\n$$\\therefore w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} + \\eta\\delta\\_{j}^{(l+1)} a\\_{i}^{(l)}$$\n#### 向量化\n$$ W^{(l)} = \\sum\\_{i=1}^{\\dim(\\mathbf{a}^{(l)})}\\sum\\_{j=1}^{\\dim(\\mathbf{a}^{(l+1)})}(w\\_{ij}^{(l)} + \\eta\\delta\\_{j}^{(l+1)} a\\_{i}^{(l)})=W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$\n### 敏感度$\\delta$\n#### 一般式\n$$\\delta\\_{i}^{(l)}=-\\frac{\\partial J}{\\partial z\\_{i}^{(l)}}=-\\sum\\_{j}\\frac{\\partial J}{\\partial z\\_{j}^{(l+1)}}\\frac{\\partial z\\_{j}^{(l+1)}}{\\partial a\\_{i}^{(l)}}\\frac{\\partial a\\_{i}^{(l)}}{\\partial z\\_{i}^{(l)}}=\\sum\\_{j}[\\delta^{(l+1)}w\\_{ij}^{(l)}]{g}'(z\\_{i}^{(l)})$$\n#### 向量化\n$$\\delta^{l}=W^{(l)}\\delta^{(l+1)}\\odot{g}'(\\mathbf{z}^{(l)})$$\n### 反向传播实现\n#### $L$层的$\\delta^L$\n$$\\delta\\_{i}^{(L)}=-\\frac{\\partial J}{\\partial z\\_{i}^{(L)}}=-\\frac{\\partial J}{\\partial a\\_{i}^{(L)}}\\frac{\\partial a\\_{i}^{(L)}}{\\partial z\\_{i}^{(L)}}=(y\\_{i}-z\\_{i}^{(L)}){g}'(z\\_{i}^{(L)})=(y\\_{i}-z\\_{i}^{(L)})a^{(L)}\\_{i}(1-a^{(L)}\\_{i})$$\n##### 向量化\n$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}'(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{(L)}\\odot(1-\\mathbf{a}^{(L)})$$\n#### $L-1$层权值更新\n$$ \\delta^{(L-1)} = W^{(L-1)}\\delta^{(L)}\\odot{g}'(\\mathbf{z}^{(L-1)})=W^{(L-1)}\\odot(\\mathbf{a}^{(L-1)}\\odot(1-\\mathbf{a}^{(L-1)}))$$\n$$ W^{(L-1)} =W^{(L-1)}+\\eta \\mathbf{a}^{(L-1)}{\\mathbf{\\delta}^{(L)}}^{T}$$\n#### $l$层权值更新\n$$ \\delta^{(l)} = W^{(l)}\\delta^{(l+1)}\\odot{g}'(\\mathbf{z}^{(l)})=W^{(l)}\\odot(\\mathbf{a}^{(l)}\\odot(1-\\mathbf{a}^{(l)}))$$\n$$ W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$\n# python3 实现\n## 超简单3层XOR网络\n```python\n###三层异或门python实现\nimport numpy as np\n###训练集\nX = np.array([ [0,0,1,1], [0,1,0,1] ])\ny = np.array([[0,1,1,0]])\n### 权值初始化\nw1 = 2*np.random.random((2,3)) - 1\nw2 = 2*np.random.random((3,1)) - 1\n### 训练网络\nfor j in range(60000):\n    #正向传播\n    l1 = 1/(1+np.exp(-(np.dot(w1.T,X))))\n    l2 = 1/(1+np.exp(-(np.dot(w2.T,l1))))\n    #反向传播\n    l2_delta = (y - l2)*(l2*(1-l2))\n    l1_delta = w2.dot(l2_delta) * (l1 * (1-l1))\n    #权值更新\n    w2 += l1.dot(l2_delta.T)\n    w1 += X.dot(l1_delta.T)\n```\n## 通用人工神经网络实现\n```python\nimport numpy as np\n#Sigmoid函数\ndef Sigmoid(x):\n    y = 1/(1+np.exp(-x))\n    return(y)\nclass NN:\n    def __init__(self,X,Y,layer,eta):\n    '''NN(输入，y，网络结构列表，学习率)'''\n        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)\n    def train(self,times):\n        layerList = self.layer ##NN结构\n        self.ErrHis = []  #误差历史\n        weight = [] #权值\n        delta=[]  #敏感度\n        #权值初始化\n        for l in range(1,len(layerList)):\n            weight.append(2*np.random.random((layerList[l-1],layerList[l])) - 1)\n        #学习\n        for i in range(times):\n            #初始化\n            activation = []\n            activation.append(self.X)\n            #向前传播，获取激励\n            for l in range(len(weight)):\n                activation.append(Sigmoid(np.dot(weight[l].T,activation[l])))\n            #误差\n            err = 1/2 * sum((self.Y.T - activation[-1].T)**2)\n            self.ErrHis.append(err[0])\n            #敏感度反向传播\n            if delta: PreDelta = delta #上次敏感度保留\n            delta=[]  #初始化\n            A = [i for i in range(len(activation))] #反向列队\n            A.remove(0)\n            A.reverse()\n            for a in A:\n                if not delta: #最后一层\n                    delta.append((self.Y-activation[a])*(activation[a]*(1-activation[a])))\n                else: #其他层\n                    delta.append(weight[a].dot(delta[-1])*(activation[a]*(1-activation[a])))\n            delta.reverse() #正向化\n            PreWeight =weight #上次权值保留\n            #权值更新\n            for l in range(len(weight)):\n                weight[l] += self.eta*activation[l].dot(delta[l].T)\n        #结束\n        self.activation = activation\n        self.delta = PreDelta\n        self.weight = PreWeight\n    def predict(self,TEST): #预测\n        if not self.weight:\n            print('untrained NN')\n            return False\n        else:\n            PredictA=[]\n            PredictA.append(TEST)\n            for l in range(len(self.weight)):\n                PredictA.append(Sigmoid(np.dot(self.weight[l].T,PredictA[l])))\n            return(PredictA[-1])\n    def weight(self): #返回权值\n        return(self.weight)      \n    def errHis(self): #返回误差历史\n        return(self.ErrHis)\n```\n## 带偏置（Bias）节点的神经网络\n带偏置的神经网络，在第1层到$L-1$层都有着一个偏置节点；偏移节点有个特定，其只与下一层的非偏移节点相连。这样，$$\\mathbf{w}^{(l)}的维度=l层所有节点数\\times(l+1)层非偏置节点数$$\n![带偏置人工神经网络](/upload/2/NN_with_bias.png '4层带偏置人工神经网络')\n正向传播时:$$\\mathbf{a}^{(l+1)}\\_{no bias} = g(\\mathbf{w}^{(l)}\\mathbf{a}^{(l)}\\_{bias})$$\n反向传播时，第$L$层敏感度不变：$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}'(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{L}\\odot(1-\\mathbf{a}^{(L)})$$\n第$L-1$层敏感度：$$\\delta^{(L-1)}=W^{(L-1)}\\delta^{(L)}\\odot{g}'(\\mathbf{z}^{(L-1)})$$\n第$l$层的敏感度：$$\\delta^{(l)}=W^{(l)}\\delta^{(l)}\\_{no bias}\\odot{g}'(\\mathbf{z}^{(l)})$$\n权值更新：$$W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}\\_{nobias}^{T}$$\n## python3实现\n```python\nimport numpy as np\ndef Sigmoid(x):\n    y = 1/(1+np.exp(-x))\n    return(y)\nclass NN:\n    def __init__(self,X,Y,layer,eta):\n        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)\n    def train(self,times):\n        layerList = self.layer\n        self.ErrHis = []\n        weight = []\n        delta=[]\n        #权值初始化\n        for l in range(1,len(layerList)):\n            weight.append(2*np.random.random((layerList[l-1]+1,layerList[l])) - 1)\n        #学习\n        for i in range(times):\n            #初始化\n            activation = []\n            activation.append(self.X)\n            biasA = np.array([np.repeat(1,np.shape(X)[1])]) #bias\n            #向前传播，获取激励\n            for l in range(len(weight)):\n                activation.append(Sigmoid(np.dot(weight[l].T,np.concatenate((biasA,activation[l])))))\n            #误差\n            err = 1/2 * sum((self.Y.T - activation[-1].T)**2)\n            self.ErrHis.append(err[0])\n            #敏感度反向传播\n            if delta: PreDelta = delta\n            delta=[]\n            A = [i for i in range(len(activation))]\n            A.remove(0)\n            A.reverse()\n            for a in A:\n                if not delta: #最后一层\n                    delta.append((self.Y-activation[a])*(activation[a]*(1-activation[a])))\n                elif len(delta) == 1: #倒数第二层\n                        delta.append(weight[a].dot(delta[-1])*(np.concatenate((biasA,activation[a]))*(1-np.concatenate((biasA,activation[a])))))\n                else: #其他层\n                    delta.append(weight[a].dot(delta[-1][1:,:])*(np.concatenate((biasA,activation[a]))*(1-np.concatenate((biasA,activation[a])))))\n            #上一次权值保存，及权值更新\n            delta.reverse()\n            PreWeight =weight\n            for l in range(len(weight)):\n                if l == len(weight)-1:\n                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l].T)\n                else:\n                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l][1:,:].T)\n        #结束\n        self.activation = activation\n        self.delta = PreDelta\n        self.weight = PreWeight\n    def predict(self,TEST):\n        if not self.weight:\n            print('untrained NN')\n            return False\n        else:\n            PredictA=[]\n            biasA = np.array([np.repeat(1,np.shape(TEST)[1])])\n            PredictA.append(TEST)\n            for l in range(len(self.weight)):\n                PredictA.append(Sigmoid(np.dot(self.weight[l].T,np.concatenate((PredictA[l],biasA)))))\n            print(PredictA)\n            return(PredictA[-1])\n    def weight(self):\n        return(self.weight)\n    def errHis(self):\n        return(self.ErrHis)\n```\n","source":"_posts/人工神经网络、反向传播算法和python3的实现.md","raw":"---\ntitle: 人工神经网络、反向传播算法和python3的简单实现\ndate: 2015-09-05 11:32:21\ncategories:\n- 机器学习\ntags:\n- 神经网络\n- 反向传播\n- python3\n---\n# 人工神经网络\n人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数据的模式。\n## 数学模型\n![人工神经网络](/upload/2/NN.png '4层人工神经网络')\n### 逻辑单元\n人工神经网络中，每个神经元上的逻辑单元是Sigmoid激励函数（Sigmoid activation function）或称逻辑激励函数（Logistic activation function）：\n$$g(x)=\\frac{1}{1+e^{-x}}$$\n其导数：\n$${g}'(x)=\\frac{e^{-x}}{(1+e^{-x})^2}=\\frac{1}{1+e^{-x}}-\\frac{1}{(1+e^{-x})^2}=y(1-y)$$\n### 变量定义\n$a\\_{i}^{(l)}$表示第$l$层第$i$个神经元（或称做激励单元（Activation Unit））的输出值。\n$\\mathbf{a}^{(l)}$表示$l$层输出值组成的向量。\n$w^{(l)}\\_{ij}$表示$l$层中的$i$神经元与$l+1$层中的$j$神经元的连接权值。\n$\\mathbf{w}^{(l)}\\_{j}$表示$l$层中各个神经元与$l+1$层中的$j$神经元的连接权值构成的向量。\n$W^{(l)}$表示第$l$层与$l+1$每条边构成的权值矩阵，$W^{(l)} \\in \\mathbb{R}^{\\dim(\\mathbf{a}^{(l)}) \\times \\dim(\\mathbf{a}^{(l+1)})}$。\n$z^{(l)}\\_{i}$表示$l$层中的$i$神经元的逻辑单元输入值。$z\\_{i}^{l}=\\sum\\_{j=1}^{\\dim(\\mathbf{a}^{(l-1)})}a\\_{j}w\\_{ji}$\n$\\mathbf{z}^{(l)}$表示$l$层中各个神经元的逻辑单元输入值。\n## 向前传播\n向前传播是指通过随机$W$来依次计算各层$\\mathbf{a}$值。对于任意$l$层的$\\mathbf{a}^{(l)}$可以通过$\\mathbf{a}^{(l-1)}$来进行计算。\n$$\\because a\\_{i}^{(l)} = g(z\\_{i}^{(l)})$$\n$$z\\_{i}^{(l)}={\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}\\_{i}^{(l-1)}$$\n$$\\therefore a\\_{i}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}\\_{i}^{(l-1)})$$\n### 向量化\n$$\\mathbf{a}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}W^{(l-1)}) $$\n## 反向传播\n### 变量定义\n$L$为最后一层。\n$\\eta$学习率。\n### 误差函数\n整体误差为：\n$$J(W)=\\frac{1}{2}\\sum\\_{i=1}^{\\dim(\\mathbf{a}^{(L)})}(a\\_{i} - y\\_{i})^2=\\frac{1}{2} \\|\\| \\mathbf{y} - \\mathbf{a}^{(L)}  \\|\\|^2$$\n### 梯度下降\n反向传播的学习方法是基于梯度下降方法。因为权值首先被初始化为随机值，然后向误差减小的方向调整。数学表达式：\n$$\\Delta W= - \\eta\\frac{\\partial J}{\\partial W}$$\n分量表示为：\n$$\\Delta w\\_{ij}^{(l)} = - \\eta\\frac{\\partial J}{\\partial w\\_{ij}^{(l)}}$$\n权值更新为：\n$$w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} + \\Delta w\\_{ij}^{(l)}$$\n### 变量定义\n$\\delta\\_{i}^{(l)}=-\\frac{\\partial J}{\\partial z\\_{i}^{(l)}}$表示第$l$层第$i$个神经元敏感度（sensitive）。\n$\\mathbf{\\delta}^{(l)}$表示第$l$层各个神经元错误构成的向量。\n### 权值更新\n$$w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} + \\Delta w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial w\\_{ij}^{(l)}}:=w\\_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial z\\_{j}^{(l+1)}}\\frac{\\partial z\\_{j}^{(l+1)}}{\\partial w\\_{ij}^{(l)}}$$\n$$\\because z\\_{j}^{(l+1)} = \\sum\\_{i=1}^{\\dim(\\mathbf{a}^{(l)})}a\\_{j}^{(l)}w\\_{ij}^{(l)}$$\n$$\\frac{\\partial z\\_{j}^{(l+1)}}{\\partial w\\_{ij}^{(l)}}=a\\_{i}^{(l)}$$\n$$\\because -\\frac{\\partial J}{\\partial z\\_{j}^{(l+1)}}=\\delta\\_{j}^{(l+1)}$$\n$$\\therefore w\\_{ij}^{(l)}:=w\\_{ij}^{(l)} + \\eta\\delta\\_{j}^{(l+1)} a\\_{i}^{(l)}$$\n#### 向量化\n$$ W^{(l)} = \\sum\\_{i=1}^{\\dim(\\mathbf{a}^{(l)})}\\sum\\_{j=1}^{\\dim(\\mathbf{a}^{(l+1)})}(w\\_{ij}^{(l)} + \\eta\\delta\\_{j}^{(l+1)} a\\_{i}^{(l)})=W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$\n### 敏感度$\\delta$\n#### 一般式\n$$\\delta\\_{i}^{(l)}=-\\frac{\\partial J}{\\partial z\\_{i}^{(l)}}=-\\sum\\_{j}\\frac{\\partial J}{\\partial z\\_{j}^{(l+1)}}\\frac{\\partial z\\_{j}^{(l+1)}}{\\partial a\\_{i}^{(l)}}\\frac{\\partial a\\_{i}^{(l)}}{\\partial z\\_{i}^{(l)}}=\\sum\\_{j}[\\delta^{(l+1)}w\\_{ij}^{(l)}]{g}'(z\\_{i}^{(l)})$$\n#### 向量化\n$$\\delta^{l}=W^{(l)}\\delta^{(l+1)}\\odot{g}'(\\mathbf{z}^{(l)})$$\n### 反向传播实现\n#### $L$层的$\\delta^L$\n$$\\delta\\_{i}^{(L)}=-\\frac{\\partial J}{\\partial z\\_{i}^{(L)}}=-\\frac{\\partial J}{\\partial a\\_{i}^{(L)}}\\frac{\\partial a\\_{i}^{(L)}}{\\partial z\\_{i}^{(L)}}=(y\\_{i}-z\\_{i}^{(L)}){g}'(z\\_{i}^{(L)})=(y\\_{i}-z\\_{i}^{(L)})a^{(L)}\\_{i}(1-a^{(L)}\\_{i})$$\n##### 向量化\n$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}'(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{(L)}\\odot(1-\\mathbf{a}^{(L)})$$\n#### $L-1$层权值更新\n$$ \\delta^{(L-1)} = W^{(L-1)}\\delta^{(L)}\\odot{g}'(\\mathbf{z}^{(L-1)})=W^{(L-1)}\\odot(\\mathbf{a}^{(L-1)}\\odot(1-\\mathbf{a}^{(L-1)}))$$\n$$ W^{(L-1)} =W^{(L-1)}+\\eta \\mathbf{a}^{(L-1)}{\\mathbf{\\delta}^{(L)}}^{T}$$\n#### $l$层权值更新\n$$ \\delta^{(l)} = W^{(l)}\\delta^{(l+1)}\\odot{g}'(\\mathbf{z}^{(l)})=W^{(l)}\\odot(\\mathbf{a}^{(l)}\\odot(1-\\mathbf{a}^{(l)}))$$\n$$ W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$\n# python3 实现\n## 超简单3层XOR网络\n```python\n###三层异或门python实现\nimport numpy as np\n###训练集\nX = np.array([ [0,0,1,1], [0,1,0,1] ])\ny = np.array([[0,1,1,0]])\n### 权值初始化\nw1 = 2*np.random.random((2,3)) - 1\nw2 = 2*np.random.random((3,1)) - 1\n### 训练网络\nfor j in range(60000):\n    #正向传播\n    l1 = 1/(1+np.exp(-(np.dot(w1.T,X))))\n    l2 = 1/(1+np.exp(-(np.dot(w2.T,l1))))\n    #反向传播\n    l2_delta = (y - l2)*(l2*(1-l2))\n    l1_delta = w2.dot(l2_delta) * (l1 * (1-l1))\n    #权值更新\n    w2 += l1.dot(l2_delta.T)\n    w1 += X.dot(l1_delta.T)\n```\n## 通用人工神经网络实现\n```python\nimport numpy as np\n#Sigmoid函数\ndef Sigmoid(x):\n    y = 1/(1+np.exp(-x))\n    return(y)\nclass NN:\n    def __init__(self,X,Y,layer,eta):\n    '''NN(输入，y，网络结构列表，学习率)'''\n        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)\n    def train(self,times):\n        layerList = self.layer ##NN结构\n        self.ErrHis = []  #误差历史\n        weight = [] #权值\n        delta=[]  #敏感度\n        #权值初始化\n        for l in range(1,len(layerList)):\n            weight.append(2*np.random.random((layerList[l-1],layerList[l])) - 1)\n        #学习\n        for i in range(times):\n            #初始化\n            activation = []\n            activation.append(self.X)\n            #向前传播，获取激励\n            for l in range(len(weight)):\n                activation.append(Sigmoid(np.dot(weight[l].T,activation[l])))\n            #误差\n            err = 1/2 * sum((self.Y.T - activation[-1].T)**2)\n            self.ErrHis.append(err[0])\n            #敏感度反向传播\n            if delta: PreDelta = delta #上次敏感度保留\n            delta=[]  #初始化\n            A = [i for i in range(len(activation))] #反向列队\n            A.remove(0)\n            A.reverse()\n            for a in A:\n                if not delta: #最后一层\n                    delta.append((self.Y-activation[a])*(activation[a]*(1-activation[a])))\n                else: #其他层\n                    delta.append(weight[a].dot(delta[-1])*(activation[a]*(1-activation[a])))\n            delta.reverse() #正向化\n            PreWeight =weight #上次权值保留\n            #权值更新\n            for l in range(len(weight)):\n                weight[l] += self.eta*activation[l].dot(delta[l].T)\n        #结束\n        self.activation = activation\n        self.delta = PreDelta\n        self.weight = PreWeight\n    def predict(self,TEST): #预测\n        if not self.weight:\n            print('untrained NN')\n            return False\n        else:\n            PredictA=[]\n            PredictA.append(TEST)\n            for l in range(len(self.weight)):\n                PredictA.append(Sigmoid(np.dot(self.weight[l].T,PredictA[l])))\n            return(PredictA[-1])\n    def weight(self): #返回权值\n        return(self.weight)      \n    def errHis(self): #返回误差历史\n        return(self.ErrHis)\n```\n## 带偏置（Bias）节点的神经网络\n带偏置的神经网络，在第1层到$L-1$层都有着一个偏置节点；偏移节点有个特定，其只与下一层的非偏移节点相连。这样，$$\\mathbf{w}^{(l)}的维度=l层所有节点数\\times(l+1)层非偏置节点数$$\n![带偏置人工神经网络](/upload/2/NN_with_bias.png '4层带偏置人工神经网络')\n正向传播时:$$\\mathbf{a}^{(l+1)}\\_{no bias} = g(\\mathbf{w}^{(l)}\\mathbf{a}^{(l)}\\_{bias})$$\n反向传播时，第$L$层敏感度不变：$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}'(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{L}\\odot(1-\\mathbf{a}^{(L)})$$\n第$L-1$层敏感度：$$\\delta^{(L-1)}=W^{(L-1)}\\delta^{(L)}\\odot{g}'(\\mathbf{z}^{(L-1)})$$\n第$l$层的敏感度：$$\\delta^{(l)}=W^{(l)}\\delta^{(l)}\\_{no bias}\\odot{g}'(\\mathbf{z}^{(l)})$$\n权值更新：$$W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}\\_{nobias}^{T}$$\n## python3实现\n```python\nimport numpy as np\ndef Sigmoid(x):\n    y = 1/(1+np.exp(-x))\n    return(y)\nclass NN:\n    def __init__(self,X,Y,layer,eta):\n        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)\n    def train(self,times):\n        layerList = self.layer\n        self.ErrHis = []\n        weight = []\n        delta=[]\n        #权值初始化\n        for l in range(1,len(layerList)):\n            weight.append(2*np.random.random((layerList[l-1]+1,layerList[l])) - 1)\n        #学习\n        for i in range(times):\n            #初始化\n            activation = []\n            activation.append(self.X)\n            biasA = np.array([np.repeat(1,np.shape(X)[1])]) #bias\n            #向前传播，获取激励\n            for l in range(len(weight)):\n                activation.append(Sigmoid(np.dot(weight[l].T,np.concatenate((biasA,activation[l])))))\n            #误差\n            err = 1/2 * sum((self.Y.T - activation[-1].T)**2)\n            self.ErrHis.append(err[0])\n            #敏感度反向传播\n            if delta: PreDelta = delta\n            delta=[]\n            A = [i for i in range(len(activation))]\n            A.remove(0)\n            A.reverse()\n            for a in A:\n                if not delta: #最后一层\n                    delta.append((self.Y-activation[a])*(activation[a]*(1-activation[a])))\n                elif len(delta) == 1: #倒数第二层\n                        delta.append(weight[a].dot(delta[-1])*(np.concatenate((biasA,activation[a]))*(1-np.concatenate((biasA,activation[a])))))\n                else: #其他层\n                    delta.append(weight[a].dot(delta[-1][1:,:])*(np.concatenate((biasA,activation[a]))*(1-np.concatenate((biasA,activation[a])))))\n            #上一次权值保存，及权值更新\n            delta.reverse()\n            PreWeight =weight\n            for l in range(len(weight)):\n                if l == len(weight)-1:\n                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l].T)\n                else:\n                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l][1:,:].T)\n        #结束\n        self.activation = activation\n        self.delta = PreDelta\n        self.weight = PreWeight\n    def predict(self,TEST):\n        if not self.weight:\n            print('untrained NN')\n            return False\n        else:\n            PredictA=[]\n            biasA = np.array([np.repeat(1,np.shape(TEST)[1])])\n            PredictA.append(TEST)\n            for l in range(len(self.weight)):\n                PredictA.append(Sigmoid(np.dot(self.weight[l].T,np.concatenate((PredictA[l],biasA)))))\n            print(PredictA)\n            return(PredictA[-1])\n    def weight(self):\n        return(self.weight)\n    def errHis(self):\n        return(self.ErrHis)\n```\n","slug":"人工神经网络、反向传播算法和python3的实现","published":1,"updated":"2016-07-01T08:36:16.417Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciqj5btoi0002c49nhfshy03s","content":"<h1 id=\"人工神经网络\"><a href=\"#人工神经网络\" class=\"headerlink\" title=\"人工神经网络\"></a>人工神经网络</h1><p>人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数据的模式。</p>\n<h2 id=\"数学模型\"><a href=\"#数学模型\" class=\"headerlink\" title=\"数学模型\"></a>数学模型</h2><p><img src=\"/upload/2/NN.png\" alt=\"人工神经网络\" title=\"4层人工神经网络\"></p>\n<h3 id=\"逻辑单元\"><a href=\"#逻辑单元\" class=\"headerlink\" title=\"逻辑单元\"></a>逻辑单元</h3><p>人工神经网络中，每个神经元上的逻辑单元是Sigmoid激励函数（Sigmoid activation function）或称逻辑激励函数（Logistic activation function）：<br>$$g(x)=\\frac{1}{1+e^{-x}}$$<br>其导数：<br>$${g}’(x)=\\frac{e^{-x}}{(1+e^{-x})^2}=\\frac{1}{1+e^{-x}}-\\frac{1}{(1+e^{-x})^2}=y(1-y)$$</p>\n<h3 id=\"变量定义\"><a href=\"#变量定义\" class=\"headerlink\" title=\"变量定义\"></a>变量定义</h3><p>$a_{i}^{(l)}$表示第$l$层第$i$个神经元（或称做激励单元（Activation Unit））的输出值。<br>$\\mathbf{a}^{(l)}$表示$l$层输出值组成的向量。<br>$w^{(l)}_{ij}$表示$l$层中的$i$神经元与$l+1$层中的$j$神经元的连接权值。<br>$\\mathbf{w}^{(l)}_{j}$表示$l$层中各个神经元与$l+1$层中的$j$神经元的连接权值构成的向量。<br>$W^{(l)}$表示第$l$层与$l+1$每条边构成的权值矩阵，$W^{(l)} \\in \\mathbb{R}^{\\dim(\\mathbf{a}^{(l)}) \\times \\dim(\\mathbf{a}^{(l+1)})}$。<br>$z^{(l)}_{i}$表示$l$层中的$i$神经元的逻辑单元输入值。$z_{i}^{l}=\\sum_{j=1}^{\\dim(\\mathbf{a}^{(l-1)})}a_{j}w_{ji}$<br>$\\mathbf{z}^{(l)}$表示$l$层中各个神经元的逻辑单元输入值。</p>\n<h2 id=\"向前传播\"><a href=\"#向前传播\" class=\"headerlink\" title=\"向前传播\"></a>向前传播</h2><p>向前传播是指通过随机$W$来依次计算各层$\\mathbf{a}$值。对于任意$l$层的$\\mathbf{a}^{(l)}$可以通过$\\mathbf{a}^{(l-1)}$来进行计算。<br>$$\\because a_{i}^{(l)} = g(z_{i}^{(l)})$$<br>$$z_{i}^{(l)}={\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}_{i}^{(l-1)}$$<br>$$\\therefore a_{i}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}_{i}^{(l-1)})$$</p>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><p>$$\\mathbf{a}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}W^{(l-1)}) $$</p>\n<h2 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h2><h3 id=\"变量定义-1\"><a href=\"#变量定义-1\" class=\"headerlink\" title=\"变量定义\"></a>变量定义</h3><p>$L$为最后一层。<br>$\\eta$学习率。</p>\n<h3 id=\"误差函数\"><a href=\"#误差函数\" class=\"headerlink\" title=\"误差函数\"></a>误差函数</h3><p>整体误差为：<br>$$J(W)=\\frac{1}{2}\\sum_{i=1}^{\\dim(\\mathbf{a}^{(L)})}(a_{i} - y_{i})^2=\\frac{1}{2} || \\mathbf{y} - \\mathbf{a}^{(L)}  ||^2$$</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>反向传播的学习方法是基于梯度下降方法。因为权值首先被初始化为随机值，然后向误差减小的方向调整。数学表达式：<br>$$\\Delta W= - \\eta\\frac{\\partial J}{\\partial W}$$<br>分量表示为：<br>$$\\Delta w_{ij}^{(l)} = - \\eta\\frac{\\partial J}{\\partial w_{ij}^{(l)}}$$<br>权值更新为：<br>$$w_{ij}^{(l)}:=w_{ij}^{(l)} + \\Delta w_{ij}^{(l)}$$</p>\n<h3 id=\"变量定义-2\"><a href=\"#变量定义-2\" class=\"headerlink\" title=\"变量定义\"></a>变量定义</h3><p>$\\delta_{i}^{(l)}=-\\frac{\\partial J}{\\partial z_{i}^{(l)}}$表示第$l$层第$i$个神经元敏感度（sensitive）。<br>$\\mathbf{\\delta}^{(l)}$表示第$l$层各个神经元错误构成的向量。</p>\n<h3 id=\"权值更新\"><a href=\"#权值更新\" class=\"headerlink\" title=\"权值更新\"></a>权值更新</h3><p>$$w_{ij}^{(l)}:=w_{ij}^{(l)} + \\Delta w_{ij}^{(l)}:=w_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial w_{ij}^{(l)}}:=w_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial z_{j}^{(l+1)}}\\frac{\\partial z_{j}^{(l+1)}}{\\partial w_{ij}^{(l)}}$$<br>$$\\because z_{j}^{(l+1)} = \\sum_{i=1}^{\\dim(\\mathbf{a}^{(l)})}a_{j}^{(l)}w_{ij}^{(l)}$$<br>$$\\frac{\\partial z_{j}^{(l+1)}}{\\partial w_{ij}^{(l)}}=a_{i}^{(l)}$$<br>$$\\because -\\frac{\\partial J}{\\partial z_{j}^{(l+1)}}=\\delta_{j}^{(l+1)}$$<br>$$\\therefore w_{ij}^{(l)}:=w_{ij}^{(l)} + \\eta\\delta_{j}^{(l+1)} a_{i}^{(l)}$$</p>\n<h4 id=\"向量化-1\"><a href=\"#向量化-1\" class=\"headerlink\" title=\"向量化\"></a>向量化</h4><p>$$ W^{(l)} = \\sum_{i=1}^{\\dim(\\mathbf{a}^{(l)})}\\sum_{j=1}^{\\dim(\\mathbf{a}^{(l+1)})}(w_{ij}^{(l)} + \\eta\\delta_{j}^{(l+1)} a_{i}^{(l)})=W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$</p>\n<h3 id=\"敏感度-delta\"><a href=\"#敏感度-delta\" class=\"headerlink\" title=\"敏感度$\\delta$\"></a>敏感度$\\delta$</h3><h4 id=\"一般式\"><a href=\"#一般式\" class=\"headerlink\" title=\"一般式\"></a>一般式</h4><p>$$\\delta_{i}^{(l)}=-\\frac{\\partial J}{\\partial z_{i}^{(l)}}=-\\sum_{j}\\frac{\\partial J}{\\partial z_{j}^{(l+1)}}\\frac{\\partial z_{j}^{(l+1)}}{\\partial a_{i}^{(l)}}\\frac{\\partial a_{i}^{(l)}}{\\partial z_{i}^{(l)}}=\\sum_{j}[\\delta^{(l+1)}w_{ij}^{(l)}]{g}’(z_{i}^{(l)})$$</p>\n<h4 id=\"向量化-2\"><a href=\"#向量化-2\" class=\"headerlink\" title=\"向量化\"></a>向量化</h4><p>$$\\delta^{l}=W^{(l)}\\delta^{(l+1)}\\odot{g}’(\\mathbf{z}^{(l)})$$</p>\n<h3 id=\"反向传播实现\"><a href=\"#反向传播实现\" class=\"headerlink\" title=\"反向传播实现\"></a>反向传播实现</h3><h4 id=\"L-层的-delta-L\"><a href=\"#L-层的-delta-L\" class=\"headerlink\" title=\"$L$层的$\\delta^L$\"></a>$L$层的$\\delta^L$</h4><p>$$\\delta_{i}^{(L)}=-\\frac{\\partial J}{\\partial z_{i}^{(L)}}=-\\frac{\\partial J}{\\partial a_{i}^{(L)}}\\frac{\\partial a_{i}^{(L)}}{\\partial z_{i}^{(L)}}=(y_{i}-z_{i}^{(L)}){g}’(z_{i}^{(L)})=(y_{i}-z_{i}^{(L)})a^{(L)}_{i}(1-a^{(L)}_{i})$$</p>\n<h5 id=\"向量化-3\"><a href=\"#向量化-3\" class=\"headerlink\" title=\"向量化\"></a>向量化</h5><p>$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}’(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{(L)}\\odot(1-\\mathbf{a}^{(L)})$$</p>\n<h4 id=\"L-1-层权值更新\"><a href=\"#L-1-层权值更新\" class=\"headerlink\" title=\"$L-1$层权值更新\"></a>$L-1$层权值更新</h4><p>$$ \\delta^{(L-1)} = W^{(L-1)}\\delta^{(L)}\\odot{g}’(\\mathbf{z}^{(L-1)})=W^{(L-1)}\\odot(\\mathbf{a}^{(L-1)}\\odot(1-\\mathbf{a}^{(L-1)}))$$<br>$$ W^{(L-1)} =W^{(L-1)}+\\eta \\mathbf{a}^{(L-1)}{\\mathbf{\\delta}^{(L)}}^{T}$$</p>\n<h4 id=\"l-层权值更新\"><a href=\"#l-层权值更新\" class=\"headerlink\" title=\"$l$层权值更新\"></a>$l$层权值更新</h4><p>$$ \\delta^{(l)} = W^{(l)}\\delta^{(l+1)}\\odot{g}’(\\mathbf{z}^{(l)})=W^{(l)}\\odot(\\mathbf{a}^{(l)}\\odot(1-\\mathbf{a}^{(l)}))$$<br>$$ W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$</p>\n<h1 id=\"python3-实现\"><a href=\"#python3-实现\" class=\"headerlink\" title=\"python3 实现\"></a>python3 实现</h1><h2 id=\"超简单3层XOR网络\"><a href=\"#超简单3层XOR网络\" class=\"headerlink\" title=\"超简单3层XOR网络\"></a>超简单3层XOR网络</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">###三层异或门python实现</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">###训练集</span></span><br><span class=\"line\">X = np.array([ [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>], [<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>] ])</span><br><span class=\"line\">y = np.array([[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]])</span><br><span class=\"line\"><span class=\"comment\">### 权值初始化</span></span><br><span class=\"line\">w1 = <span class=\"number\">2</span>*np.random.random((<span class=\"number\">2</span>,<span class=\"number\">3</span>)) - <span class=\"number\">1</span></span><br><span class=\"line\">w2 = <span class=\"number\">2</span>*np.random.random((<span class=\"number\">3</span>,<span class=\"number\">1</span>)) - <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\">### 训练网络</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">60000</span>):</span><br><span class=\"line\">    <span class=\"comment\">#正向传播</span></span><br><span class=\"line\">    l1 = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-(np.dot(w1.T,X))))</span><br><span class=\"line\">    l2 = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-(np.dot(w2.T,l1))))</span><br><span class=\"line\">    <span class=\"comment\">#反向传播</span></span><br><span class=\"line\">    l2_delta = (y - l2)*(l2*(<span class=\"number\">1</span>-l2))</span><br><span class=\"line\">    l1_delta = w2.dot(l2_delta) * (l1 * (<span class=\"number\">1</span>-l1))</span><br><span class=\"line\">    <span class=\"comment\">#权值更新</span></span><br><span class=\"line\">    w2 += l1.dot(l2_delta.T)</span><br><span class=\"line\">    w1 += X.dot(l1_delta.T)</span><br></pre></td></tr></table></figure>\n<h2 id=\"通用人工神经网络实现\"><a href=\"#通用人工神经网络实现\" class=\"headerlink\" title=\"通用人工神经网络实现\"></a>通用人工神经网络实现</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">#Sigmoid函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Sigmoid</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    y = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span>(y)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NN</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,X,Y,layer,eta)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">'''NN(输入，y，网络结构列表，学习率)'''</span></span><br><span class=\"line\">        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self,times)</span>:</span></span><br><span class=\"line\">        layerList = self.layer <span class=\"comment\">##NN结构</span></span><br><span class=\"line\">        self.ErrHis = []  <span class=\"comment\">#误差历史</span></span><br><span class=\"line\">        weight = [] <span class=\"comment\">#权值</span></span><br><span class=\"line\">        delta=[]  <span class=\"comment\">#敏感度</span></span><br><span class=\"line\">        <span class=\"comment\">#权值初始化</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(layerList)):</span><br><span class=\"line\">            weight.append(<span class=\"number\">2</span>*np.random.random((layerList[l<span class=\"number\">-1</span>],layerList[l])) - <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\">#学习</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(times):</span><br><span class=\"line\">            <span class=\"comment\">#初始化</span></span><br><span class=\"line\">            activation = []</span><br><span class=\"line\">            activation.append(self.X)</span><br><span class=\"line\">            <span class=\"comment\">#向前传播，获取激励</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                activation.append(Sigmoid(np.dot(weight[l].T,activation[l])))</span><br><span class=\"line\">            <span class=\"comment\">#误差</span></span><br><span class=\"line\">            err = <span class=\"number\">1</span>/<span class=\"number\">2</span> * sum((self.Y.T - activation[<span class=\"number\">-1</span>].T)**<span class=\"number\">2</span>)</span><br><span class=\"line\">            self.ErrHis.append(err[<span class=\"number\">0</span>])</span><br><span class=\"line\">            <span class=\"comment\">#敏感度反向传播</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> delta: PreDelta = delta <span class=\"comment\">#上次敏感度保留</span></span><br><span class=\"line\">            delta=[]  <span class=\"comment\">#初始化</span></span><br><span class=\"line\">            A = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(activation))] <span class=\"comment\">#反向列队</span></span><br><span class=\"line\">            A.remove(<span class=\"number\">0</span>)</span><br><span class=\"line\">            A.reverse()</span><br><span class=\"line\">            <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> delta: <span class=\"comment\">#最后一层</span></span><br><span class=\"line\">                    delta.append((self.Y-activation[a])*(activation[a]*(<span class=\"number\">1</span>-activation[a])))</span><br><span class=\"line\">                <span class=\"keyword\">else</span>: <span class=\"comment\">#其他层</span></span><br><span class=\"line\">                    delta.append(weight[a].dot(delta[<span class=\"number\">-1</span>])*(activation[a]*(<span class=\"number\">1</span>-activation[a])))</span><br><span class=\"line\">            delta.reverse() <span class=\"comment\">#正向化</span></span><br><span class=\"line\">            PreWeight =weight <span class=\"comment\">#上次权值保留</span></span><br><span class=\"line\">            <span class=\"comment\">#权值更新</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                weight[l] += self.eta*activation[l].dot(delta[l].T)</span><br><span class=\"line\">        <span class=\"comment\">#结束</span></span><br><span class=\"line\">        self.activation = activation</span><br><span class=\"line\">        self.delta = PreDelta</span><br><span class=\"line\">        self.weight = PreWeight</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self,TEST)</span>:</span> <span class=\"comment\">#预测</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.weight:</span><br><span class=\"line\">            print(<span class=\"string\">'untrained NN'</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            PredictA=[]</span><br><span class=\"line\">            PredictA.append(TEST)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(self.weight)):</span><br><span class=\"line\">                PredictA.append(Sigmoid(np.dot(self.weight[l].T,PredictA[l])))</span><br><span class=\"line\">            <span class=\"keyword\">return</span>(PredictA[<span class=\"number\">-1</span>])</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight</span><span class=\"params\">(self)</span>:</span> <span class=\"comment\">#返回权值</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.weight)      </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">errHis</span><span class=\"params\">(self)</span>:</span> <span class=\"comment\">#返回误差历史</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.ErrHis)</span><br></pre></td></tr></table></figure>\n<h2 id=\"带偏置（Bias）节点的神经网络\"><a href=\"#带偏置（Bias）节点的神经网络\" class=\"headerlink\" title=\"带偏置（Bias）节点的神经网络\"></a>带偏置（Bias）节点的神经网络</h2><p>带偏置的神经网络，在第1层到$L-1$层都有着一个偏置节点；偏移节点有个特定，其只与下一层的非偏移节点相连。这样，$$\\mathbf{w}^{(l)}的维度=l层所有节点数\\times(l+1)层非偏置节点数$$<br><img src=\"/upload/2/NN_with_bias.png\" alt=\"带偏置人工神经网络\" title=\"4层带偏置人工神经网络\"><br>正向传播时:$$\\mathbf{a}^{(l+1)}_{no bias} = g(\\mathbf{w}^{(l)}\\mathbf{a}^{(l)}_{bias})$$<br>反向传播时，第$L$层敏感度不变：$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}’(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{L}\\odot(1-\\mathbf{a}^{(L)})$$<br>第$L-1$层敏感度：$$\\delta^{(L-1)}=W^{(L-1)}\\delta^{(L)}\\odot{g}’(\\mathbf{z}^{(L-1)})$$<br>第$l$层的敏感度：$$\\delta^{(l)}=W^{(l)}\\delta^{(l)}_{no bias}\\odot{g}’(\\mathbf{z}^{(l)})$$<br>权值更新：$$W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}_{nobias}^{T}$$</p>\n<h2 id=\"python3实现\"><a href=\"#python3实现\" class=\"headerlink\" title=\"python3实现\"></a>python3实现</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Sigmoid</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    y = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span>(y)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NN</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,X,Y,layer,eta)</span>:</span></span><br><span class=\"line\">        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self,times)</span>:</span></span><br><span class=\"line\">        layerList = self.layer</span><br><span class=\"line\">        self.ErrHis = []</span><br><span class=\"line\">        weight = []</span><br><span class=\"line\">        delta=[]</span><br><span class=\"line\">        <span class=\"comment\">#权值初始化</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(layerList)):</span><br><span class=\"line\">            weight.append(<span class=\"number\">2</span>*np.random.random((layerList[l<span class=\"number\">-1</span>]+<span class=\"number\">1</span>,layerList[l])) - <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\">#学习</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(times):</span><br><span class=\"line\">            <span class=\"comment\">#初始化</span></span><br><span class=\"line\">            activation = []</span><br><span class=\"line\">            activation.append(self.X)</span><br><span class=\"line\">            biasA = np.array([np.repeat(<span class=\"number\">1</span>,np.shape(X)[<span class=\"number\">1</span>])]) <span class=\"comment\">#bias</span></span><br><span class=\"line\">            <span class=\"comment\">#向前传播，获取激励</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                activation.append(Sigmoid(np.dot(weight[l].T,np.concatenate((biasA,activation[l])))))</span><br><span class=\"line\">            <span class=\"comment\">#误差</span></span><br><span class=\"line\">            err = <span class=\"number\">1</span>/<span class=\"number\">2</span> * sum((self.Y.T - activation[<span class=\"number\">-1</span>].T)**<span class=\"number\">2</span>)</span><br><span class=\"line\">            self.ErrHis.append(err[<span class=\"number\">0</span>])</span><br><span class=\"line\">            <span class=\"comment\">#敏感度反向传播</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> delta: PreDelta = delta</span><br><span class=\"line\">            delta=[]</span><br><span class=\"line\">            A = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(activation))]</span><br><span class=\"line\">            A.remove(<span class=\"number\">0</span>)</span><br><span class=\"line\">            A.reverse()</span><br><span class=\"line\">            <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> delta: <span class=\"comment\">#最后一层</span></span><br><span class=\"line\">                    delta.append((self.Y-activation[a])*(activation[a]*(<span class=\"number\">1</span>-activation[a])))</span><br><span class=\"line\">                <span class=\"keyword\">elif</span> len(delta) == <span class=\"number\">1</span>: <span class=\"comment\">#倒数第二层</span></span><br><span class=\"line\">                        delta.append(weight[a].dot(delta[<span class=\"number\">-1</span>])*(np.concatenate((biasA,activation[a]))*(<span class=\"number\">1</span>-np.concatenate((biasA,activation[a])))))</span><br><span class=\"line\">                <span class=\"keyword\">else</span>: <span class=\"comment\">#其他层</span></span><br><span class=\"line\">                    delta.append(weight[a].dot(delta[<span class=\"number\">-1</span>][<span class=\"number\">1</span>:,:])*(np.concatenate((biasA,activation[a]))*(<span class=\"number\">1</span>-np.concatenate((biasA,activation[a])))))</span><br><span class=\"line\">            <span class=\"comment\">#上一次权值保存，及权值更新</span></span><br><span class=\"line\">            delta.reverse()</span><br><span class=\"line\">            PreWeight =weight</span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> l == len(weight)<span class=\"number\">-1</span>:</span><br><span class=\"line\">                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l].T)</span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l][<span class=\"number\">1</span>:,:].T)</span><br><span class=\"line\">        <span class=\"comment\">#结束</span></span><br><span class=\"line\">        self.activation = activation</span><br><span class=\"line\">        self.delta = PreDelta</span><br><span class=\"line\">        self.weight = PreWeight</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self,TEST)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.weight:</span><br><span class=\"line\">            print(<span class=\"string\">'untrained NN'</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            PredictA=[]</span><br><span class=\"line\">            biasA = np.array([np.repeat(<span class=\"number\">1</span>,np.shape(TEST)[<span class=\"number\">1</span>])])</span><br><span class=\"line\">            PredictA.append(TEST)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(self.weight)):</span><br><span class=\"line\">                PredictA.append(Sigmoid(np.dot(self.weight[l].T,np.concatenate((PredictA[l],biasA)))))</span><br><span class=\"line\">            print(PredictA)</span><br><span class=\"line\">            <span class=\"keyword\">return</span>(PredictA[<span class=\"number\">-1</span>])</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.weight)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">errHis</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.ErrHis)</span><br></pre></td></tr></table></figure>\n","excerpt":"","more":"<h1 id=\"人工神经网络\"><a href=\"#人工神经网络\" class=\"headerlink\" title=\"人工神经网络\"></a>人工神经网络</h1><p>人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络(动物的中枢神经系统，特别是大脑)的结构和功能的数学模型或计算模型。神经网络由大量的人工神经元联结进行计算。大多数情况下人工神经网络能在外界信息的基础上改变内部结构，是一种自适应系统。现代神经网络是一种非线性统计性数据建模工具，常用来对输入和输出间复杂的关系进行建模，或用来探索数据的模式。</p>\n<h2 id=\"数学模型\"><a href=\"#数学模型\" class=\"headerlink\" title=\"数学模型\"></a>数学模型</h2><p><img src=\"/upload/2/NN.png\" alt=\"人工神经网络\" title=\"4层人工神经网络\"></p>\n<h3 id=\"逻辑单元\"><a href=\"#逻辑单元\" class=\"headerlink\" title=\"逻辑单元\"></a>逻辑单元</h3><p>人工神经网络中，每个神经元上的逻辑单元是Sigmoid激励函数（Sigmoid activation function）或称逻辑激励函数（Logistic activation function）：<br>$$g(x)=\\frac{1}{1+e^{-x}}$$<br>其导数：<br>$${g}’(x)=\\frac{e^{-x}}{(1+e^{-x})^2}=\\frac{1}{1+e^{-x}}-\\frac{1}{(1+e^{-x})^2}=y(1-y)$$</p>\n<h3 id=\"变量定义\"><a href=\"#变量定义\" class=\"headerlink\" title=\"变量定义\"></a>变量定义</h3><p>$a_{i}^{(l)}$表示第$l$层第$i$个神经元（或称做激励单元（Activation Unit））的输出值。<br>$\\mathbf{a}^{(l)}$表示$l$层输出值组成的向量。<br>$w^{(l)}_{ij}$表示$l$层中的$i$神经元与$l+1$层中的$j$神经元的连接权值。<br>$\\mathbf{w}^{(l)}_{j}$表示$l$层中各个神经元与$l+1$层中的$j$神经元的连接权值构成的向量。<br>$W^{(l)}$表示第$l$层与$l+1$每条边构成的权值矩阵，$W^{(l)} \\in \\mathbb{R}^{\\dim(\\mathbf{a}^{(l)}) \\times \\dim(\\mathbf{a}^{(l+1)})}$。<br>$z^{(l)}_{i}$表示$l$层中的$i$神经元的逻辑单元输入值。$z_{i}^{l}=\\sum_{j=1}^{\\dim(\\mathbf{a}^{(l-1)})}a_{j}w_{ji}$<br>$\\mathbf{z}^{(l)}$表示$l$层中各个神经元的逻辑单元输入值。</p>\n<h2 id=\"向前传播\"><a href=\"#向前传播\" class=\"headerlink\" title=\"向前传播\"></a>向前传播</h2><p>向前传播是指通过随机$W$来依次计算各层$\\mathbf{a}$值。对于任意$l$层的$\\mathbf{a}^{(l)}$可以通过$\\mathbf{a}^{(l-1)}$来进行计算。<br>$$\\because a_{i}^{(l)} = g(z_{i}^{(l)})$$<br>$$z_{i}^{(l)}={\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}_{i}^{(l-1)}$$<br>$$\\therefore a_{i}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}\\mathbf{w}_{i}^{(l-1)})$$</p>\n<h3 id=\"向量化\"><a href=\"#向量化\" class=\"headerlink\" title=\"向量化\"></a>向量化</h3><p>$$\\mathbf{a}^{(l)} = g({\\mathbf{a}^{(l-1)}}^{T}W^{(l-1)}) $$</p>\n<h2 id=\"反向传播\"><a href=\"#反向传播\" class=\"headerlink\" title=\"反向传播\"></a>反向传播</h2><h3 id=\"变量定义-1\"><a href=\"#变量定义-1\" class=\"headerlink\" title=\"变量定义\"></a>变量定义</h3><p>$L$为最后一层。<br>$\\eta$学习率。</p>\n<h3 id=\"误差函数\"><a href=\"#误差函数\" class=\"headerlink\" title=\"误差函数\"></a>误差函数</h3><p>整体误差为：<br>$$J(W)=\\frac{1}{2}\\sum_{i=1}^{\\dim(\\mathbf{a}^{(L)})}(a_{i} - y_{i})^2=\\frac{1}{2} || \\mathbf{y} - \\mathbf{a}^{(L)}  ||^2$$</p>\n<h3 id=\"梯度下降\"><a href=\"#梯度下降\" class=\"headerlink\" title=\"梯度下降\"></a>梯度下降</h3><p>反向传播的学习方法是基于梯度下降方法。因为权值首先被初始化为随机值，然后向误差减小的方向调整。数学表达式：<br>$$\\Delta W= - \\eta\\frac{\\partial J}{\\partial W}$$<br>分量表示为：<br>$$\\Delta w_{ij}^{(l)} = - \\eta\\frac{\\partial J}{\\partial w_{ij}^{(l)}}$$<br>权值更新为：<br>$$w_{ij}^{(l)}:=w_{ij}^{(l)} + \\Delta w_{ij}^{(l)}$$</p>\n<h3 id=\"变量定义-2\"><a href=\"#变量定义-2\" class=\"headerlink\" title=\"变量定义\"></a>变量定义</h3><p>$\\delta_{i}^{(l)}=-\\frac{\\partial J}{\\partial z_{i}^{(l)}}$表示第$l$层第$i$个神经元敏感度（sensitive）。<br>$\\mathbf{\\delta}^{(l)}$表示第$l$层各个神经元错误构成的向量。</p>\n<h3 id=\"权值更新\"><a href=\"#权值更新\" class=\"headerlink\" title=\"权值更新\"></a>权值更新</h3><p>$$w_{ij}^{(l)}:=w_{ij}^{(l)} + \\Delta w_{ij}^{(l)}:=w_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial w_{ij}^{(l)}}:=w_{ij}^{(l)} - \\eta\\frac{\\partial J}{\\partial z_{j}^{(l+1)}}\\frac{\\partial z_{j}^{(l+1)}}{\\partial w_{ij}^{(l)}}$$<br>$$\\because z_{j}^{(l+1)} = \\sum_{i=1}^{\\dim(\\mathbf{a}^{(l)})}a_{j}^{(l)}w_{ij}^{(l)}$$<br>$$\\frac{\\partial z_{j}^{(l+1)}}{\\partial w_{ij}^{(l)}}=a_{i}^{(l)}$$<br>$$\\because -\\frac{\\partial J}{\\partial z_{j}^{(l+1)}}=\\delta_{j}^{(l+1)}$$<br>$$\\therefore w_{ij}^{(l)}:=w_{ij}^{(l)} + \\eta\\delta_{j}^{(l+1)} a_{i}^{(l)}$$</p>\n<h4 id=\"向量化-1\"><a href=\"#向量化-1\" class=\"headerlink\" title=\"向量化\"></a>向量化</h4><p>$$ W^{(l)} = \\sum_{i=1}^{\\dim(\\mathbf{a}^{(l)})}\\sum_{j=1}^{\\dim(\\mathbf{a}^{(l+1)})}(w_{ij}^{(l)} + \\eta\\delta_{j}^{(l+1)} a_{i}^{(l)})=W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$</p>\n<h3 id=\"敏感度-delta\"><a href=\"#敏感度-delta\" class=\"headerlink\" title=\"敏感度$\\delta$\"></a>敏感度$\\delta$</h3><h4 id=\"一般式\"><a href=\"#一般式\" class=\"headerlink\" title=\"一般式\"></a>一般式</h4><p>$$\\delta_{i}^{(l)}=-\\frac{\\partial J}{\\partial z_{i}^{(l)}}=-\\sum_{j}\\frac{\\partial J}{\\partial z_{j}^{(l+1)}}\\frac{\\partial z_{j}^{(l+1)}}{\\partial a_{i}^{(l)}}\\frac{\\partial a_{i}^{(l)}}{\\partial z_{i}^{(l)}}=\\sum_{j}[\\delta^{(l+1)}w_{ij}^{(l)}]{g}’(z_{i}^{(l)})$$</p>\n<h4 id=\"向量化-2\"><a href=\"#向量化-2\" class=\"headerlink\" title=\"向量化\"></a>向量化</h4><p>$$\\delta^{l}=W^{(l)}\\delta^{(l+1)}\\odot{g}’(\\mathbf{z}^{(l)})$$</p>\n<h3 id=\"反向传播实现\"><a href=\"#反向传播实现\" class=\"headerlink\" title=\"反向传播实现\"></a>反向传播实现</h3><h4 id=\"L-层的-delta-L\"><a href=\"#L-层的-delta-L\" class=\"headerlink\" title=\"$L$层的$\\delta^L$\"></a>$L$层的$\\delta^L$</h4><p>$$\\delta_{i}^{(L)}=-\\frac{\\partial J}{\\partial z_{i}^{(L)}}=-\\frac{\\partial J}{\\partial a_{i}^{(L)}}\\frac{\\partial a_{i}^{(L)}}{\\partial z_{i}^{(L)}}=(y_{i}-z_{i}^{(L)}){g}’(z_{i}^{(L)})=(y_{i}-z_{i}^{(L)})a^{(L)}_{i}(1-a^{(L)}_{i})$$</p>\n<h5 id=\"向量化-3\"><a href=\"#向量化-3\" class=\"headerlink\" title=\"向量化\"></a>向量化</h5><p>$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}’(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{(L)}\\odot(1-\\mathbf{a}^{(L)})$$</p>\n<h4 id=\"L-1-层权值更新\"><a href=\"#L-1-层权值更新\" class=\"headerlink\" title=\"$L-1$层权值更新\"></a>$L-1$层权值更新</h4><p>$$ \\delta^{(L-1)} = W^{(L-1)}\\delta^{(L)}\\odot{g}’(\\mathbf{z}^{(L-1)})=W^{(L-1)}\\odot(\\mathbf{a}^{(L-1)}\\odot(1-\\mathbf{a}^{(L-1)}))$$<br>$$ W^{(L-1)} =W^{(L-1)}+\\eta \\mathbf{a}^{(L-1)}{\\mathbf{\\delta}^{(L)}}^{T}$$</p>\n<h4 id=\"l-层权值更新\"><a href=\"#l-层权值更新\" class=\"headerlink\" title=\"$l$层权值更新\"></a>$l$层权值更新</h4><p>$$ \\delta^{(l)} = W^{(l)}\\delta^{(l+1)}\\odot{g}’(\\mathbf{z}^{(l)})=W^{(l)}\\odot(\\mathbf{a}^{(l)}\\odot(1-\\mathbf{a}^{(l)}))$$<br>$$ W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}^{T}$$</p>\n<h1 id=\"python3-实现\"><a href=\"#python3-实现\" class=\"headerlink\" title=\"python3 实现\"></a>python3 实现</h1><h2 id=\"超简单3层XOR网络\"><a href=\"#超简单3层XOR网络\" class=\"headerlink\" title=\"超简单3层XOR网络\"></a>超简单3层XOR网络</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">###三层异或门python实现</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">###训练集</span></span><br><span class=\"line\">X = np.array([ [<span class=\"number\">0</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>], [<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>,<span class=\"number\">1</span>] ])</span><br><span class=\"line\">y = np.array([[<span class=\"number\">0</span>,<span class=\"number\">1</span>,<span class=\"number\">1</span>,<span class=\"number\">0</span>]])</span><br><span class=\"line\"><span class=\"comment\">### 权值初始化</span></span><br><span class=\"line\">w1 = <span class=\"number\">2</span>*np.random.random((<span class=\"number\">2</span>,<span class=\"number\">3</span>)) - <span class=\"number\">1</span></span><br><span class=\"line\">w2 = <span class=\"number\">2</span>*np.random.random((<span class=\"number\">3</span>,<span class=\"number\">1</span>)) - <span class=\"number\">1</span></span><br><span class=\"line\"><span class=\"comment\">### 训练网络</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> j <span class=\"keyword\">in</span> range(<span class=\"number\">60000</span>):</span><br><span class=\"line\">    <span class=\"comment\">#正向传播</span></span><br><span class=\"line\">    l1 = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-(np.dot(w1.T,X))))</span><br><span class=\"line\">    l2 = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-(np.dot(w2.T,l1))))</span><br><span class=\"line\">    <span class=\"comment\">#反向传播</span></span><br><span class=\"line\">    l2_delta = (y - l2)*(l2*(<span class=\"number\">1</span>-l2))</span><br><span class=\"line\">    l1_delta = w2.dot(l2_delta) * (l1 * (<span class=\"number\">1</span>-l1))</span><br><span class=\"line\">    <span class=\"comment\">#权值更新</span></span><br><span class=\"line\">    w2 += l1.dot(l2_delta.T)</span><br><span class=\"line\">    w1 += X.dot(l1_delta.T)</span><br></pre></td></tr></table></figure>\n<h2 id=\"通用人工神经网络实现\"><a href=\"#通用人工神经网络实现\" class=\"headerlink\" title=\"通用人工神经网络实现\"></a>通用人工神经网络实现</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"comment\">#Sigmoid函数</span></span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Sigmoid</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    y = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span>(y)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NN</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,X,Y,layer,eta)</span>:</span></span><br><span class=\"line\">    <span class=\"string\">'''NN(输入，y，网络结构列表，学习率)'''</span></span><br><span class=\"line\">        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self,times)</span>:</span></span><br><span class=\"line\">        layerList = self.layer <span class=\"comment\">##NN结构</span></span><br><span class=\"line\">        self.ErrHis = []  <span class=\"comment\">#误差历史</span></span><br><span class=\"line\">        weight = [] <span class=\"comment\">#权值</span></span><br><span class=\"line\">        delta=[]  <span class=\"comment\">#敏感度</span></span><br><span class=\"line\">        <span class=\"comment\">#权值初始化</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(layerList)):</span><br><span class=\"line\">            weight.append(<span class=\"number\">2</span>*np.random.random((layerList[l<span class=\"number\">-1</span>],layerList[l])) - <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\">#学习</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(times):</span><br><span class=\"line\">            <span class=\"comment\">#初始化</span></span><br><span class=\"line\">            activation = []</span><br><span class=\"line\">            activation.append(self.X)</span><br><span class=\"line\">            <span class=\"comment\">#向前传播，获取激励</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                activation.append(Sigmoid(np.dot(weight[l].T,activation[l])))</span><br><span class=\"line\">            <span class=\"comment\">#误差</span></span><br><span class=\"line\">            err = <span class=\"number\">1</span>/<span class=\"number\">2</span> * sum((self.Y.T - activation[<span class=\"number\">-1</span>].T)**<span class=\"number\">2</span>)</span><br><span class=\"line\">            self.ErrHis.append(err[<span class=\"number\">0</span>])</span><br><span class=\"line\">            <span class=\"comment\">#敏感度反向传播</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> delta: PreDelta = delta <span class=\"comment\">#上次敏感度保留</span></span><br><span class=\"line\">            delta=[]  <span class=\"comment\">#初始化</span></span><br><span class=\"line\">            A = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(activation))] <span class=\"comment\">#反向列队</span></span><br><span class=\"line\">            A.remove(<span class=\"number\">0</span>)</span><br><span class=\"line\">            A.reverse()</span><br><span class=\"line\">            <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> delta: <span class=\"comment\">#最后一层</span></span><br><span class=\"line\">                    delta.append((self.Y-activation[a])*(activation[a]*(<span class=\"number\">1</span>-activation[a])))</span><br><span class=\"line\">                <span class=\"keyword\">else</span>: <span class=\"comment\">#其他层</span></span><br><span class=\"line\">                    delta.append(weight[a].dot(delta[<span class=\"number\">-1</span>])*(activation[a]*(<span class=\"number\">1</span>-activation[a])))</span><br><span class=\"line\">            delta.reverse() <span class=\"comment\">#正向化</span></span><br><span class=\"line\">            PreWeight =weight <span class=\"comment\">#上次权值保留</span></span><br><span class=\"line\">            <span class=\"comment\">#权值更新</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                weight[l] += self.eta*activation[l].dot(delta[l].T)</span><br><span class=\"line\">        <span class=\"comment\">#结束</span></span><br><span class=\"line\">        self.activation = activation</span><br><span class=\"line\">        self.delta = PreDelta</span><br><span class=\"line\">        self.weight = PreWeight</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self,TEST)</span>:</span> <span class=\"comment\">#预测</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.weight:</span><br><span class=\"line\">            print(<span class=\"string\">'untrained NN'</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            PredictA=[]</span><br><span class=\"line\">            PredictA.append(TEST)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(self.weight)):</span><br><span class=\"line\">                PredictA.append(Sigmoid(np.dot(self.weight[l].T,PredictA[l])))</span><br><span class=\"line\">            <span class=\"keyword\">return</span>(PredictA[<span class=\"number\">-1</span>])</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight</span><span class=\"params\">(self)</span>:</span> <span class=\"comment\">#返回权值</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.weight)      </span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">errHis</span><span class=\"params\">(self)</span>:</span> <span class=\"comment\">#返回误差历史</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.ErrHis)</span><br></pre></td></tr></table></figure>\n<h2 id=\"带偏置（Bias）节点的神经网络\"><a href=\"#带偏置（Bias）节点的神经网络\" class=\"headerlink\" title=\"带偏置（Bias）节点的神经网络\"></a>带偏置（Bias）节点的神经网络</h2><p>带偏置的神经网络，在第1层到$L-1$层都有着一个偏置节点；偏移节点有个特定，其只与下一层的非偏移节点相连。这样，$$\\mathbf{w}^{(l)}的维度=l层所有节点数\\times(l+1)层非偏置节点数$$<br><img src=\"/upload/2/NN_with_bias.png\" alt=\"带偏置人工神经网络\" title=\"4层带偏置人工神经网络\"><br>正向传播时:$$\\mathbf{a}^{(l+1)}_{no bias} = g(\\mathbf{w}^{(l)}\\mathbf{a}^{(l)}_{bias})$$<br>反向传播时，第$L$层敏感度不变：$$\\delta^{(L)}=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot{g}’(\\mathbf{z}^{(L)})=(\\mathbf{y}-\\mathbf{z}^{(L)})\\odot\\mathbf{a}^{L}\\odot(1-\\mathbf{a}^{(L)})$$<br>第$L-1$层敏感度：$$\\delta^{(L-1)}=W^{(L-1)}\\delta^{(L)}\\odot{g}’(\\mathbf{z}^{(L-1)})$$<br>第$l$层的敏感度：$$\\delta^{(l)}=W^{(l)}\\delta^{(l)}_{no bias}\\odot{g}’(\\mathbf{z}^{(l)})$$<br>权值更新：$$W^{(l)} =W^{(l)}+\\eta \\mathbf{a}^{(l)}{\\mathbf{\\delta}^{(l+1)}}_{nobias}^{T}$$</p>\n<h2 id=\"python3实现\"><a href=\"#python3实现\" class=\"headerlink\" title=\"python3实现\"></a>python3实现</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">Sigmoid</span><span class=\"params\">(x)</span>:</span></span><br><span class=\"line\">    y = <span class=\"number\">1</span>/(<span class=\"number\">1</span>+np.exp(-x))</span><br><span class=\"line\">    <span class=\"keyword\">return</span>(y)</span><br><span class=\"line\"><span class=\"class\"><span class=\"keyword\">class</span> <span class=\"title\">NN</span>:</span></span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">__init__</span><span class=\"params\">(self,X,Y,layer,eta)</span>:</span></span><br><span class=\"line\">        self.X,self.Y,self.layer,self.eta=(X,Y,layer,eta)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">train</span><span class=\"params\">(self,times)</span>:</span></span><br><span class=\"line\">        layerList = self.layer</span><br><span class=\"line\">        self.ErrHis = []</span><br><span class=\"line\">        weight = []</span><br><span class=\"line\">        delta=[]</span><br><span class=\"line\">        <span class=\"comment\">#权值初始化</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(<span class=\"number\">1</span>,len(layerList)):</span><br><span class=\"line\">            weight.append(<span class=\"number\">2</span>*np.random.random((layerList[l<span class=\"number\">-1</span>]+<span class=\"number\">1</span>,layerList[l])) - <span class=\"number\">1</span>)</span><br><span class=\"line\">        <span class=\"comment\">#学习</span></span><br><span class=\"line\">        <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(times):</span><br><span class=\"line\">            <span class=\"comment\">#初始化</span></span><br><span class=\"line\">            activation = []</span><br><span class=\"line\">            activation.append(self.X)</span><br><span class=\"line\">            biasA = np.array([np.repeat(<span class=\"number\">1</span>,np.shape(X)[<span class=\"number\">1</span>])]) <span class=\"comment\">#bias</span></span><br><span class=\"line\">            <span class=\"comment\">#向前传播，获取激励</span></span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                activation.append(Sigmoid(np.dot(weight[l].T,np.concatenate((biasA,activation[l])))))</span><br><span class=\"line\">            <span class=\"comment\">#误差</span></span><br><span class=\"line\">            err = <span class=\"number\">1</span>/<span class=\"number\">2</span> * sum((self.Y.T - activation[<span class=\"number\">-1</span>].T)**<span class=\"number\">2</span>)</span><br><span class=\"line\">            self.ErrHis.append(err[<span class=\"number\">0</span>])</span><br><span class=\"line\">            <span class=\"comment\">#敏感度反向传播</span></span><br><span class=\"line\">            <span class=\"keyword\">if</span> delta: PreDelta = delta</span><br><span class=\"line\">            delta=[]</span><br><span class=\"line\">            A = [i <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> range(len(activation))]</span><br><span class=\"line\">            A.remove(<span class=\"number\">0</span>)</span><br><span class=\"line\">            A.reverse()</span><br><span class=\"line\">            <span class=\"keyword\">for</span> a <span class=\"keyword\">in</span> A:</span><br><span class=\"line\">                <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> delta: <span class=\"comment\">#最后一层</span></span><br><span class=\"line\">                    delta.append((self.Y-activation[a])*(activation[a]*(<span class=\"number\">1</span>-activation[a])))</span><br><span class=\"line\">                <span class=\"keyword\">elif</span> len(delta) == <span class=\"number\">1</span>: <span class=\"comment\">#倒数第二层</span></span><br><span class=\"line\">                        delta.append(weight[a].dot(delta[<span class=\"number\">-1</span>])*(np.concatenate((biasA,activation[a]))*(<span class=\"number\">1</span>-np.concatenate((biasA,activation[a])))))</span><br><span class=\"line\">                <span class=\"keyword\">else</span>: <span class=\"comment\">#其他层</span></span><br><span class=\"line\">                    delta.append(weight[a].dot(delta[<span class=\"number\">-1</span>][<span class=\"number\">1</span>:,:])*(np.concatenate((biasA,activation[a]))*(<span class=\"number\">1</span>-np.concatenate((biasA,activation[a])))))</span><br><span class=\"line\">            <span class=\"comment\">#上一次权值保存，及权值更新</span></span><br><span class=\"line\">            delta.reverse()</span><br><span class=\"line\">            PreWeight =weight</span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(weight)):</span><br><span class=\"line\">                <span class=\"keyword\">if</span> l == len(weight)<span class=\"number\">-1</span>:</span><br><span class=\"line\">                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l].T)</span><br><span class=\"line\">                <span class=\"keyword\">else</span>:</span><br><span class=\"line\">                    weight[l] += self.eta*np.concatenate((biasA,activation[l])).dot(delta[l][<span class=\"number\">1</span>:,:].T)</span><br><span class=\"line\">        <span class=\"comment\">#结束</span></span><br><span class=\"line\">        self.activation = activation</span><br><span class=\"line\">        self.delta = PreDelta</span><br><span class=\"line\">        self.weight = PreWeight</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">predict</span><span class=\"params\">(self,TEST)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">if</span> <span class=\"keyword\">not</span> self.weight:</span><br><span class=\"line\">            print(<span class=\"string\">'untrained NN'</span>)</span><br><span class=\"line\">            <span class=\"keyword\">return</span> <span class=\"keyword\">False</span></span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            PredictA=[]</span><br><span class=\"line\">            biasA = np.array([np.repeat(<span class=\"number\">1</span>,np.shape(TEST)[<span class=\"number\">1</span>])])</span><br><span class=\"line\">            PredictA.append(TEST)</span><br><span class=\"line\">            <span class=\"keyword\">for</span> l <span class=\"keyword\">in</span> range(len(self.weight)):</span><br><span class=\"line\">                PredictA.append(Sigmoid(np.dot(self.weight[l].T,np.concatenate((PredictA[l],biasA)))))</span><br><span class=\"line\">            print(PredictA)</span><br><span class=\"line\">            <span class=\"keyword\">return</span>(PredictA[<span class=\"number\">-1</span>])</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">weight</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.weight)</span><br><span class=\"line\">    <span class=\"function\"><span class=\"keyword\">def</span> <span class=\"title\">errHis</span><span class=\"params\">(self)</span>:</span></span><br><span class=\"line\">        <span class=\"keyword\">return</span>(self.ErrHis)</span><br></pre></td></tr></table></figure>\n"},{"title":"利用决策树来识别Cufflinks中的技术噪音","date":"2015-07-26T07:38:48.000Z","_content":"\n# 简介\n熟悉RNA-Seq分析流程的朋友大多都知道，在经典的TopHat-Cufflinks分析流程中，对于最终的转录子（transfrag）组装的结果，很难去判断是否真实存在。在实际操作中，我们一般采用简单的一刀切，来进行过滤。比如，对于FPKM<1的进行过滤。这种方法虽然很有效，但是可能会丢失部分有价值的信息。在这篇文章中，我向大家介绍一种基于决策树的机器学习方法来识别Cufflins结果中的技术噪音（Tech Noise）。\n## 决策树\n### 什么是决策树\n决策树是一种依托选择策略建立的图解法。在机器学习中，决策树属于有督导学习（Supervised Learning）中分类（Classification）算法范围。决策树，经常出现在我们生活中。比如，\n\n>顾客：这手机能拍照吗？  \n>销售：能！  \n>顾客：这手机能上网吗？\n>销售：能！ \n>顾客：这手机能防水吗？  \n>销售：能！ \n>顾客：我买了！ \n\n用图解法表示决策过程：\n![买手机决策过程图解](/upload/1/decisionTreeSample.png '买手机决策树')\n这样，有了上面的认识，我们可以发现：\n- 决策树是一个树结构（可以是二叉树或非二叉树）。\n- 决策树上的每个非叶节点表示了对某一属性的判断。\n- 非叶节点的分支表示该属性的判断结果。\n- 叶节点表示分类结果。\n\n### 决策树分类算法\n决策树中最关键步骤是分裂属性。什么是分裂属性？分裂属性是指在某个节点，根据对某个属性的判断来把该节点集合分裂为两个或多个子集合（二叉树或多叉树）。理想的分裂，应该使各个子集合中的各个元素的类别一致。这样一来，分裂属性的情况包括以下三种：\n- 属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。\n- 属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。\n- 属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。  \n\n这样看来，属性的量度对于决策树的构造非常重要。选择属性度量的算法有很多，大部分都是采用自顶向下递归分治法（Recursive Partitioning），同时采用不回溯的贪心策略（Greedy Strategy）。\n#### ID3算法\nID3算法，全称Iterative Dichotomiser 3 ，译名迭代二叉树3代，1986年由Quinlan提出。ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。即在节点处选择当前集合中最大信息增益的属性作为分裂属性，并在子集中不断递归。以下是信息论中的基本概念：\n- 信息熵（Entropy）\n若信源符号有n种取值：$U_1$…$U_i$…$U_n$，对应概率为：$P_1$…$P_i$…$P_n$，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性$-logP_i$的统计平均值（$E$），可称为信息熵，即$$H(U)=E\\left | -logP\\_i \\right |=-\\sum_{i=0}^{n}P\\_ilogP\\_i$$式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算。 信息熵是表示随机变量不确定性的度量。从直观上，信息熵越大，变量包含的信息量越大，变量的不确定性也越大。一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。如果没有信息，任何公式或者数字的游戏都无法排除不确定性。\n![信息熵函数](/upload/1/EntropyFunction.png '信息熵函数')\n![二元信息熵](/upload/1/towbit.png '二元信息熵')\n- 复合熵（Composite Entropy）\n根据贝叶斯定理，结合信息熵。复合熵函数：$$H(x,y)=E|-logP\\_{(x,y)}|=\\sum\\_{x}^{ }\\sum\\_{y}^{ }P\\_{(x,y)}logP\\_{(x,y)}=-\\sum\\_{x}^{ }\\sum\\_{y}^{ }P\\_xP\\_ylog(P\\_xP\\_y)$$\n- 条件熵（conditional Entropy）\n根据贝叶斯定理，结合信息熵。条件熵函数：$$H(X|Y)=E|E|-logP\\_{(X|Y)}||=-\\sum\\_{Y}^{}\\sum\\_{X}^{}P\\_{Y\\_j}P\\_{(X\\_i|Y\\_j)}logP\\_{(X\\_i|Y\\_j)}$$\n- 信息增益（IG，Information Gain）\n信息增益在信息论中是很有效的特征选择方法。但凡是特征选择，总是将特征的重要程度先进行量化后再进行选择，而如何量化特征的重要性，就成了各种方法最大的不同。在信息增益中，量化特征的重要性就是看特征能够为分类系统带来多少的信息，带来的信息越多，该特征就越为重要。这可以用信息熵去量化。另外需要注意的是，信息增益是针对特征而言，也就是系统中有没有某个特征间系统信息熵的变化。令$A$为分裂特征，$A\\_i$为A特征分裂子集，$S\\_n$为n类样本，$U$为所有样本集合，信息增益函数可以表示为：$$IG(A)=H(U)-H(U|A)=\\sum\\_{j=1}^{S}\\sum\\_{i=1}^{A}P\\_{A\\_i}P\\_{S\\_j|A\\_i}log(P\\_{S\\_j|A\\_i})-\\sum\\_{j=1}^{S}P\\_{(S\\_j)}log(P\\_{(S\\_j)})$$\n\n\nID3算法就是利用信息增益来量化特征的重要性。其理论在于，每次分裂实在使系统信息增益最大化的特征上进行。同时ID3只有正负两个样本（全局二叉树），根据信息增益函数，ID3可以表示为：\n$$\\max\\_{i}(IG(A\\_i))=\\max\\_{i}(H(U)-H(U|A))=$$\n\n$$\\max\\_{i}(P\\_A(P\\_{(\\oplus|A)}logP\\_{(\\oplus|A)}+P\\_{(\\ominus|A)}logP\\_{(\\ominus|A)})+(1-P\\_A)(P\\_{(\\oplus|C\\_SA)}logP\\_{(\\oplus|C\\_SA)}+P\\_{(\\ominus|C\\_SA)}logP\\_{(\\ominus|C\\_SA)})-P\\_{\\oplus}logP\\_{\\oplus}-P\\_{\\ominus}logP\\_{\\ominus})$$\n##### 实例\n有类数据如下表，m,n为两个特征，c表示分类。\n\n| m | n | c |\n|----|----|----|\n| + | + | + |\n| + | + | + |\n| + | - | + |\n| - | + | - |\n| - | - | - |\n| + | - | - |\n\n根据ID3算法对其建决策树：\n1. 以m特征进行分裂的信息增量。\n$$IG(m)=H(U)-H(U|A)=-(\\frac{1}{2}log\\_2\\frac{1}{2}+\\frac{1}{2}log\\_2\\frac{1}{2})-(-\\frac{4}{6}(\\frac{1}{4}log\\_2\\frac{1}{4}+\\frac{3}{4}log\\_2\\frac{3}{4})-\\frac{2}{6}(0+0))=0.459$$\n![以m特征分裂](/upload/1/mclass.png '以m特征分裂')\n2. 以n特征进行分裂的信息增量。\n$$IG(n)=H(U)-H(U|A)=-(\\frac{1}{2}log\\_2\\frac{1}{2}+\\frac{1}{2}log\\_2\\frac{1}{2})-(-\\frac{3}{6}(\\frac{1}{3}log\\_2\\frac{1}{3}+\\frac{2}{3}log\\_2\\frac{2}{3})-\\frac{3}{6}(\\frac{1}{3}log\\_2\\frac{1}{3}+\\frac{2}{3}log\\_2\\frac{2}{3}))=0.082$$\n![以n特征分裂](/upload/1/nclass.png '以n特征分裂')\n3. 因为$IG(m)>IG(n)$，选择信息增益最大的特征进行分裂，所以选择m进行分裂。\n\n\n#### C4.5算法\nC4.5算法是ID3算法进行改进算法，主要不同点是对特征重要性量化上采用的是信息增益率（Info Gain Ratio）。这种方法解决ID3算法在某个属性存在大量不同值时导致的过拟合现象。相比于ID3算法，改进有如下几个要点：\n- 用信息增益率来选择属性。\n- 在决策树构造过程中可以进行剪枝。ID3算法会由于某些具有很少元素的结点可能会使构造的决策树过适应（Overfitting），这时候不考虑这些结点可能会更好。\n- 对非离散数据也能处理。\n- 能够对不完整数据进行处理。\n\n##### 信息增益率\n分裂属性$A$，分裂后子集空间$S$，信息增益率IGR则可表示为：\n$$IGR(S,A)=\\frac{IG(S,A)}{II(S,A)}$$\n其中II表示内在信息（Intrinsic Information），其定义为：\n$$II(S,A)=-\\sum\\frac{|S\\_i|}{|S|}log\\_2\\frac{|S\\_i|}{|S|}$$\n其中$S\\_i$是属性A分割S而形成的$i$子集。\n\n##### 实例\n依旧上述例子\n- 以m特征进行分裂的信息增量。$$IGR(m)=\\frac{IG(m)}{II{m}}=\\frac{0.459}{\\frac{4}{6}log\\_2\\frac{4}{6}+\\frac{2}{6}log\\_2\\frac{2}{6}}=\\frac{0.459}{0.918}=0.5$$\n\n#### CART算法\n分类和回归树（Classification and regression tree，CART）在模式分类中被视作一种通用的树生长算法。CART提供一种通用框架，利用它，可以实例化各种各样不同的判定树。按照CART，有6个问题需要回答：\n- 节点分支数应该是多少？\n- 节点测试是哪个属性？\n- 何时可以令某节点成为叶节点？\n- 如何剪枝？\n- 如果叶节点仍不“纯”，那么怎样给它赋类别标记？\n- 缺省数据处理？\n\n##### 分支数目\n任意多类别样本都可以通过多个二叉树进行划分。所以，二叉树具有万能的表达能力，并且在训练上很简便。所以，CART一般采用二叉树结构。\n##### 节点不纯度\nCART中有四种方式来量化不纯度（Impurity）：\n- 信息熵不纯度（Entropy Impurity）$$I(N)=-sum\\_{j}^{}P(w\\_j)log\\_2P(w\\_j)$$\n- 方差不纯度(Variance Impurity) $$I(N)=\\prod\\_{j}P(w\\_j)=P(1-P)$$\n- Gini不纯度(Gini Impurity)$$I(N)=\\sum\\_{j}^{}P(w\\_j)P(w\\_{C\\_{S}j})=1-\\sum\\_{j}^{}P^2(w\\_j)$$\n- 误分类不纯度(Missclassification Impurity)$$I(N)=1-\\max\\_{j}P(w\\_j)$$\n![4种不纯度量化方法](/upload/1/class.png '4种不纯度量化方法')\n\n在如何选择分裂属性上，很明显的启发思路是选择使不纯度下降最快的那个属性（ID3算法）。可记作$$\\max\\_{i}{\\Delta}I(N)=I(N)-P\\_LI(N\\_L)-(1-P\\_L)I(N\\_R)$$，其中$N\\_L$和$N\\_R$分别是左右子节点。这种贪心算法有个缺点，就是无法确保顺序局部优化过程能得到全局最优。实践中，熵不纯度和Gini不纯度运用广泛，但在实践过程中，不同的不纯度函数对最后的分类效果和其性能影响往往比预期的小。所以，在实践中，反倒是停止判决和剪枝算法受到更多的重视。\n##### 分支停止准则\n二叉树，分支训练中，分支过多会过拟合，分支过少，训练样本误差大，分类效果也差。究竟何时应该停止分支呢？这里介绍五种方法：\n1. 验证和交叉验证技术（validation and cross-validation）。\n将部分样本用于训练树，剩余样本作为验证。持续节点分支，直到对于验证集的分类误差最小化。\n2. 不纯度下降门限。\n预先设定一个不纯度下降差的（小）门限值，当候选分支小于这个值时，停止分支。这种方法有两个优点，一是全部样本均用于训练，二是树的每层都可能存在叶节点。但该方法也有个固有缺点，即门限值预先设定相当困难，因为最终性能与门限大小并无直接的函数关系\n3. 节点最小样本数。\n当候选分支样本数小于设定值，停止分支。这种方法类似k-近临分类器，当样本集中时，分割子集就小；当样本稀疏时，分割子集就大。\n4. 高复杂度换取高的准确度。\n通过最小化全局指标：$$\\alpha{\\cdot}size+\\sum\\_{叶节点}^{}I(N)$$这里$size$表示节点或分支的数目，$\\alpha$是个正常数（类似神经网络中的正则化方法）。这种方法中，$\\alpha$的设定也很难，它也与最终分类性能无简单的相关关系。\n5. 不纯度下降的显著性检验。\n估计之前的所有${\\Delta}I(N)$概率分布，在对某一候选分支$i$时，检验${\\Delta}I(N\\_i)$是否与上述分布存在统计学差异。\n![卡方检验](/upload/1/chisq.png '采取卡方检验。对于候选分支，其理论发生概率$p$对应得卡方值函数为$\\frac{(1-p)^2}{p}$，在0.05的置信水平下，$p$应该大于0.18。')\n6. $\\chi^2$假设检验。\n要点是判断候选分支是否存在统计学上的意义，也就是判断分支是否明显有别于随机分支。假设节点存在$n$个样本（$n\\_1$个$w\\_1$类，$n\\_2$个$w\\_2$类），候选分支将$Pn$个样本分到左分支，$(1-P)n$分到了右支。此时假设是随机分支，左枝应该有$Pn\\_1$个$w\\_1$和$Pn\\_2$个$w\\_2$，其它剩余的都在右支。如果用$\\chi^2$来评估这次分支与随机的偏离度，那偏离度$$\\chi^2=\\sum\\_{i=1}^{2}\\frac{(n\\_{iL}-n\\_{ie})^2}{n\\_{ie}}$$其中$n\\_{iL}$是指$w\\_i$在左分支的数目，而$n\\_{ie}=Pn\\_i$则对应随机分支情况下的值。当两者相同时，$\\chi^2$接近0，相反，当$\\chi^2$越大，说明两者差异越大。当其大于某临界值时，就可以拒绝零假设（候选分支等于随机分支）。\n7. 损失矩阵\n构建损失矩阵，根据损失矩阵计算节点损失，当叶节点的损失大于其父节点的损失时，则停止分支。\n\n##### 剪枝\n有时，分支停止方法会因缺少足够的前瞻性而过早停止（视界局限效应）。这时，需要剪枝（pruning）来克服这种缺陷。在剪枝过程中，树首先要充足生长，直到叶节点都有最小不纯度值为止，因而没有任何推定的“视界局限”。然后，对所有相邻的成对叶节点，考虑能否合并它们，如果合并它们只引起很小的不纯度增长，那就合并它们为新的叶节点。\n\n## cufflinks中决策树的理论基础\ncufflinks软件原理是将mapped reads组装成transfrag，由于reads很短，拼接还原难度高。这个过程中可能会产生错误的结果。我们假设：\n1. 同一染色体上的基因的特征相似。\n2. cufflinks产生的未知的transfrag大部分是noise，可靠的transfrag仅占小部分，这些可靠的transfrag拥有与正常基因相似的特征，且与noise存在较大差异。\n这样，我们就拥有了一个正样本集（真实基因），和一个负样本集（noise），这就可以建立决策树，来确认真实基因的特征。\n在算法上，采取常用的CART算法，软件选用R语言及其rpart模块。\n\n## 特征选择\n该次选取了个5特征变量：长度，外显子数目，样本中表达比例，FPKM值。\n\n|特征变量|选择原因|\n|--------|----------------|\n|  长度  | 人类基因90%以上都在300bp以上，过短的transfrag可能是由于技术噪音产生 |\n| 外显子数 | 人类大部分基因都拥有多个外显子数，趋于更为复杂的结构；而技术噪音一般都很简单。|\n| 样本中表达比例 | 正常的基因，应该会在多个样本中都会出现表达，而部分技术噪音可能仅在个别样本中表达。 |\n| FPKM值 | 正常基因的FPKM值应该会在一定范围内，而技术噪音序列可能会存在FPKM极值。 |\n\n\n# 数据建模\n## 数据描述\n数据使用的是10个人类样本cufflinks组装结果中1号染色体上的数据。\n数据集中共28,290个转录子，其中26,185是已知转录位点，2,105是未知转录位点。\n\n|特征变量|最小值|中值|均值|最大值|\n|------|-----|-----|-----|-------|\n| 长度 | 1 | 1540 | 2679 | 79680 |\n| 外显子数 | 1 | 5 | 7.887 | 100 |\n| 表达比例 | 0.10 | 0.50 | 0.54 | 1.00 |\n| FPKM | 0 | 0.115 | 2.131 | 5271 |\n\n\n![数据描述](/upload/1/summary.png '数据描述(已知转录和未知转录并不能明显区分开)')\n![PCA分析](/upload/1/pca_before.png 'PCA结果（无法区分）')\n## rpart包参数简介\n```\n函数：rpart\n用法：rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)\n参数：\n\tformula：公式，例Y~X1+X2+X3+X4\n\tdata：数据源，一般是数据框\n\tweights: 向量，各个自变量的权重\n\tsubset：只用特定列\n\tmethod：如果Y是生存类，选择\"exp\"；如果Y有两列，则选择\"poisson\"；如果Y是因子，则选择\"class\"；如果Y是连续变量,则选择\"anova\";\n\tparms：分裂参数；\"anova\"类没有该参数，\"exp\"和\"poisson\"仅有先验分布；\"class\"有三项，prior对先验概率，loss对损失矩阵，split对不纯度计算方式———\"gini\"或\"information\"。\n\tcontrol：参见rpart.control\n\tcost：各自变量代价列表\n函数：rpart.control\n用法：rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...)\n参数：\n\tminsplit：父节点最小样本数。\n\tminbucket：叶节点最小样本数。\n\tcp：complexity parameter.复杂性参数。分支后cp必须高于设定值。\n\tmaxcompete：保留的最大候选分支数。\n\tmaxsurrogate：替代分裂最大数。\n\tusesurrogate：替代分裂处理。0，仅展示；1，使用替代分裂；2，不使用替代分裂\n\txval：cv数\n\tsurrogatestyle：替代分裂风格。0，数值；1，比例。\n\tmaxdepth：树的最大深度\n```\n## 代码\n```R\n# 加载rpart包\nlibrary(rpart)\n# 构建数据框\nd <- data.frame(length,exon,ratio,FPKM,class)\n# 训练集和测试集\ntrain.l <- sample(1:dim(d)[1],as.integer(dim(d)[1]*0.7)) #取样列\ntrain <- d[train.l,] #训练集\ntest <- d[-train.l,] #测试集\n# rpart控制参数设置\nct <- rpart.control(xval=10, minsplit=100, minbucket=40, maxdepth=4) #采用10折交叉验证；停止分裂条件：1、父节点样本小于100；2、叶节点样本大于40；3、深度小于4层；\n# rpart建模\nfit<-rpart(class~length+exon+rat+FPKM,train,method='class',control=ct,parms = list(split = \"information\"))\n# 可视化\nplot(fit,margin=0.1)\ntext(fit,use.n=T) #原始树\n# 剪枝\nplotcp(fit)  #筛选合适的cp值\nplot(prune(fit,cp=0.01),margin=0.1) \ntext(prune(fit,cp=0.01),use.n=T) #length过拟合，增大cp值，减少\nplot(prune(fit,cp=0.045),margin=0.1)\ntext(prune(fit,cp=0.045),use.n=T) \n# 确定模型\nfit.used<-prune(fit,cp=0.045)\n# 精度\nsummary(test$type == predict(fit.used,test,type='class'))  #统计预测结果与实际结果是否相同\nsummary(test$type[test$type != predict(fit.used,test,type='class')]) #统计预测结果与实际一致\nsummary(test$type[test$type == predict(fit.used,test,type='class')]) #统计预测结果与实际不一致\n```\n![原始树](/upload/1/raw_tree.png '原始树')\n![第一次剪枝](/upload/1/pruned_tree1.png '0.01cp值剪枝结果，过拟合严重。（默认cp值为0.01，所以与原始树一致）')\n![第二次剪枝](/upload/1/pruned_tree2.png '最终模型，0.045cp值剪枝结果，主要依靠exon和length属性进行分类')\n## 模型分析\n最终的模型，只用到了两个属性。这个和之前数据描述相符（长度和外显子数勉强可以区分，表达比例和FPKM重叠严重）。将模型用于测试数据集，模型表现：\n![2x2列联表](/upload/1/2x2ContingencyTable.png '2x2列联表')\n敏感度$TPR$:\n$$TPR=\\frac{7546}{7546+332}=0.9578573$$\n精度$PPV$:\n$$PPV=\\frac{7546}{179+7546}=0.9768285$$\n特异性$SPC$:\n$$SPC=\\frac{430}{430+332}=0.5643045$$\n准确度$ACC$:\n$$ACC=\\frac{430+7546}{430+179+332+7546}=0.9397903$$\n从上述四个指标上看，除了特异性差很多，其他指标都表现良好。考虑到unknown中有潜在的基因，特异性差也是在预料当中。\n在将该模型用于原始数据中unknown部分再进行重新分类，2105个unknown数据中预测出641个潜在的基因，再进一步对模型分类后的数据进行描述：\n![预测后数据描述](/upload/1/summary1.png '预测后数据描述')\n可以看出，预测后基因与技术噪音在长度和外显子数这两个属性区别更加明显。\n# 结论\n相对之前的经验判断，使用了CART算法构建技术噪音过滤决策树，避免了主观判断，科学量化了过滤条件。\n# 参考文献\n1. Duda R O, Hart P E, Stork D G. Pattern classification[M]. John Wiley & Sons, 2012.\n2. Prensner J R, Iyer M K, Balbin O A, et al. Transcriptome sequencing across a prostate cancer cohort identifies PCAT-1, an unannotated lincRNA implicated in disease progression[J]. Nature biotechnology, 2011, 29(8): 742-749.\n3. Harrington P. 机器学习实战[J]. 人民邮电出版社, 北京, 2013.\n","source":"_posts/利用决策树来识别Cufflinks中的技术噪音.md","raw":"---\ntitle: 利用决策树来识别Cufflinks中的技术噪音\ndate: 2015-07-26 15:38:48\ncategories:\n- 机器学习\ntags:\n- RNA-seq\n- 决策树\n- Cufflinks\n- 技术噪音\n---\n\n# 简介\n熟悉RNA-Seq分析流程的朋友大多都知道，在经典的TopHat-Cufflinks分析流程中，对于最终的转录子（transfrag）组装的结果，很难去判断是否真实存在。在实际操作中，我们一般采用简单的一刀切，来进行过滤。比如，对于FPKM<1的进行过滤。这种方法虽然很有效，但是可能会丢失部分有价值的信息。在这篇文章中，我向大家介绍一种基于决策树的机器学习方法来识别Cufflins结果中的技术噪音（Tech Noise）。\n## 决策树\n### 什么是决策树\n决策树是一种依托选择策略建立的图解法。在机器学习中，决策树属于有督导学习（Supervised Learning）中分类（Classification）算法范围。决策树，经常出现在我们生活中。比如，\n\n>顾客：这手机能拍照吗？  \n>销售：能！  \n>顾客：这手机能上网吗？\n>销售：能！ \n>顾客：这手机能防水吗？  \n>销售：能！ \n>顾客：我买了！ \n\n用图解法表示决策过程：\n![买手机决策过程图解](/upload/1/decisionTreeSample.png '买手机决策树')\n这样，有了上面的认识，我们可以发现：\n- 决策树是一个树结构（可以是二叉树或非二叉树）。\n- 决策树上的每个非叶节点表示了对某一属性的判断。\n- 非叶节点的分支表示该属性的判断结果。\n- 叶节点表示分类结果。\n\n### 决策树分类算法\n决策树中最关键步骤是分裂属性。什么是分裂属性？分裂属性是指在某个节点，根据对某个属性的判断来把该节点集合分裂为两个或多个子集合（二叉树或多叉树）。理想的分裂，应该使各个子集合中的各个元素的类别一致。这样一来，分裂属性的情况包括以下三种：\n- 属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。\n- 属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。\n- 属性是连续值。此时确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支。  \n\n这样看来，属性的量度对于决策树的构造非常重要。选择属性度量的算法有很多，大部分都是采用自顶向下递归分治法（Recursive Partitioning），同时采用不回溯的贪心策略（Greedy Strategy）。\n#### ID3算法\nID3算法，全称Iterative Dichotomiser 3 ，译名迭代二叉树3代，1986年由Quinlan提出。ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。即在节点处选择当前集合中最大信息增益的属性作为分裂属性，并在子集中不断递归。以下是信息论中的基本概念：\n- 信息熵（Entropy）\n若信源符号有n种取值：$U_1$…$U_i$…$U_n$，对应概率为：$P_1$…$P_i$…$P_n$，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性$-logP_i$的统计平均值（$E$），可称为信息熵，即$$H(U)=E\\left | -logP\\_i \\right |=-\\sum_{i=0}^{n}P\\_ilogP\\_i$$式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算。 信息熵是表示随机变量不确定性的度量。从直观上，信息熵越大，变量包含的信息量越大，变量的不确定性也越大。一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。如果没有信息，任何公式或者数字的游戏都无法排除不确定性。\n![信息熵函数](/upload/1/EntropyFunction.png '信息熵函数')\n![二元信息熵](/upload/1/towbit.png '二元信息熵')\n- 复合熵（Composite Entropy）\n根据贝叶斯定理，结合信息熵。复合熵函数：$$H(x,y)=E|-logP\\_{(x,y)}|=\\sum\\_{x}^{ }\\sum\\_{y}^{ }P\\_{(x,y)}logP\\_{(x,y)}=-\\sum\\_{x}^{ }\\sum\\_{y}^{ }P\\_xP\\_ylog(P\\_xP\\_y)$$\n- 条件熵（conditional Entropy）\n根据贝叶斯定理，结合信息熵。条件熵函数：$$H(X|Y)=E|E|-logP\\_{(X|Y)}||=-\\sum\\_{Y}^{}\\sum\\_{X}^{}P\\_{Y\\_j}P\\_{(X\\_i|Y\\_j)}logP\\_{(X\\_i|Y\\_j)}$$\n- 信息增益（IG，Information Gain）\n信息增益在信息论中是很有效的特征选择方法。但凡是特征选择，总是将特征的重要程度先进行量化后再进行选择，而如何量化特征的重要性，就成了各种方法最大的不同。在信息增益中，量化特征的重要性就是看特征能够为分类系统带来多少的信息，带来的信息越多，该特征就越为重要。这可以用信息熵去量化。另外需要注意的是，信息增益是针对特征而言，也就是系统中有没有某个特征间系统信息熵的变化。令$A$为分裂特征，$A\\_i$为A特征分裂子集，$S\\_n$为n类样本，$U$为所有样本集合，信息增益函数可以表示为：$$IG(A)=H(U)-H(U|A)=\\sum\\_{j=1}^{S}\\sum\\_{i=1}^{A}P\\_{A\\_i}P\\_{S\\_j|A\\_i}log(P\\_{S\\_j|A\\_i})-\\sum\\_{j=1}^{S}P\\_{(S\\_j)}log(P\\_{(S\\_j)})$$\n\n\nID3算法就是利用信息增益来量化特征的重要性。其理论在于，每次分裂实在使系统信息增益最大化的特征上进行。同时ID3只有正负两个样本（全局二叉树），根据信息增益函数，ID3可以表示为：\n$$\\max\\_{i}(IG(A\\_i))=\\max\\_{i}(H(U)-H(U|A))=$$\n\n$$\\max\\_{i}(P\\_A(P\\_{(\\oplus|A)}logP\\_{(\\oplus|A)}+P\\_{(\\ominus|A)}logP\\_{(\\ominus|A)})+(1-P\\_A)(P\\_{(\\oplus|C\\_SA)}logP\\_{(\\oplus|C\\_SA)}+P\\_{(\\ominus|C\\_SA)}logP\\_{(\\ominus|C\\_SA)})-P\\_{\\oplus}logP\\_{\\oplus}-P\\_{\\ominus}logP\\_{\\ominus})$$\n##### 实例\n有类数据如下表，m,n为两个特征，c表示分类。\n\n| m | n | c |\n|----|----|----|\n| + | + | + |\n| + | + | + |\n| + | - | + |\n| - | + | - |\n| - | - | - |\n| + | - | - |\n\n根据ID3算法对其建决策树：\n1. 以m特征进行分裂的信息增量。\n$$IG(m)=H(U)-H(U|A)=-(\\frac{1}{2}log\\_2\\frac{1}{2}+\\frac{1}{2}log\\_2\\frac{1}{2})-(-\\frac{4}{6}(\\frac{1}{4}log\\_2\\frac{1}{4}+\\frac{3}{4}log\\_2\\frac{3}{4})-\\frac{2}{6}(0+0))=0.459$$\n![以m特征分裂](/upload/1/mclass.png '以m特征分裂')\n2. 以n特征进行分裂的信息增量。\n$$IG(n)=H(U)-H(U|A)=-(\\frac{1}{2}log\\_2\\frac{1}{2}+\\frac{1}{2}log\\_2\\frac{1}{2})-(-\\frac{3}{6}(\\frac{1}{3}log\\_2\\frac{1}{3}+\\frac{2}{3}log\\_2\\frac{2}{3})-\\frac{3}{6}(\\frac{1}{3}log\\_2\\frac{1}{3}+\\frac{2}{3}log\\_2\\frac{2}{3}))=0.082$$\n![以n特征分裂](/upload/1/nclass.png '以n特征分裂')\n3. 因为$IG(m)>IG(n)$，选择信息增益最大的特征进行分裂，所以选择m进行分裂。\n\n\n#### C4.5算法\nC4.5算法是ID3算法进行改进算法，主要不同点是对特征重要性量化上采用的是信息增益率（Info Gain Ratio）。这种方法解决ID3算法在某个属性存在大量不同值时导致的过拟合现象。相比于ID3算法，改进有如下几个要点：\n- 用信息增益率来选择属性。\n- 在决策树构造过程中可以进行剪枝。ID3算法会由于某些具有很少元素的结点可能会使构造的决策树过适应（Overfitting），这时候不考虑这些结点可能会更好。\n- 对非离散数据也能处理。\n- 能够对不完整数据进行处理。\n\n##### 信息增益率\n分裂属性$A$，分裂后子集空间$S$，信息增益率IGR则可表示为：\n$$IGR(S,A)=\\frac{IG(S,A)}{II(S,A)}$$\n其中II表示内在信息（Intrinsic Information），其定义为：\n$$II(S,A)=-\\sum\\frac{|S\\_i|}{|S|}log\\_2\\frac{|S\\_i|}{|S|}$$\n其中$S\\_i$是属性A分割S而形成的$i$子集。\n\n##### 实例\n依旧上述例子\n- 以m特征进行分裂的信息增量。$$IGR(m)=\\frac{IG(m)}{II{m}}=\\frac{0.459}{\\frac{4}{6}log\\_2\\frac{4}{6}+\\frac{2}{6}log\\_2\\frac{2}{6}}=\\frac{0.459}{0.918}=0.5$$\n\n#### CART算法\n分类和回归树（Classification and regression tree，CART）在模式分类中被视作一种通用的树生长算法。CART提供一种通用框架，利用它，可以实例化各种各样不同的判定树。按照CART，有6个问题需要回答：\n- 节点分支数应该是多少？\n- 节点测试是哪个属性？\n- 何时可以令某节点成为叶节点？\n- 如何剪枝？\n- 如果叶节点仍不“纯”，那么怎样给它赋类别标记？\n- 缺省数据处理？\n\n##### 分支数目\n任意多类别样本都可以通过多个二叉树进行划分。所以，二叉树具有万能的表达能力，并且在训练上很简便。所以，CART一般采用二叉树结构。\n##### 节点不纯度\nCART中有四种方式来量化不纯度（Impurity）：\n- 信息熵不纯度（Entropy Impurity）$$I(N)=-sum\\_{j}^{}P(w\\_j)log\\_2P(w\\_j)$$\n- 方差不纯度(Variance Impurity) $$I(N)=\\prod\\_{j}P(w\\_j)=P(1-P)$$\n- Gini不纯度(Gini Impurity)$$I(N)=\\sum\\_{j}^{}P(w\\_j)P(w\\_{C\\_{S}j})=1-\\sum\\_{j}^{}P^2(w\\_j)$$\n- 误分类不纯度(Missclassification Impurity)$$I(N)=1-\\max\\_{j}P(w\\_j)$$\n![4种不纯度量化方法](/upload/1/class.png '4种不纯度量化方法')\n\n在如何选择分裂属性上，很明显的启发思路是选择使不纯度下降最快的那个属性（ID3算法）。可记作$$\\max\\_{i}{\\Delta}I(N)=I(N)-P\\_LI(N\\_L)-(1-P\\_L)I(N\\_R)$$，其中$N\\_L$和$N\\_R$分别是左右子节点。这种贪心算法有个缺点，就是无法确保顺序局部优化过程能得到全局最优。实践中，熵不纯度和Gini不纯度运用广泛，但在实践过程中，不同的不纯度函数对最后的分类效果和其性能影响往往比预期的小。所以，在实践中，反倒是停止判决和剪枝算法受到更多的重视。\n##### 分支停止准则\n二叉树，分支训练中，分支过多会过拟合，分支过少，训练样本误差大，分类效果也差。究竟何时应该停止分支呢？这里介绍五种方法：\n1. 验证和交叉验证技术（validation and cross-validation）。\n将部分样本用于训练树，剩余样本作为验证。持续节点分支，直到对于验证集的分类误差最小化。\n2. 不纯度下降门限。\n预先设定一个不纯度下降差的（小）门限值，当候选分支小于这个值时，停止分支。这种方法有两个优点，一是全部样本均用于训练，二是树的每层都可能存在叶节点。但该方法也有个固有缺点，即门限值预先设定相当困难，因为最终性能与门限大小并无直接的函数关系\n3. 节点最小样本数。\n当候选分支样本数小于设定值，停止分支。这种方法类似k-近临分类器，当样本集中时，分割子集就小；当样本稀疏时，分割子集就大。\n4. 高复杂度换取高的准确度。\n通过最小化全局指标：$$\\alpha{\\cdot}size+\\sum\\_{叶节点}^{}I(N)$$这里$size$表示节点或分支的数目，$\\alpha$是个正常数（类似神经网络中的正则化方法）。这种方法中，$\\alpha$的设定也很难，它也与最终分类性能无简单的相关关系。\n5. 不纯度下降的显著性检验。\n估计之前的所有${\\Delta}I(N)$概率分布，在对某一候选分支$i$时，检验${\\Delta}I(N\\_i)$是否与上述分布存在统计学差异。\n![卡方检验](/upload/1/chisq.png '采取卡方检验。对于候选分支，其理论发生概率$p$对应得卡方值函数为$\\frac{(1-p)^2}{p}$，在0.05的置信水平下，$p$应该大于0.18。')\n6. $\\chi^2$假设检验。\n要点是判断候选分支是否存在统计学上的意义，也就是判断分支是否明显有别于随机分支。假设节点存在$n$个样本（$n\\_1$个$w\\_1$类，$n\\_2$个$w\\_2$类），候选分支将$Pn$个样本分到左分支，$(1-P)n$分到了右支。此时假设是随机分支，左枝应该有$Pn\\_1$个$w\\_1$和$Pn\\_2$个$w\\_2$，其它剩余的都在右支。如果用$\\chi^2$来评估这次分支与随机的偏离度，那偏离度$$\\chi^2=\\sum\\_{i=1}^{2}\\frac{(n\\_{iL}-n\\_{ie})^2}{n\\_{ie}}$$其中$n\\_{iL}$是指$w\\_i$在左分支的数目，而$n\\_{ie}=Pn\\_i$则对应随机分支情况下的值。当两者相同时，$\\chi^2$接近0，相反，当$\\chi^2$越大，说明两者差异越大。当其大于某临界值时，就可以拒绝零假设（候选分支等于随机分支）。\n7. 损失矩阵\n构建损失矩阵，根据损失矩阵计算节点损失，当叶节点的损失大于其父节点的损失时，则停止分支。\n\n##### 剪枝\n有时，分支停止方法会因缺少足够的前瞻性而过早停止（视界局限效应）。这时，需要剪枝（pruning）来克服这种缺陷。在剪枝过程中，树首先要充足生长，直到叶节点都有最小不纯度值为止，因而没有任何推定的“视界局限”。然后，对所有相邻的成对叶节点，考虑能否合并它们，如果合并它们只引起很小的不纯度增长，那就合并它们为新的叶节点。\n\n## cufflinks中决策树的理论基础\ncufflinks软件原理是将mapped reads组装成transfrag，由于reads很短，拼接还原难度高。这个过程中可能会产生错误的结果。我们假设：\n1. 同一染色体上的基因的特征相似。\n2. cufflinks产生的未知的transfrag大部分是noise，可靠的transfrag仅占小部分，这些可靠的transfrag拥有与正常基因相似的特征，且与noise存在较大差异。\n这样，我们就拥有了一个正样本集（真实基因），和一个负样本集（noise），这就可以建立决策树，来确认真实基因的特征。\n在算法上，采取常用的CART算法，软件选用R语言及其rpart模块。\n\n## 特征选择\n该次选取了个5特征变量：长度，外显子数目，样本中表达比例，FPKM值。\n\n|特征变量|选择原因|\n|--------|----------------|\n|  长度  | 人类基因90%以上都在300bp以上，过短的transfrag可能是由于技术噪音产生 |\n| 外显子数 | 人类大部分基因都拥有多个外显子数，趋于更为复杂的结构；而技术噪音一般都很简单。|\n| 样本中表达比例 | 正常的基因，应该会在多个样本中都会出现表达，而部分技术噪音可能仅在个别样本中表达。 |\n| FPKM值 | 正常基因的FPKM值应该会在一定范围内，而技术噪音序列可能会存在FPKM极值。 |\n\n\n# 数据建模\n## 数据描述\n数据使用的是10个人类样本cufflinks组装结果中1号染色体上的数据。\n数据集中共28,290个转录子，其中26,185是已知转录位点，2,105是未知转录位点。\n\n|特征变量|最小值|中值|均值|最大值|\n|------|-----|-----|-----|-------|\n| 长度 | 1 | 1540 | 2679 | 79680 |\n| 外显子数 | 1 | 5 | 7.887 | 100 |\n| 表达比例 | 0.10 | 0.50 | 0.54 | 1.00 |\n| FPKM | 0 | 0.115 | 2.131 | 5271 |\n\n\n![数据描述](/upload/1/summary.png '数据描述(已知转录和未知转录并不能明显区分开)')\n![PCA分析](/upload/1/pca_before.png 'PCA结果（无法区分）')\n## rpart包参数简介\n```\n函数：rpart\n用法：rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)\n参数：\n\tformula：公式，例Y~X1+X2+X3+X4\n\tdata：数据源，一般是数据框\n\tweights: 向量，各个自变量的权重\n\tsubset：只用特定列\n\tmethod：如果Y是生存类，选择\"exp\"；如果Y有两列，则选择\"poisson\"；如果Y是因子，则选择\"class\"；如果Y是连续变量,则选择\"anova\";\n\tparms：分裂参数；\"anova\"类没有该参数，\"exp\"和\"poisson\"仅有先验分布；\"class\"有三项，prior对先验概率，loss对损失矩阵，split对不纯度计算方式———\"gini\"或\"information\"。\n\tcontrol：参见rpart.control\n\tcost：各自变量代价列表\n函数：rpart.control\n用法：rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...)\n参数：\n\tminsplit：父节点最小样本数。\n\tminbucket：叶节点最小样本数。\n\tcp：complexity parameter.复杂性参数。分支后cp必须高于设定值。\n\tmaxcompete：保留的最大候选分支数。\n\tmaxsurrogate：替代分裂最大数。\n\tusesurrogate：替代分裂处理。0，仅展示；1，使用替代分裂；2，不使用替代分裂\n\txval：cv数\n\tsurrogatestyle：替代分裂风格。0，数值；1，比例。\n\tmaxdepth：树的最大深度\n```\n## 代码\n```R\n# 加载rpart包\nlibrary(rpart)\n# 构建数据框\nd <- data.frame(length,exon,ratio,FPKM,class)\n# 训练集和测试集\ntrain.l <- sample(1:dim(d)[1],as.integer(dim(d)[1]*0.7)) #取样列\ntrain <- d[train.l,] #训练集\ntest <- d[-train.l,] #测试集\n# rpart控制参数设置\nct <- rpart.control(xval=10, minsplit=100, minbucket=40, maxdepth=4) #采用10折交叉验证；停止分裂条件：1、父节点样本小于100；2、叶节点样本大于40；3、深度小于4层；\n# rpart建模\nfit<-rpart(class~length+exon+rat+FPKM,train,method='class',control=ct,parms = list(split = \"information\"))\n# 可视化\nplot(fit,margin=0.1)\ntext(fit,use.n=T) #原始树\n# 剪枝\nplotcp(fit)  #筛选合适的cp值\nplot(prune(fit,cp=0.01),margin=0.1) \ntext(prune(fit,cp=0.01),use.n=T) #length过拟合，增大cp值，减少\nplot(prune(fit,cp=0.045),margin=0.1)\ntext(prune(fit,cp=0.045),use.n=T) \n# 确定模型\nfit.used<-prune(fit,cp=0.045)\n# 精度\nsummary(test$type == predict(fit.used,test,type='class'))  #统计预测结果与实际结果是否相同\nsummary(test$type[test$type != predict(fit.used,test,type='class')]) #统计预测结果与实际一致\nsummary(test$type[test$type == predict(fit.used,test,type='class')]) #统计预测结果与实际不一致\n```\n![原始树](/upload/1/raw_tree.png '原始树')\n![第一次剪枝](/upload/1/pruned_tree1.png '0.01cp值剪枝结果，过拟合严重。（默认cp值为0.01，所以与原始树一致）')\n![第二次剪枝](/upload/1/pruned_tree2.png '最终模型，0.045cp值剪枝结果，主要依靠exon和length属性进行分类')\n## 模型分析\n最终的模型，只用到了两个属性。这个和之前数据描述相符（长度和外显子数勉强可以区分，表达比例和FPKM重叠严重）。将模型用于测试数据集，模型表现：\n![2x2列联表](/upload/1/2x2ContingencyTable.png '2x2列联表')\n敏感度$TPR$:\n$$TPR=\\frac{7546}{7546+332}=0.9578573$$\n精度$PPV$:\n$$PPV=\\frac{7546}{179+7546}=0.9768285$$\n特异性$SPC$:\n$$SPC=\\frac{430}{430+332}=0.5643045$$\n准确度$ACC$:\n$$ACC=\\frac{430+7546}{430+179+332+7546}=0.9397903$$\n从上述四个指标上看，除了特异性差很多，其他指标都表现良好。考虑到unknown中有潜在的基因，特异性差也是在预料当中。\n在将该模型用于原始数据中unknown部分再进行重新分类，2105个unknown数据中预测出641个潜在的基因，再进一步对模型分类后的数据进行描述：\n![预测后数据描述](/upload/1/summary1.png '预测后数据描述')\n可以看出，预测后基因与技术噪音在长度和外显子数这两个属性区别更加明显。\n# 结论\n相对之前的经验判断，使用了CART算法构建技术噪音过滤决策树，避免了主观判断，科学量化了过滤条件。\n# 参考文献\n1. Duda R O, Hart P E, Stork D G. Pattern classification[M]. John Wiley & Sons, 2012.\n2. Prensner J R, Iyer M K, Balbin O A, et al. Transcriptome sequencing across a prostate cancer cohort identifies PCAT-1, an unannotated lincRNA implicated in disease progression[J]. Nature biotechnology, 2011, 29(8): 742-749.\n3. Harrington P. 机器学习实战[J]. 人民邮电出版社, 北京, 2013.\n","slug":"利用决策树来识别Cufflinks中的技术噪音","published":1,"updated":"2016-06-13T02:22:12.397Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ciqj5btoz0006c49niw13exdq","content":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>熟悉RNA-Seq分析流程的朋友大多都知道，在经典的TopHat-Cufflinks分析流程中，对于最终的转录子（transfrag）组装的结果，很难去判断是否真实存在。在实际操作中，我们一般采用简单的一刀切，来进行过滤。比如，对于FPKM&lt;1的进行过滤。这种方法虽然很有效，但是可能会丢失部分有价值的信息。在这篇文章中，我向大家介绍一种基于决策树的机器学习方法来识别Cufflins结果中的技术噪音（Tech Noise）。</p>\n<h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><h3 id=\"什么是决策树\"><a href=\"#什么是决策树\" class=\"headerlink\" title=\"什么是决策树\"></a>什么是决策树</h3><p>决策树是一种依托选择策略建立的图解法。在机器学习中，决策树属于有督导学习（Supervised Learning）中分类（Classification）算法范围。决策树，经常出现在我们生活中。比如，</p>\n<blockquote>\n<p>顾客：这手机能拍照吗？<br>销售：能！<br>顾客：这手机能上网吗？<br>销售：能！<br>顾客：这手机能防水吗？<br>销售：能！<br>顾客：我买了！ </p>\n</blockquote>\n<p>用图解法表示决策过程：<br><img src=\"/upload/1/decisionTreeSample.png\" alt=\"买手机决策过程图解\" title=\"买手机决策树\"><br>这样，有了上面的认识，我们可以发现：</p>\n<ul>\n<li>决策树是一个树结构（可以是二叉树或非二叉树）。</li>\n<li>决策树上的每个非叶节点表示了对某一属性的判断。</li>\n<li>非叶节点的分支表示该属性的判断结果。</li>\n<li>叶节点表示分类结果。</li>\n</ul>\n<h3 id=\"决策树分类算法\"><a href=\"#决策树分类算法\" class=\"headerlink\" title=\"决策树分类算法\"></a>决策树分类算法</h3><p>决策树中最关键步骤是分裂属性。什么是分裂属性？分裂属性是指在某个节点，根据对某个属性的判断来把该节点集合分裂为两个或多个子集合（二叉树或多叉树）。理想的分裂，应该使各个子集合中的各个元素的类别一致。这样一来，分裂属性的情况包括以下三种：</p>\n<ul>\n<li>属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。</li>\n<li>属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。</li>\n<li>属性是连续值。此时确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支。  </li>\n</ul>\n<p>这样看来，属性的量度对于决策树的构造非常重要。选择属性度量的算法有很多，大部分都是采用自顶向下递归分治法（Recursive Partitioning），同时采用不回溯的贪心策略（Greedy Strategy）。</p>\n<h4 id=\"ID3算法\"><a href=\"#ID3算法\" class=\"headerlink\" title=\"ID3算法\"></a>ID3算法</h4><p>ID3算法，全称Iterative Dichotomiser 3 ，译名迭代二叉树3代，1986年由Quinlan提出。ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。即在节点处选择当前集合中最大信息增益的属性作为分裂属性，并在子集中不断递归。以下是信息论中的基本概念：</p>\n<ul>\n<li>信息熵（Entropy）<br>若信源符号有n种取值：$U_1$…$U_i$…$U_n$，对应概率为：$P_1$…$P_i$…$P_n$，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性$-logP_i$的统计平均值（$E$），可称为信息熵，即$$H(U)=E\\left | -logP_i \\right |=-\\sum_{i=0}^{n}P_ilogP_i$$式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算。 信息熵是表示随机变量不确定性的度量。从直观上，信息熵越大，变量包含的信息量越大，变量的不确定性也越大。一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。如果没有信息，任何公式或者数字的游戏都无法排除不确定性。<br><img src=\"/upload/1/EntropyFunction.png\" alt=\"信息熵函数\" title=\"信息熵函数\"><br><img src=\"/upload/1/towbit.png\" alt=\"二元信息熵\" title=\"二元信息熵\"></li>\n<li>复合熵（Composite Entropy）<br>根据贝叶斯定理，结合信息熵。复合熵函数：$$H(x,y)=E|-logP_{(x,y)}|=\\sum_{x}^{ }\\sum_{y}^{ }P_{(x,y)}logP_{(x,y)}=-\\sum_{x}^{ }\\sum_{y}^{ }P_xP_ylog(P_xP_y)$$</li>\n<li>条件熵（conditional Entropy）<br>根据贝叶斯定理，结合信息熵。条件熵函数：$$H(X|Y)=E|E|-logP_{(X|Y)}||=-\\sum_{Y}^{}\\sum_{X}^{}P_{Y_j}P_{(X_i|Y_j)}logP_{(X_i|Y_j)}$$</li>\n<li>信息增益（IG，Information Gain）<br>信息增益在信息论中是很有效的特征选择方法。但凡是特征选择，总是将特征的重要程度先进行量化后再进行选择，而如何量化特征的重要性，就成了各种方法最大的不同。在信息增益中，量化特征的重要性就是看特征能够为分类系统带来多少的信息，带来的信息越多，该特征就越为重要。这可以用信息熵去量化。另外需要注意的是，信息增益是针对特征而言，也就是系统中有没有某个特征间系统信息熵的变化。令$A$为分裂特征，$A_i$为A特征分裂子集，$S_n$为n类样本，$U$为所有样本集合，信息增益函数可以表示为：$$IG(A)=H(U)-H(U|A)=\\sum_{j=1}^{S}\\sum_{i=1}^{A}P_{A_i}P_{S_j|A_i}log(P_{S_j|A_i})-\\sum_{j=1}^{S}P_{(S_j)}log(P_{(S_j)})$$</li>\n</ul>\n<p>ID3算法就是利用信息增益来量化特征的重要性。其理论在于，每次分裂实在使系统信息增益最大化的特征上进行。同时ID3只有正负两个样本（全局二叉树），根据信息增益函数，ID3可以表示为：<br>$$\\max_{i}(IG(A_i))=\\max_{i}(H(U)-H(U|A))=$$</p>\n<p>$$\\max_{i}(P_A(P_{(\\oplus|A)}logP_{(\\oplus|A)}+P_{(\\ominus|A)}logP_{(\\ominus|A)})+(1-P_A)(P_{(\\oplus|C_SA)}logP_{(\\oplus|C_SA)}+P_{(\\ominus|C_SA)}logP_{(\\ominus|C_SA)})-P_{\\oplus}logP_{\\oplus}-P_{\\ominus}logP_{\\ominus})$$</p>\n<h5 id=\"实例\"><a href=\"#实例\" class=\"headerlink\" title=\"实例\"></a>实例</h5><p>有类数据如下表，m,n为两个特征，c表示分类。</p>\n<table>\n<thead>\n<tr>\n<th>m</th>\n<th>n</th>\n<th>c</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>+</td>\n<td>+</td>\n<td>+</td>\n</tr>\n<tr>\n<td>+</td>\n<td>+</td>\n<td>+</td>\n</tr>\n<tr>\n<td>+</td>\n<td>-</td>\n<td>+</td>\n</tr>\n<tr>\n<td>-</td>\n<td>+</td>\n<td>-</td>\n</tr>\n<tr>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>+</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>根据ID3算法对其建决策树：</p>\n<ol>\n<li>以m特征进行分裂的信息增量。<br>$$IG(m)=H(U)-H(U|A)=-(\\frac{1}{2}log_2\\frac{1}{2}+\\frac{1}{2}log_2\\frac{1}{2})-(-\\frac{4}{6}(\\frac{1}{4}log_2\\frac{1}{4}+\\frac{3}{4}log_2\\frac{3}{4})-\\frac{2}{6}(0+0))=0.459$$<br><img src=\"/upload/1/mclass.png\" alt=\"以m特征分裂\" title=\"以m特征分裂\"></li>\n<li>以n特征进行分裂的信息增量。<br>$$IG(n)=H(U)-H(U|A)=-(\\frac{1}{2}log_2\\frac{1}{2}+\\frac{1}{2}log_2\\frac{1}{2})-(-\\frac{3}{6}(\\frac{1}{3}log_2\\frac{1}{3}+\\frac{2}{3}log_2\\frac{2}{3})-\\frac{3}{6}(\\frac{1}{3}log_2\\frac{1}{3}+\\frac{2}{3}log_2\\frac{2}{3}))=0.082$$<br><img src=\"/upload/1/nclass.png\" alt=\"以n特征分裂\" title=\"以n特征分裂\"></li>\n<li>因为$IG(m)&gt;IG(n)$，选择信息增益最大的特征进行分裂，所以选择m进行分裂。</li>\n</ol>\n<h4 id=\"C4-5算法\"><a href=\"#C4-5算法\" class=\"headerlink\" title=\"C4.5算法\"></a>C4.5算法</h4><p>C4.5算法是ID3算法进行改进算法，主要不同点是对特征重要性量化上采用的是信息增益率（Info Gain Ratio）。这种方法解决ID3算法在某个属性存在大量不同值时导致的过拟合现象。相比于ID3算法，改进有如下几个要点：</p>\n<ul>\n<li>用信息增益率来选择属性。</li>\n<li>在决策树构造过程中可以进行剪枝。ID3算法会由于某些具有很少元素的结点可能会使构造的决策树过适应（Overfitting），这时候不考虑这些结点可能会更好。</li>\n<li>对非离散数据也能处理。</li>\n<li>能够对不完整数据进行处理。</li>\n</ul>\n<h5 id=\"信息增益率\"><a href=\"#信息增益率\" class=\"headerlink\" title=\"信息增益率\"></a>信息增益率</h5><p>分裂属性$A$，分裂后子集空间$S$，信息增益率IGR则可表示为：<br>$$IGR(S,A)=\\frac{IG(S,A)}{II(S,A)}$$<br>其中II表示内在信息（Intrinsic Information），其定义为：<br>$$II(S,A)=-\\sum\\frac{|S_i|}{|S|}log_2\\frac{|S_i|}{|S|}$$<br>其中$S_i$是属性A分割S而形成的$i$子集。</p>\n<h5 id=\"实例-1\"><a href=\"#实例-1\" class=\"headerlink\" title=\"实例\"></a>实例</h5><p>依旧上述例子</p>\n<ul>\n<li>以m特征进行分裂的信息增量。$$IGR(m)=\\frac{IG(m)}{II{m}}=\\frac{0.459}{\\frac{4}{6}log_2\\frac{4}{6}+\\frac{2}{6}log_2\\frac{2}{6}}=\\frac{0.459}{0.918}=0.5$$</li>\n</ul>\n<h4 id=\"CART算法\"><a href=\"#CART算法\" class=\"headerlink\" title=\"CART算法\"></a>CART算法</h4><p>分类和回归树（Classification and regression tree，CART）在模式分类中被视作一种通用的树生长算法。CART提供一种通用框架，利用它，可以实例化各种各样不同的判定树。按照CART，有6个问题需要回答：</p>\n<ul>\n<li>节点分支数应该是多少？</li>\n<li>节点测试是哪个属性？</li>\n<li>何时可以令某节点成为叶节点？</li>\n<li>如何剪枝？</li>\n<li>如果叶节点仍不“纯”，那么怎样给它赋类别标记？</li>\n<li>缺省数据处理？</li>\n</ul>\n<h5 id=\"分支数目\"><a href=\"#分支数目\" class=\"headerlink\" title=\"分支数目\"></a>分支数目</h5><p>任意多类别样本都可以通过多个二叉树进行划分。所以，二叉树具有万能的表达能力，并且在训练上很简便。所以，CART一般采用二叉树结构。</p>\n<h5 id=\"节点不纯度\"><a href=\"#节点不纯度\" class=\"headerlink\" title=\"节点不纯度\"></a>节点不纯度</h5><p>CART中有四种方式来量化不纯度（Impurity）：</p>\n<ul>\n<li>信息熵不纯度（Entropy Impurity）$$I(N)=-sum_{j}^{}P(w_j)log_2P(w_j)$$</li>\n<li>方差不纯度(Variance Impurity) $$I(N)=\\prod_{j}P(w_j)=P(1-P)$$</li>\n<li>Gini不纯度(Gini Impurity)$$I(N)=\\sum_{j}^{}P(w_j)P(w_{C_{S}j})=1-\\sum_{j}^{}P^2(w_j)$$</li>\n<li>误分类不纯度(Missclassification Impurity)$$I(N)=1-\\max_{j}P(w_j)$$<br><img src=\"/upload/1/class.png\" alt=\"4种不纯度量化方法\" title=\"4种不纯度量化方法\"></li>\n</ul>\n<p>在如何选择分裂属性上，很明显的启发思路是选择使不纯度下降最快的那个属性（ID3算法）。可记作$$\\max_{i}{\\Delta}I(N)=I(N)-P_LI(N_L)-(1-P_L)I(N_R)$$，其中$N_L$和$N_R$分别是左右子节点。这种贪心算法有个缺点，就是无法确保顺序局部优化过程能得到全局最优。实践中，熵不纯度和Gini不纯度运用广泛，但在实践过程中，不同的不纯度函数对最后的分类效果和其性能影响往往比预期的小。所以，在实践中，反倒是停止判决和剪枝算法受到更多的重视。</p>\n<h5 id=\"分支停止准则\"><a href=\"#分支停止准则\" class=\"headerlink\" title=\"分支停止准则\"></a>分支停止准则</h5><p>二叉树，分支训练中，分支过多会过拟合，分支过少，训练样本误差大，分类效果也差。究竟何时应该停止分支呢？这里介绍五种方法：</p>\n<ol>\n<li>验证和交叉验证技术（validation and cross-validation）。<br>将部分样本用于训练树，剩余样本作为验证。持续节点分支，直到对于验证集的分类误差最小化。</li>\n<li>不纯度下降门限。<br>预先设定一个不纯度下降差的（小）门限值，当候选分支小于这个值时，停止分支。这种方法有两个优点，一是全部样本均用于训练，二是树的每层都可能存在叶节点。但该方法也有个固有缺点，即门限值预先设定相当困难，因为最终性能与门限大小并无直接的函数关系</li>\n<li>节点最小样本数。<br>当候选分支样本数小于设定值，停止分支。这种方法类似k-近临分类器，当样本集中时，分割子集就小；当样本稀疏时，分割子集就大。</li>\n<li>高复杂度换取高的准确度。<br>通过最小化全局指标：$$\\alpha{\\cdot}size+\\sum_{叶节点}^{}I(N)$$这里$size$表示节点或分支的数目，$\\alpha$是个正常数（类似神经网络中的正则化方法）。这种方法中，$\\alpha$的设定也很难，它也与最终分类性能无简单的相关关系。</li>\n<li>不纯度下降的显著性检验。<br>估计之前的所有${\\Delta}I(N)$概率分布，在对某一候选分支$i$时，检验${\\Delta}I(N_i)$是否与上述分布存在统计学差异。<br><img src=\"/upload/1/chisq.png\" alt=\"卡方检验\" title=\"采取卡方检验。对于候选分支，其理论发生概率$p$对应得卡方值函数为$\\frac{(1-p)^2}{p}$，在0.05的置信水平下，$p$应该大于0.18。\"></li>\n<li>$\\chi^2$假设检验。<br>要点是判断候选分支是否存在统计学上的意义，也就是判断分支是否明显有别于随机分支。假设节点存在$n$个样本（$n_1$个$w_1$类，$n_2$个$w_2$类），候选分支将$Pn$个样本分到左分支，$(1-P)n$分到了右支。此时假设是随机分支，左枝应该有$Pn_1$个$w_1$和$Pn_2$个$w_2$，其它剩余的都在右支。如果用$\\chi^2$来评估这次分支与随机的偏离度，那偏离度$$\\chi^2=\\sum_{i=1}^{2}\\frac{(n_{iL}-n_{ie})^2}{n_{ie}}$$其中$n_{iL}$是指$w_i$在左分支的数目，而$n_{ie}=Pn_i$则对应随机分支情况下的值。当两者相同时，$\\chi^2$接近0，相反，当$\\chi^2$越大，说明两者差异越大。当其大于某临界值时，就可以拒绝零假设（候选分支等于随机分支）。</li>\n<li>损失矩阵<br>构建损失矩阵，根据损失矩阵计算节点损失，当叶节点的损失大于其父节点的损失时，则停止分支。</li>\n</ol>\n<h5 id=\"剪枝\"><a href=\"#剪枝\" class=\"headerlink\" title=\"剪枝\"></a>剪枝</h5><p>有时，分支停止方法会因缺少足够的前瞻性而过早停止（视界局限效应）。这时，需要剪枝（pruning）来克服这种缺陷。在剪枝过程中，树首先要充足生长，直到叶节点都有最小不纯度值为止，因而没有任何推定的“视界局限”。然后，对所有相邻的成对叶节点，考虑能否合并它们，如果合并它们只引起很小的不纯度增长，那就合并它们为新的叶节点。</p>\n<h2 id=\"cufflinks中决策树的理论基础\"><a href=\"#cufflinks中决策树的理论基础\" class=\"headerlink\" title=\"cufflinks中决策树的理论基础\"></a>cufflinks中决策树的理论基础</h2><p>cufflinks软件原理是将mapped reads组装成transfrag，由于reads很短，拼接还原难度高。这个过程中可能会产生错误的结果。我们假设：</p>\n<ol>\n<li>同一染色体上的基因的特征相似。</li>\n<li>cufflinks产生的未知的transfrag大部分是noise，可靠的transfrag仅占小部分，这些可靠的transfrag拥有与正常基因相似的特征，且与noise存在较大差异。<br>这样，我们就拥有了一个正样本集（真实基因），和一个负样本集（noise），这就可以建立决策树，来确认真实基因的特征。<br>在算法上，采取常用的CART算法，软件选用R语言及其rpart模块。</li>\n</ol>\n<h2 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h2><p>该次选取了个5特征变量：长度，外显子数目，样本中表达比例，FPKM值。</p>\n<table>\n<thead>\n<tr>\n<th>特征变量</th>\n<th>选择原因</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>长度</td>\n<td>人类基因90%以上都在300bp以上，过短的transfrag可能是由于技术噪音产生</td>\n</tr>\n<tr>\n<td>外显子数</td>\n<td>人类大部分基因都拥有多个外显子数，趋于更为复杂的结构；而技术噪音一般都很简单。</td>\n</tr>\n<tr>\n<td>样本中表达比例</td>\n<td>正常的基因，应该会在多个样本中都会出现表达，而部分技术噪音可能仅在个别样本中表达。</td>\n</tr>\n<tr>\n<td>FPKM值</td>\n<td>正常基因的FPKM值应该会在一定范围内，而技术噪音序列可能会存在FPKM极值。</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"数据建模\"><a href=\"#数据建模\" class=\"headerlink\" title=\"数据建模\"></a>数据建模</h1><h2 id=\"数据描述\"><a href=\"#数据描述\" class=\"headerlink\" title=\"数据描述\"></a>数据描述</h2><p>数据使用的是10个人类样本cufflinks组装结果中1号染色体上的数据。<br>数据集中共28,290个转录子，其中26,185是已知转录位点，2,105是未知转录位点。</p>\n<table>\n<thead>\n<tr>\n<th>特征变量</th>\n<th>最小值</th>\n<th>中值</th>\n<th>均值</th>\n<th>最大值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>长度</td>\n<td>1</td>\n<td>1540</td>\n<td>2679</td>\n<td>79680</td>\n</tr>\n<tr>\n<td>外显子数</td>\n<td>1</td>\n<td>5</td>\n<td>7.887</td>\n<td>100</td>\n</tr>\n<tr>\n<td>表达比例</td>\n<td>0.10</td>\n<td>0.50</td>\n<td>0.54</td>\n<td>1.00</td>\n</tr>\n<tr>\n<td>FPKM</td>\n<td>0</td>\n<td>0.115</td>\n<td>2.131</td>\n<td>5271</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"/upload/1/summary.png\" alt=\"数据描述\" title=\"数据描述(已知转录和未知转录并不能明显区分开)\"><br><img src=\"/upload/1/pca_before.png\" alt=\"PCA分析\" title=\"PCA结果（无法区分）\"></p>\n<h2 id=\"rpart包参数简介\"><a href=\"#rpart包参数简介\" class=\"headerlink\" title=\"rpart包参数简介\"></a>rpart包参数简介</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">函数：rpart</span><br><span class=\"line\">用法：rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tformula：公式，例Y~X1+X2+X3+X4</span><br><span class=\"line\">\tdata：数据源，一般是数据框</span><br><span class=\"line\">\tweights: 向量，各个自变量的权重</span><br><span class=\"line\">\tsubset：只用特定列</span><br><span class=\"line\">\tmethod：如果Y是生存类，选择&quot;exp&quot;；如果Y有两列，则选择&quot;poisson&quot;；如果Y是因子，则选择&quot;class&quot;；如果Y是连续变量,则选择&quot;anova&quot;;</span><br><span class=\"line\">\tparms：分裂参数；&quot;anova&quot;类没有该参数，&quot;exp&quot;和&quot;poisson&quot;仅有先验分布；&quot;class&quot;有三项，prior对先验概率，loss对损失矩阵，split对不纯度计算方式———&quot;gini&quot;或&quot;information&quot;。</span><br><span class=\"line\">\tcontrol：参见rpart.control</span><br><span class=\"line\">\tcost：各自变量代价列表</span><br><span class=\"line\">函数：rpart.control</span><br><span class=\"line\">用法：rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tminsplit：父节点最小样本数。</span><br><span class=\"line\">\tminbucket：叶节点最小样本数。</span><br><span class=\"line\">\tcp：complexity parameter.复杂性参数。分支后cp必须高于设定值。</span><br><span class=\"line\">\tmaxcompete：保留的最大候选分支数。</span><br><span class=\"line\">\tmaxsurrogate：替代分裂最大数。</span><br><span class=\"line\">\tusesurrogate：替代分裂处理。0，仅展示；1，使用替代分裂；2，不使用替代分裂</span><br><span class=\"line\">\txval：cv数</span><br><span class=\"line\">\tsurrogatestyle：替代分裂风格。0，数值；1，比例。</span><br><span class=\"line\">\tmaxdepth：树的最大深度</span><br></pre></td></tr></table></figure>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 加载rpart包</span></span><br><span class=\"line\"><span class=\"keyword\">library</span>(rpart)</span><br><span class=\"line\"><span class=\"comment\"># 构建数据框</span></span><br><span class=\"line\">d &lt;- data.frame(length,exon,ratio,FPKM,class)</span><br><span class=\"line\"><span class=\"comment\"># 训练集和测试集</span></span><br><span class=\"line\">train.l &lt;- sample(<span class=\"number\">1</span>:dim(d)[<span class=\"number\">1</span>],as.integer(dim(d)[<span class=\"number\">1</span>]*<span class=\"number\">0.7</span>)) <span class=\"comment\">#取样列</span></span><br><span class=\"line\">train &lt;- d[train.l,] <span class=\"comment\">#训练集</span></span><br><span class=\"line\">test &lt;- d[-train.l,] <span class=\"comment\">#测试集</span></span><br><span class=\"line\"><span class=\"comment\"># rpart控制参数设置</span></span><br><span class=\"line\">ct &lt;- rpart.control(xval=<span class=\"number\">10</span>, minsplit=<span class=\"number\">100</span>, minbucket=<span class=\"number\">40</span>, maxdepth=<span class=\"number\">4</span>) <span class=\"comment\">#采用10折交叉验证；停止分裂条件：1、父节点样本小于100；2、叶节点样本大于40；3、深度小于4层；</span></span><br><span class=\"line\"><span class=\"comment\"># rpart建模</span></span><br><span class=\"line\">fit&lt;-rpart(class~length+exon+rat+FPKM,train,method=<span class=\"string\">'class'</span>,control=ct,parms = list(split = <span class=\"string\">\"information\"</span>))</span><br><span class=\"line\"><span class=\"comment\"># 可视化</span></span><br><span class=\"line\">plot(fit,margin=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">text(fit,use.n=<span class=\"literal\">T</span>) <span class=\"comment\">#原始树</span></span><br><span class=\"line\"><span class=\"comment\"># 剪枝</span></span><br><span class=\"line\">plotcp(fit)  <span class=\"comment\">#筛选合适的cp值</span></span><br><span class=\"line\">plot(prune(fit,cp=<span class=\"number\">0.01</span>),margin=<span class=\"number\">0.1</span>) </span><br><span class=\"line\">text(prune(fit,cp=<span class=\"number\">0.01</span>),use.n=<span class=\"literal\">T</span>) <span class=\"comment\">#length过拟合，增大cp值，减少</span></span><br><span class=\"line\">plot(prune(fit,cp=<span class=\"number\">0.045</span>),margin=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">text(prune(fit,cp=<span class=\"number\">0.045</span>),use.n=<span class=\"literal\">T</span>) </span><br><span class=\"line\"><span class=\"comment\"># 确定模型</span></span><br><span class=\"line\">fit.used&lt;-prune(fit,cp=<span class=\"number\">0.045</span>)</span><br><span class=\"line\"><span class=\"comment\"># 精度</span></span><br><span class=\"line\">summary(test$type == predict(fit.used,test,type=<span class=\"string\">'class'</span>))  <span class=\"comment\">#统计预测结果与实际结果是否相同</span></span><br><span class=\"line\">summary(test$type[test$type != predict(fit.used,test,type=<span class=\"string\">'class'</span>)]) <span class=\"comment\">#统计预测结果与实际一致</span></span><br><span class=\"line\">summary(test$type[test$type == predict(fit.used,test,type=<span class=\"string\">'class'</span>)]) <span class=\"comment\">#统计预测结果与实际不一致</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/upload/1/raw_tree.png\" alt=\"原始树\" title=\"原始树\"><br><img src=\"/upload/1/pruned_tree1.png\" alt=\"第一次剪枝\" title=\"0.01cp值剪枝结果，过拟合严重。（默认cp值为0.01，所以与原始树一致）\"><br><img src=\"/upload/1/pruned_tree2.png\" alt=\"第二次剪枝\" title=\"最终模型，0.045cp值剪枝结果，主要依靠exon和length属性进行分类\"></p>\n<h2 id=\"模型分析\"><a href=\"#模型分析\" class=\"headerlink\" title=\"模型分析\"></a>模型分析</h2><p>最终的模型，只用到了两个属性。这个和之前数据描述相符（长度和外显子数勉强可以区分，表达比例和FPKM重叠严重）。将模型用于测试数据集，模型表现：<br><img src=\"/upload/1/2x2ContingencyTable.png\" alt=\"2x2列联表\" title=\"2x2列联表\"><br>敏感度$TPR$:<br>$$TPR=\\frac{7546}{7546+332}=0.9578573$$<br>精度$PPV$:<br>$$PPV=\\frac{7546}{179+7546}=0.9768285$$<br>特异性$SPC$:<br>$$SPC=\\frac{430}{430+332}=0.5643045$$<br>准确度$ACC$:<br>$$ACC=\\frac{430+7546}{430+179+332+7546}=0.9397903$$<br>从上述四个指标上看，除了特异性差很多，其他指标都表现良好。考虑到unknown中有潜在的基因，特异性差也是在预料当中。<br>在将该模型用于原始数据中unknown部分再进行重新分类，2105个unknown数据中预测出641个潜在的基因，再进一步对模型分类后的数据进行描述：<br><img src=\"/upload/1/summary1.png\" alt=\"预测后数据描述\" title=\"预测后数据描述\"><br>可以看出，预测后基因与技术噪音在长度和外显子数这两个属性区别更加明显。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>相对之前的经验判断，使用了CART算法构建技术噪音过滤决策树，避免了主观判断，科学量化了过滤条件。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><ol>\n<li>Duda R O, Hart P E, Stork D G. Pattern classification[M]. John Wiley &amp; Sons, 2012.</li>\n<li>Prensner J R, Iyer M K, Balbin O A, et al. Transcriptome sequencing across a prostate cancer cohort identifies PCAT-1, an unannotated lincRNA implicated in disease progression[J]. Nature biotechnology, 2011, 29(8): 742-749.</li>\n<li>Harrington P. 机器学习实战[J]. 人民邮电出版社, 北京, 2013.</li>\n</ol>\n","excerpt":"","more":"<h1 id=\"简介\"><a href=\"#简介\" class=\"headerlink\" title=\"简介\"></a>简介</h1><p>熟悉RNA-Seq分析流程的朋友大多都知道，在经典的TopHat-Cufflinks分析流程中，对于最终的转录子（transfrag）组装的结果，很难去判断是否真实存在。在实际操作中，我们一般采用简单的一刀切，来进行过滤。比如，对于FPKM&lt;1的进行过滤。这种方法虽然很有效，但是可能会丢失部分有价值的信息。在这篇文章中，我向大家介绍一种基于决策树的机器学习方法来识别Cufflins结果中的技术噪音（Tech Noise）。</p>\n<h2 id=\"决策树\"><a href=\"#决策树\" class=\"headerlink\" title=\"决策树\"></a>决策树</h2><h3 id=\"什么是决策树\"><a href=\"#什么是决策树\" class=\"headerlink\" title=\"什么是决策树\"></a>什么是决策树</h3><p>决策树是一种依托选择策略建立的图解法。在机器学习中，决策树属于有督导学习（Supervised Learning）中分类（Classification）算法范围。决策树，经常出现在我们生活中。比如，</p>\n<blockquote>\n<p>顾客：这手机能拍照吗？<br>销售：能！<br>顾客：这手机能上网吗？<br>销售：能！<br>顾客：这手机能防水吗？<br>销售：能！<br>顾客：我买了！ </p>\n</blockquote>\n<p>用图解法表示决策过程：<br><img src=\"/upload/1/decisionTreeSample.png\" alt=\"买手机决策过程图解\" title=\"买手机决策树\"><br>这样，有了上面的认识，我们可以发现：</p>\n<ul>\n<li>决策树是一个树结构（可以是二叉树或非二叉树）。</li>\n<li>决策树上的每个非叶节点表示了对某一属性的判断。</li>\n<li>非叶节点的分支表示该属性的判断结果。</li>\n<li>叶节点表示分类结果。</li>\n</ul>\n<h3 id=\"决策树分类算法\"><a href=\"#决策树分类算法\" class=\"headerlink\" title=\"决策树分类算法\"></a>决策树分类算法</h3><p>决策树中最关键步骤是分裂属性。什么是分裂属性？分裂属性是指在某个节点，根据对某个属性的判断来把该节点集合分裂为两个或多个子集合（二叉树或多叉树）。理想的分裂，应该使各个子集合中的各个元素的类别一致。这样一来，分裂属性的情况包括以下三种：</p>\n<ul>\n<li>属性是离散值且不要求生成二叉决策树。此时用属性的每一个划分作为一个分支。</li>\n<li>属性是离散值且要求生成二叉决策树。此时使用属性划分的一个子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支。</li>\n<li>属性是连续值。此时确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支。  </li>\n</ul>\n<p>这样看来，属性的量度对于决策树的构造非常重要。选择属性度量的算法有很多，大部分都是采用自顶向下递归分治法（Recursive Partitioning），同时采用不回溯的贪心策略（Greedy Strategy）。</p>\n<h4 id=\"ID3算法\"><a href=\"#ID3算法\" class=\"headerlink\" title=\"ID3算法\"></a>ID3算法</h4><p>ID3算法，全称Iterative Dichotomiser 3 ，译名迭代二叉树3代，1986年由Quinlan提出。ID3算法是以信息论为基础，以信息熵和信息增益度为衡量标准，从而实现对数据的归纳分类。即在节点处选择当前集合中最大信息增益的属性作为分裂属性，并在子集中不断递归。以下是信息论中的基本概念：</p>\n<ul>\n<li>信息熵（Entropy）<br>若信源符号有n种取值：$U_1$…$U_i$…$U_n$，对应概率为：$P_1$…$P_i$…$P_n$，且各种符号的出现彼此独立。这时，信源的平均不确定性应当为单个符号不确定性$-logP_i$的统计平均值（$E$），可称为信息熵，即$$H(U)=E\\left | -logP_i \\right |=-\\sum_{i=0}^{n}P_ilogP_i$$式中对数一般取2为底，单位为比特。但是，也可以取其它对数底，采用其它相应的单位，它们间可用换底公式换算。 信息熵是表示随机变量不确定性的度量。从直观上，信息熵越大，变量包含的信息量越大，变量的不确定性也越大。一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。如果没有信息，任何公式或者数字的游戏都无法排除不确定性。<br><img src=\"/upload/1/EntropyFunction.png\" alt=\"信息熵函数\" title=\"信息熵函数\"><br><img src=\"/upload/1/towbit.png\" alt=\"二元信息熵\" title=\"二元信息熵\"></li>\n<li>复合熵（Composite Entropy）<br>根据贝叶斯定理，结合信息熵。复合熵函数：$$H(x,y)=E|-logP_{(x,y)}|=\\sum_{x}^{ }\\sum_{y}^{ }P_{(x,y)}logP_{(x,y)}=-\\sum_{x}^{ }\\sum_{y}^{ }P_xP_ylog(P_xP_y)$$</li>\n<li>条件熵（conditional Entropy）<br>根据贝叶斯定理，结合信息熵。条件熵函数：$$H(X|Y)=E|E|-logP_{(X|Y)}||=-\\sum_{Y}^{}\\sum_{X}^{}P_{Y_j}P_{(X_i|Y_j)}logP_{(X_i|Y_j)}$$</li>\n<li>信息增益（IG，Information Gain）<br>信息增益在信息论中是很有效的特征选择方法。但凡是特征选择，总是将特征的重要程度先进行量化后再进行选择，而如何量化特征的重要性，就成了各种方法最大的不同。在信息增益中，量化特征的重要性就是看特征能够为分类系统带来多少的信息，带来的信息越多，该特征就越为重要。这可以用信息熵去量化。另外需要注意的是，信息增益是针对特征而言，也就是系统中有没有某个特征间系统信息熵的变化。令$A$为分裂特征，$A_i$为A特征分裂子集，$S_n$为n类样本，$U$为所有样本集合，信息增益函数可以表示为：$$IG(A)=H(U)-H(U|A)=\\sum_{j=1}^{S}\\sum_{i=1}^{A}P_{A_i}P_{S_j|A_i}log(P_{S_j|A_i})-\\sum_{j=1}^{S}P_{(S_j)}log(P_{(S_j)})$$</li>\n</ul>\n<p>ID3算法就是利用信息增益来量化特征的重要性。其理论在于，每次分裂实在使系统信息增益最大化的特征上进行。同时ID3只有正负两个样本（全局二叉树），根据信息增益函数，ID3可以表示为：<br>$$\\max_{i}(IG(A_i))=\\max_{i}(H(U)-H(U|A))=$$</p>\n<p>$$\\max_{i}(P_A(P_{(\\oplus|A)}logP_{(\\oplus|A)}+P_{(\\ominus|A)}logP_{(\\ominus|A)})+(1-P_A)(P_{(\\oplus|C_SA)}logP_{(\\oplus|C_SA)}+P_{(\\ominus|C_SA)}logP_{(\\ominus|C_SA)})-P_{\\oplus}logP_{\\oplus}-P_{\\ominus}logP_{\\ominus})$$</p>\n<h5 id=\"实例\"><a href=\"#实例\" class=\"headerlink\" title=\"实例\"></a>实例</h5><p>有类数据如下表，m,n为两个特征，c表示分类。</p>\n<table>\n<thead>\n<tr>\n<th>m</th>\n<th>n</th>\n<th>c</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>+</td>\n<td>+</td>\n<td>+</td>\n</tr>\n<tr>\n<td>+</td>\n<td>+</td>\n<td>+</td>\n</tr>\n<tr>\n<td>+</td>\n<td>-</td>\n<td>+</td>\n</tr>\n<tr>\n<td>-</td>\n<td>+</td>\n<td>-</td>\n</tr>\n<tr>\n<td>-</td>\n<td>-</td>\n<td>-</td>\n</tr>\n<tr>\n<td>+</td>\n<td>-</td>\n<td>-</td>\n</tr>\n</tbody>\n</table>\n<p>根据ID3算法对其建决策树：</p>\n<ol>\n<li>以m特征进行分裂的信息增量。<br>$$IG(m)=H(U)-H(U|A)=-(\\frac{1}{2}log_2\\frac{1}{2}+\\frac{1}{2}log_2\\frac{1}{2})-(-\\frac{4}{6}(\\frac{1}{4}log_2\\frac{1}{4}+\\frac{3}{4}log_2\\frac{3}{4})-\\frac{2}{6}(0+0))=0.459$$<br><img src=\"/upload/1/mclass.png\" alt=\"以m特征分裂\" title=\"以m特征分裂\"></li>\n<li>以n特征进行分裂的信息增量。<br>$$IG(n)=H(U)-H(U|A)=-(\\frac{1}{2}log_2\\frac{1}{2}+\\frac{1}{2}log_2\\frac{1}{2})-(-\\frac{3}{6}(\\frac{1}{3}log_2\\frac{1}{3}+\\frac{2}{3}log_2\\frac{2}{3})-\\frac{3}{6}(\\frac{1}{3}log_2\\frac{1}{3}+\\frac{2}{3}log_2\\frac{2}{3}))=0.082$$<br><img src=\"/upload/1/nclass.png\" alt=\"以n特征分裂\" title=\"以n特征分裂\"></li>\n<li>因为$IG(m)&gt;IG(n)$，选择信息增益最大的特征进行分裂，所以选择m进行分裂。</li>\n</ol>\n<h4 id=\"C4-5算法\"><a href=\"#C4-5算法\" class=\"headerlink\" title=\"C4.5算法\"></a>C4.5算法</h4><p>C4.5算法是ID3算法进行改进算法，主要不同点是对特征重要性量化上采用的是信息增益率（Info Gain Ratio）。这种方法解决ID3算法在某个属性存在大量不同值时导致的过拟合现象。相比于ID3算法，改进有如下几个要点：</p>\n<ul>\n<li>用信息增益率来选择属性。</li>\n<li>在决策树构造过程中可以进行剪枝。ID3算法会由于某些具有很少元素的结点可能会使构造的决策树过适应（Overfitting），这时候不考虑这些结点可能会更好。</li>\n<li>对非离散数据也能处理。</li>\n<li>能够对不完整数据进行处理。</li>\n</ul>\n<h5 id=\"信息增益率\"><a href=\"#信息增益率\" class=\"headerlink\" title=\"信息增益率\"></a>信息增益率</h5><p>分裂属性$A$，分裂后子集空间$S$，信息增益率IGR则可表示为：<br>$$IGR(S,A)=\\frac{IG(S,A)}{II(S,A)}$$<br>其中II表示内在信息（Intrinsic Information），其定义为：<br>$$II(S,A)=-\\sum\\frac{|S_i|}{|S|}log_2\\frac{|S_i|}{|S|}$$<br>其中$S_i$是属性A分割S而形成的$i$子集。</p>\n<h5 id=\"实例-1\"><a href=\"#实例-1\" class=\"headerlink\" title=\"实例\"></a>实例</h5><p>依旧上述例子</p>\n<ul>\n<li>以m特征进行分裂的信息增量。$$IGR(m)=\\frac{IG(m)}{II{m}}=\\frac{0.459}{\\frac{4}{6}log_2\\frac{4}{6}+\\frac{2}{6}log_2\\frac{2}{6}}=\\frac{0.459}{0.918}=0.5$$</li>\n</ul>\n<h4 id=\"CART算法\"><a href=\"#CART算法\" class=\"headerlink\" title=\"CART算法\"></a>CART算法</h4><p>分类和回归树（Classification and regression tree，CART）在模式分类中被视作一种通用的树生长算法。CART提供一种通用框架，利用它，可以实例化各种各样不同的判定树。按照CART，有6个问题需要回答：</p>\n<ul>\n<li>节点分支数应该是多少？</li>\n<li>节点测试是哪个属性？</li>\n<li>何时可以令某节点成为叶节点？</li>\n<li>如何剪枝？</li>\n<li>如果叶节点仍不“纯”，那么怎样给它赋类别标记？</li>\n<li>缺省数据处理？</li>\n</ul>\n<h5 id=\"分支数目\"><a href=\"#分支数目\" class=\"headerlink\" title=\"分支数目\"></a>分支数目</h5><p>任意多类别样本都可以通过多个二叉树进行划分。所以，二叉树具有万能的表达能力，并且在训练上很简便。所以，CART一般采用二叉树结构。</p>\n<h5 id=\"节点不纯度\"><a href=\"#节点不纯度\" class=\"headerlink\" title=\"节点不纯度\"></a>节点不纯度</h5><p>CART中有四种方式来量化不纯度（Impurity）：</p>\n<ul>\n<li>信息熵不纯度（Entropy Impurity）$$I(N)=-sum_{j}^{}P(w_j)log_2P(w_j)$$</li>\n<li>方差不纯度(Variance Impurity) $$I(N)=\\prod_{j}P(w_j)=P(1-P)$$</li>\n<li>Gini不纯度(Gini Impurity)$$I(N)=\\sum_{j}^{}P(w_j)P(w_{C_{S}j})=1-\\sum_{j}^{}P^2(w_j)$$</li>\n<li>误分类不纯度(Missclassification Impurity)$$I(N)=1-\\max_{j}P(w_j)$$<br><img src=\"/upload/1/class.png\" alt=\"4种不纯度量化方法\" title=\"4种不纯度量化方法\"></li>\n</ul>\n<p>在如何选择分裂属性上，很明显的启发思路是选择使不纯度下降最快的那个属性（ID3算法）。可记作$$\\max_{i}{\\Delta}I(N)=I(N)-P_LI(N_L)-(1-P_L)I(N_R)$$，其中$N_L$和$N_R$分别是左右子节点。这种贪心算法有个缺点，就是无法确保顺序局部优化过程能得到全局最优。实践中，熵不纯度和Gini不纯度运用广泛，但在实践过程中，不同的不纯度函数对最后的分类效果和其性能影响往往比预期的小。所以，在实践中，反倒是停止判决和剪枝算法受到更多的重视。</p>\n<h5 id=\"分支停止准则\"><a href=\"#分支停止准则\" class=\"headerlink\" title=\"分支停止准则\"></a>分支停止准则</h5><p>二叉树，分支训练中，分支过多会过拟合，分支过少，训练样本误差大，分类效果也差。究竟何时应该停止分支呢？这里介绍五种方法：</p>\n<ol>\n<li>验证和交叉验证技术（validation and cross-validation）。<br>将部分样本用于训练树，剩余样本作为验证。持续节点分支，直到对于验证集的分类误差最小化。</li>\n<li>不纯度下降门限。<br>预先设定一个不纯度下降差的（小）门限值，当候选分支小于这个值时，停止分支。这种方法有两个优点，一是全部样本均用于训练，二是树的每层都可能存在叶节点。但该方法也有个固有缺点，即门限值预先设定相当困难，因为最终性能与门限大小并无直接的函数关系</li>\n<li>节点最小样本数。<br>当候选分支样本数小于设定值，停止分支。这种方法类似k-近临分类器，当样本集中时，分割子集就小；当样本稀疏时，分割子集就大。</li>\n<li>高复杂度换取高的准确度。<br>通过最小化全局指标：$$\\alpha{\\cdot}size+\\sum_{叶节点}^{}I(N)$$这里$size$表示节点或分支的数目，$\\alpha$是个正常数（类似神经网络中的正则化方法）。这种方法中，$\\alpha$的设定也很难，它也与最终分类性能无简单的相关关系。</li>\n<li>不纯度下降的显著性检验。<br>估计之前的所有${\\Delta}I(N)$概率分布，在对某一候选分支$i$时，检验${\\Delta}I(N_i)$是否与上述分布存在统计学差异。<br><img src=\"/upload/1/chisq.png\" alt=\"卡方检验\" title=\"采取卡方检验。对于候选分支，其理论发生概率$p$对应得卡方值函数为$\\frac{(1-p)^2}{p}$，在0.05的置信水平下，$p$应该大于0.18。\"></li>\n<li>$\\chi^2$假设检验。<br>要点是判断候选分支是否存在统计学上的意义，也就是判断分支是否明显有别于随机分支。假设节点存在$n$个样本（$n_1$个$w_1$类，$n_2$个$w_2$类），候选分支将$Pn$个样本分到左分支，$(1-P)n$分到了右支。此时假设是随机分支，左枝应该有$Pn_1$个$w_1$和$Pn_2$个$w_2$，其它剩余的都在右支。如果用$\\chi^2$来评估这次分支与随机的偏离度，那偏离度$$\\chi^2=\\sum_{i=1}^{2}\\frac{(n_{iL}-n_{ie})^2}{n_{ie}}$$其中$n_{iL}$是指$w_i$在左分支的数目，而$n_{ie}=Pn_i$则对应随机分支情况下的值。当两者相同时，$\\chi^2$接近0，相反，当$\\chi^2$越大，说明两者差异越大。当其大于某临界值时，就可以拒绝零假设（候选分支等于随机分支）。</li>\n<li>损失矩阵<br>构建损失矩阵，根据损失矩阵计算节点损失，当叶节点的损失大于其父节点的损失时，则停止分支。</li>\n</ol>\n<h5 id=\"剪枝\"><a href=\"#剪枝\" class=\"headerlink\" title=\"剪枝\"></a>剪枝</h5><p>有时，分支停止方法会因缺少足够的前瞻性而过早停止（视界局限效应）。这时，需要剪枝（pruning）来克服这种缺陷。在剪枝过程中，树首先要充足生长，直到叶节点都有最小不纯度值为止，因而没有任何推定的“视界局限”。然后，对所有相邻的成对叶节点，考虑能否合并它们，如果合并它们只引起很小的不纯度增长，那就合并它们为新的叶节点。</p>\n<h2 id=\"cufflinks中决策树的理论基础\"><a href=\"#cufflinks中决策树的理论基础\" class=\"headerlink\" title=\"cufflinks中决策树的理论基础\"></a>cufflinks中决策树的理论基础</h2><p>cufflinks软件原理是将mapped reads组装成transfrag，由于reads很短，拼接还原难度高。这个过程中可能会产生错误的结果。我们假设：</p>\n<ol>\n<li>同一染色体上的基因的特征相似。</li>\n<li>cufflinks产生的未知的transfrag大部分是noise，可靠的transfrag仅占小部分，这些可靠的transfrag拥有与正常基因相似的特征，且与noise存在较大差异。<br>这样，我们就拥有了一个正样本集（真实基因），和一个负样本集（noise），这就可以建立决策树，来确认真实基因的特征。<br>在算法上，采取常用的CART算法，软件选用R语言及其rpart模块。</li>\n</ol>\n<h2 id=\"特征选择\"><a href=\"#特征选择\" class=\"headerlink\" title=\"特征选择\"></a>特征选择</h2><p>该次选取了个5特征变量：长度，外显子数目，样本中表达比例，FPKM值。</p>\n<table>\n<thead>\n<tr>\n<th>特征变量</th>\n<th>选择原因</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>长度</td>\n<td>人类基因90%以上都在300bp以上，过短的transfrag可能是由于技术噪音产生</td>\n</tr>\n<tr>\n<td>外显子数</td>\n<td>人类大部分基因都拥有多个外显子数，趋于更为复杂的结构；而技术噪音一般都很简单。</td>\n</tr>\n<tr>\n<td>样本中表达比例</td>\n<td>正常的基因，应该会在多个样本中都会出现表达，而部分技术噪音可能仅在个别样本中表达。</td>\n</tr>\n<tr>\n<td>FPKM值</td>\n<td>正常基因的FPKM值应该会在一定范围内，而技术噪音序列可能会存在FPKM极值。</td>\n</tr>\n</tbody>\n</table>\n<h1 id=\"数据建模\"><a href=\"#数据建模\" class=\"headerlink\" title=\"数据建模\"></a>数据建模</h1><h2 id=\"数据描述\"><a href=\"#数据描述\" class=\"headerlink\" title=\"数据描述\"></a>数据描述</h2><p>数据使用的是10个人类样本cufflinks组装结果中1号染色体上的数据。<br>数据集中共28,290个转录子，其中26,185是已知转录位点，2,105是未知转录位点。</p>\n<table>\n<thead>\n<tr>\n<th>特征变量</th>\n<th>最小值</th>\n<th>中值</th>\n<th>均值</th>\n<th>最大值</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>长度</td>\n<td>1</td>\n<td>1540</td>\n<td>2679</td>\n<td>79680</td>\n</tr>\n<tr>\n<td>外显子数</td>\n<td>1</td>\n<td>5</td>\n<td>7.887</td>\n<td>100</td>\n</tr>\n<tr>\n<td>表达比例</td>\n<td>0.10</td>\n<td>0.50</td>\n<td>0.54</td>\n<td>1.00</td>\n</tr>\n<tr>\n<td>FPKM</td>\n<td>0</td>\n<td>0.115</td>\n<td>2.131</td>\n<td>5271</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"/upload/1/summary.png\" alt=\"数据描述\" title=\"数据描述(已知转录和未知转录并不能明显区分开)\"><br><img src=\"/upload/1/pca_before.png\" alt=\"PCA分析\" title=\"PCA结果（无法区分）\"></p>\n<h2 id=\"rpart包参数简介\"><a href=\"#rpart包参数简介\" class=\"headerlink\" title=\"rpart包参数简介\"></a>rpart包参数简介</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">函数：rpart</span><br><span class=\"line\">用法：rpart(formula, data, weights, subset, na.action = na.rpart, method, model = FALSE, x = FALSE, y = TRUE, parms, control, cost, ...)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tformula：公式，例Y~X1+X2+X3+X4</span><br><span class=\"line\">\tdata：数据源，一般是数据框</span><br><span class=\"line\">\tweights: 向量，各个自变量的权重</span><br><span class=\"line\">\tsubset：只用特定列</span><br><span class=\"line\">\tmethod：如果Y是生存类，选择&quot;exp&quot;；如果Y有两列，则选择&quot;poisson&quot;；如果Y是因子，则选择&quot;class&quot;；如果Y是连续变量,则选择&quot;anova&quot;;</span><br><span class=\"line\">\tparms：分裂参数；&quot;anova&quot;类没有该参数，&quot;exp&quot;和&quot;poisson&quot;仅有先验分布；&quot;class&quot;有三项，prior对先验概率，loss对损失矩阵，split对不纯度计算方式———&quot;gini&quot;或&quot;information&quot;。</span><br><span class=\"line\">\tcontrol：参见rpart.control</span><br><span class=\"line\">\tcost：各自变量代价列表</span><br><span class=\"line\">函数：rpart.control</span><br><span class=\"line\">用法：rpart.control(minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, maxdepth = 30, ...)</span><br><span class=\"line\">参数：</span><br><span class=\"line\">\tminsplit：父节点最小样本数。</span><br><span class=\"line\">\tminbucket：叶节点最小样本数。</span><br><span class=\"line\">\tcp：complexity parameter.复杂性参数。分支后cp必须高于设定值。</span><br><span class=\"line\">\tmaxcompete：保留的最大候选分支数。</span><br><span class=\"line\">\tmaxsurrogate：替代分裂最大数。</span><br><span class=\"line\">\tusesurrogate：替代分裂处理。0，仅展示；1，使用替代分裂；2，不使用替代分裂</span><br><span class=\"line\">\txval：cv数</span><br><span class=\"line\">\tsurrogatestyle：替代分裂风格。0，数值；1，比例。</span><br><span class=\"line\">\tmaxdepth：树的最大深度</span><br></pre></td></tr></table></figure>\n<h2 id=\"代码\"><a href=\"#代码\" class=\"headerlink\" title=\"代码\"></a>代码</h2><figure class=\"highlight r\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 加载rpart包</span></span><br><span class=\"line\"><span class=\"keyword\">library</span>(rpart)</span><br><span class=\"line\"><span class=\"comment\"># 构建数据框</span></span><br><span class=\"line\">d &lt;- data.frame(length,exon,ratio,FPKM,class)</span><br><span class=\"line\"><span class=\"comment\"># 训练集和测试集</span></span><br><span class=\"line\">train.l &lt;- sample(<span class=\"number\">1</span>:dim(d)[<span class=\"number\">1</span>],as.integer(dim(d)[<span class=\"number\">1</span>]*<span class=\"number\">0.7</span>)) <span class=\"comment\">#取样列</span></span><br><span class=\"line\">train &lt;- d[train.l,] <span class=\"comment\">#训练集</span></span><br><span class=\"line\">test &lt;- d[-train.l,] <span class=\"comment\">#测试集</span></span><br><span class=\"line\"><span class=\"comment\"># rpart控制参数设置</span></span><br><span class=\"line\">ct &lt;- rpart.control(xval=<span class=\"number\">10</span>, minsplit=<span class=\"number\">100</span>, minbucket=<span class=\"number\">40</span>, maxdepth=<span class=\"number\">4</span>) <span class=\"comment\">#采用10折交叉验证；停止分裂条件：1、父节点样本小于100；2、叶节点样本大于40；3、深度小于4层；</span></span><br><span class=\"line\"><span class=\"comment\"># rpart建模</span></span><br><span class=\"line\">fit&lt;-rpart(class~length+exon+rat+FPKM,train,method=<span class=\"string\">'class'</span>,control=ct,parms = list(split = <span class=\"string\">\"information\"</span>))</span><br><span class=\"line\"><span class=\"comment\"># 可视化</span></span><br><span class=\"line\">plot(fit,margin=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">text(fit,use.n=<span class=\"literal\">T</span>) <span class=\"comment\">#原始树</span></span><br><span class=\"line\"><span class=\"comment\"># 剪枝</span></span><br><span class=\"line\">plotcp(fit)  <span class=\"comment\">#筛选合适的cp值</span></span><br><span class=\"line\">plot(prune(fit,cp=<span class=\"number\">0.01</span>),margin=<span class=\"number\">0.1</span>) </span><br><span class=\"line\">text(prune(fit,cp=<span class=\"number\">0.01</span>),use.n=<span class=\"literal\">T</span>) <span class=\"comment\">#length过拟合，增大cp值，减少</span></span><br><span class=\"line\">plot(prune(fit,cp=<span class=\"number\">0.045</span>),margin=<span class=\"number\">0.1</span>)</span><br><span class=\"line\">text(prune(fit,cp=<span class=\"number\">0.045</span>),use.n=<span class=\"literal\">T</span>) </span><br><span class=\"line\"><span class=\"comment\"># 确定模型</span></span><br><span class=\"line\">fit.used&lt;-prune(fit,cp=<span class=\"number\">0.045</span>)</span><br><span class=\"line\"><span class=\"comment\"># 精度</span></span><br><span class=\"line\">summary(test$type == predict(fit.used,test,type=<span class=\"string\">'class'</span>))  <span class=\"comment\">#统计预测结果与实际结果是否相同</span></span><br><span class=\"line\">summary(test$type[test$type != predict(fit.used,test,type=<span class=\"string\">'class'</span>)]) <span class=\"comment\">#统计预测结果与实际一致</span></span><br><span class=\"line\">summary(test$type[test$type == predict(fit.used,test,type=<span class=\"string\">'class'</span>)]) <span class=\"comment\">#统计预测结果与实际不一致</span></span><br></pre></td></tr></table></figure>\n<p><img src=\"/upload/1/raw_tree.png\" alt=\"原始树\" title=\"原始树\"><br><img src=\"/upload/1/pruned_tree1.png\" alt=\"第一次剪枝\" title=\"0.01cp值剪枝结果，过拟合严重。（默认cp值为0.01，所以与原始树一致）\"><br><img src=\"/upload/1/pruned_tree2.png\" alt=\"第二次剪枝\" title=\"最终模型，0.045cp值剪枝结果，主要依靠exon和length属性进行分类\"></p>\n<h2 id=\"模型分析\"><a href=\"#模型分析\" class=\"headerlink\" title=\"模型分析\"></a>模型分析</h2><p>最终的模型，只用到了两个属性。这个和之前数据描述相符（长度和外显子数勉强可以区分，表达比例和FPKM重叠严重）。将模型用于测试数据集，模型表现：<br><img src=\"/upload/1/2x2ContingencyTable.png\" alt=\"2x2列联表\" title=\"2x2列联表\"><br>敏感度$TPR$:<br>$$TPR=\\frac{7546}{7546+332}=0.9578573$$<br>精度$PPV$:<br>$$PPV=\\frac{7546}{179+7546}=0.9768285$$<br>特异性$SPC$:<br>$$SPC=\\frac{430}{430+332}=0.5643045$$<br>准确度$ACC$:<br>$$ACC=\\frac{430+7546}{430+179+332+7546}=0.9397903$$<br>从上述四个指标上看，除了特异性差很多，其他指标都表现良好。考虑到unknown中有潜在的基因，特异性差也是在预料当中。<br>在将该模型用于原始数据中unknown部分再进行重新分类，2105个unknown数据中预测出641个潜在的基因，再进一步对模型分类后的数据进行描述：<br><img src=\"/upload/1/summary1.png\" alt=\"预测后数据描述\" title=\"预测后数据描述\"><br>可以看出，预测后基因与技术噪音在长度和外显子数这两个属性区别更加明显。</p>\n<h1 id=\"结论\"><a href=\"#结论\" class=\"headerlink\" title=\"结论\"></a>结论</h1><p>相对之前的经验判断，使用了CART算法构建技术噪音过滤决策树，避免了主观判断，科学量化了过滤条件。</p>\n<h1 id=\"参考文献\"><a href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"></a>参考文献</h1><ol>\n<li>Duda R O, Hart P E, Stork D G. Pattern classification[M]. John Wiley &amp; Sons, 2012.</li>\n<li>Prensner J R, Iyer M K, Balbin O A, et al. Transcriptome sequencing across a prostate cancer cohort identifies PCAT-1, an unannotated lincRNA implicated in disease progression[J]. Nature biotechnology, 2011, 29(8): 742-749.</li>\n<li>Harrington P. 机器学习实战[J]. 人民邮电出版社, 北京, 2013.</li>\n</ol>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ciqj5bto00000c49n4xlpw3b7","category_id":"ciqj5btoy0005c49nmjl508xs","_id":"ciqj5btpd0009c49ns4z8nzky"},{"post_id":"ciqj5btoi0002c49nhfshy03s","category_id":"ciqj5btoy0005c49nmjl508xs","_id":"ciqj5btpj000cc49n32w6lfyh"},{"post_id":"ciqj5btoz0006c49niw13exdq","category_id":"ciqj5btoy0005c49nmjl508xs","_id":"ciqj5btpl000ec49nq9yw6kfq"}],"PostTag":[{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btou0004c49nb768weiy","_id":"ciqj5btq5000kc49nac4t5w07"},{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btp60007c49n58r6tju6","_id":"ciqj5btq6000lc49nblizkgnj"},{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btpe000bc49npq762o6j","_id":"ciqj5btq9000nc49ny31ig88o"},{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btpk000dc49ng7qi1bxm","_id":"ciqj5btqc000oc49n0ew20ek6"},{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btpn000fc49nhjkd5j7f","_id":"ciqj5btqi000qc49n0vayqmwg"},{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btpo000gc49ndkv0wq22","_id":"ciqj5btqj000rc49n3rrnvhkj"},{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btpp000hc49n1bzboj46","_id":"ciqj5btqk000tc49n89noy9uh"},{"post_id":"ciqj5bto00000c49n4xlpw3b7","tag_id":"ciqj5btps000ic49nv3mjfew7","_id":"ciqj5btqm000uc49ndgbbuo4m"},{"post_id":"ciqj5btoi0002c49nhfshy03s","tag_id":"ciqj5btq1000jc49npsoy43i7","_id":"ciqj5btqn000wc49nflo1vjto"},{"post_id":"ciqj5btoi0002c49nhfshy03s","tag_id":"ciqj5btpe000bc49npq762o6j","_id":"ciqj5btqp000xc49noh53kzvq"},{"post_id":"ciqj5btoi0002c49nhfshy03s","tag_id":"ciqj5btqf000pc49nhl49u9n4","_id":"ciqj5btqt000yc49ny09r1fho"},{"post_id":"ciqj5btoz0006c49niw13exdq","tag_id":"ciqj5btqk000sc49n858vnrlt","_id":"ciqj5btqy0011c49ntp5z8dmw"},{"post_id":"ciqj5btoz0006c49niw13exdq","tag_id":"ciqj5btqn000vc49nnhr14a57","_id":"ciqj5btqz0012c49nkjrqloct"},{"post_id":"ciqj5btoz0006c49niw13exdq","tag_id":"ciqj5btqt000zc49ndx8g3lw9","_id":"ciqj5btqz0013c49n8e2rk8et"},{"post_id":"ciqj5btoz0006c49niw13exdq","tag_id":"ciqj5btqv0010c49n2g89bilw","_id":"ciqj5btr00014c49noolc7r96"}],"Tag":[{"name":"卷积神经网络","_id":"ciqj5btou0004c49nb768weiy"},{"name":"正向传播","_id":"ciqj5btp60007c49n58r6tju6"},{"name":"反向传播","_id":"ciqj5btpe000bc49npq762o6j"},{"name":"深度学习","_id":"ciqj5btpk000dc49ng7qi1bxm"},{"name":"数学推导","_id":"ciqj5btpn000fc49nhjkd5j7f"},{"name":"DL","_id":"ciqj5btpo000gc49ndkv0wq22"},{"name":"CNNs","_id":"ciqj5btpp000hc49n1bzboj46"},{"name":"Backpropagation","_id":"ciqj5btps000ic49nv3mjfew7"},{"name":"神经网络","_id":"ciqj5btq1000jc49npsoy43i7"},{"name":"python3","_id":"ciqj5btqf000pc49nhl49u9n4"},{"name":"RNA-seq","_id":"ciqj5btqk000sc49n858vnrlt"},{"name":"决策树","_id":"ciqj5btqn000vc49nnhr14a57"},{"name":"Cufflinks","_id":"ciqj5btqt000zc49ndx8g3lw9"},{"name":"技术噪音","_id":"ciqj5btqv0010c49n2g89bilw"}]}}